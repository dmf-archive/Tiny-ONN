---
created: 2025-10-19T03:50:33 (UTC +00:00)
tags: []
source: chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/
author: Currently at OpenAI
---

# The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton

> ## Excerpt
>
> Natalie Abreu
> Kempner Institute, Harvard University
> <natalieabreu@g.harvard.edu>
> &Nikhil Vyas
> Department of Computer Science, Harvard University
> <vyasnikhil96@gmail.com>
> &Sham Kakade
> Kempner Institute, Harvard University
> <sham@seas.harvard.edu>
> &Depen Morwani
> Kempner Institute, Harvard University
> <dmorwani@g.harvard.edu>

---

Natalie Abreu  
Kempner Institute, Harvard University <natalieabreu@g.harvard.edu> &Nikhil Vyas  
Department of Computer Science, Harvard University <vyasnikhil96@gmail.com> &Sham Kakade  
Kempner Institute, Harvard University <sham@seas.harvard.edu> &Depen Morwani  
Kempner Institute, Harvard University <dmorwani@g.harvard.edu>

###### Abstract

Recent efforts to accelerate LLM pretraining have focused on computationally-efficient approximations that exploit second-order structure. This raises a key question for large-scale training: how much performance is forfeited by these approximations? To probe this question, we establish a practical upper bound on iteration complexity by applying full Gauss-Newton (GN) preconditioning to transformer models of up to 150M parameters. Our experiments show that full GN updates yield substantial gains over existing optimizers, achieving a 5.4x reduction in training iterations compared to strong baselines like SOAP and Muon. Furthermore, we find that a precise layerwise GN preconditioner, which ignores cross-layer information, nearly matches the performance of the full GN method. Collectively, our results suggest: (1) the GN approximation is highly effective for preconditioning, implying higher-order loss terms may not be critical for convergence speed; (2) the layerwise Hessian structure contains sufficient information to achieve most of these potential gains; and (3) a significant performance gap exists between current approximate methods and an idealized layerwise oracle.

## 1 Introduction

With rising compute requirements for training large language models (LLMs), improving optimization methods has become a central strategy for improving training efficiency. Better optimizers can directly reduce the serial runtime to train an LLM, which is crucial for large-scale models that train from days to months. Optimization for LLMs has traditionally leveraged first-order methods such as SGD and Adam (Kingma & Ba, [2017](https://arxiv.org/html/2510.09378v1#bib.bib15)). However, recent research in optimization has started exploring the use of second-order optimizers for large-scale models, motivated by the faster convergence rates known from theory (Nesterov, [2018](https://arxiv.org/html/2510.09378v1#bib.bib26)) and potential to scale to larger batch sizes (Zhang et al., [2019](https://arxiv.org/html/2510.09378v1#bib.bib37)) – two ways of reducing serial runtime.

Some recent popular second-order methods include Shampoo (Gupta et al., [2018](https://arxiv.org/html/2510.09378v1#bib.bib8)), SOAP (Vyas et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib34)) and Muon (Jordan et al., [2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)). Shampoo won the recent optimization algorithms benchmark called AlgoPerf (Kasimbeg et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib14)), outperforming Adam by a margin of 28%. SOAP, a recent generalization of the Shampoo algorithm, has shown impressive performance on language modeling benchmarks, and has been used for training physics-informed neural networks (PINNs) (Wang et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib35)). Muon has been extensively optimized on the nanoGPT benchmark (Jordan et al., [2024a](https://arxiv.org/html/2510.09378v1#bib.bib12)), and was also recently scaled up to 16B LLMs, showing 50% improvements over AdamW (Liu et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib17)).

However, these methods do not use complete second-order information, instead focusing on memory- and computationally-efficient approximations of the Hessian. Indeed, precisely storing or computing the Hessian required for second-order methods such as Newton’s method is prohibitively expensive for modern LLMs that have billions of parameters. To remain practical, these methods leverage computationally-efficient estimators for the layerwise Hessian of neural networks.

The success of current methods motivates a better understanding of the fundamental potential of second-order optimizers. Our work is driven by the following question:

> _What are the fundamental performance limits of second-order optimization for LLMs, and what structural properties of the Hessian are essential for achieving them?_

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/x1.png)

Figure 1: Training step versus validation loss until loss 3.25 when each method is beyond its critical batch size. Gauss-Newton and Layerwise Gauss-Newton reach the target loss in 54 and 78 steps respectively, compared to 292 steps for SOAP.

To answer this, we first establish the performance limits of an idealized second-order method, full Gauss-Newton (GN), and measure its performance in terms of iteration complexity (the number of steps to reach a target loss). This serves as a practical lower bound for any second-order approach. We also analyze how this idealized method affects the critical batch size (McCandlish et al., [2018](https://arxiv.org/html/2510.09378v1#bib.bib24); Shallue et al., [2019](https://arxiv.org/html/2510.09378v1#bib.bib31); Jain et al., [2018b](https://arxiv.org/html/2510.09378v1#bib.bib11)), a key measure of data parallelism efficiency.

To isolate the essential structural properties of the Hessian, we then compare the full GN method to two variants:

1. 1.

   A prox-linear version of Gauss-Newton (GN-prox-linear) (Burke, [1985](https://arxiv.org/html/2510.09378v1#bib.bib3); Drusvyatskiy, [2017](https://arxiv.org/html/2510.09378v1#bib.bib5)), which utilizes the higher order information of the loss function itself (see Algorithms [1](https://arxiv.org/html/2510.09378v1#algorithm1 "Algorithm 1 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") and  [2](https://arxiv.org/html/2510.09378v1#algorithm2 "Algorithm 2 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") for comparison).

2. 2.

   A purely layerwise version, which ignores all cross-layer curvature information.

Our main findings are three-fold. First, both idealized second-order methods provide a substantial improvement over existing optimizers at large batch sizes, with the full Gauss-Newton method achieving a 5.4x reduction in iteration complexity over the SOAP optimizer. Second, the Gauss-Newton method significantly extends the critical batch size beyond that of prior methods, displaying near-optimal scaling. Finally, we find that the layerwise approach, despite its structural limitations, still substantially outperforms both SOAP and Adam. This suggests that even layer-wise curvature information alone is sufficient to achieve major gains in compute efficiency.

We stress that this work is an empirical study aimed at understanding performance limits with better higher order information, not at directly designing computationally cheaper optimizers themselves. Our implementation avoids materializing the Hessian by using Jacobian-vector products, though still has substantial computational overhead. We believe this work is best viewed as a tool for analysis that provides a target for more practical second-order methods to aspire to, and we provide further discussion on this point.

##### Paper organization

In Section [2](https://arxiv.org/html/2510.09378v1#S2 "2 Related work ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"), we cover related work and in Section [3](https://arxiv.org/html/2510.09378v1#S3 "3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") we provide background on existing optimization methods. In Section [4](https://arxiv.org/html/2510.09378v1#S4 "4 Full second-order optimization ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"), we introduce the setting for full second-order optimization and the Gauss-Newton matrix. In Section [5](https://arxiv.org/html/2510.09378v1#S5 "5 Experiment details ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") we provide the setup for our main experiments and in Section  [6.1](https://arxiv.org/html/2510.09378v1#S6.SS1 "6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") we discuss our results on iteration complexity and critical batch size of the full second-order method. In Section [6.2](https://arxiv.org/html/2510.09378v1#S6.SS2 "6.2 GN-prox-linear method ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") we compare the Gauss-Newton method to the GN-prox-linear method and in Section [6.3](https://arxiv.org/html/2510.09378v1#S6.SS3 "6.3 Layerwise Gauss-Newton ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") we compare to a layerwise variation. Finally, in Section [7](https://arxiv.org/html/2510.09378v1#S7 "7 Discussion and Conclusion ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") we discuss the implications as well as limitations of our work.

## 2 Related work

We mention a few of the most related works here and provide additional related work in Appendix [B](https://arxiv.org/html/2510.09378v1#A2 "Appendix B Additional related work ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"); work on specific optimizers for LLMs is discussed in Section [3](https://arxiv.org/html/2510.09378v1#S3 "3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). Most related to our work is Hessian-free optimization, which avoids explicit Hessian formation by leveraging Hessian-vector products (Martens, [2010](https://arxiv.org/html/2510.09378v1#bib.bib19)). This approach serves as an alternative to layerwise approximation methods of the Hessian as discussed in Section [3](https://arxiv.org/html/2510.09378v1#S3 "3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). Specifically, prior work on Hessian-free optimizers use the conjugate gradient (CG) to solve an incomplete (unconverged) optimization of the Newton step rather than storing an approximation to the Hessian. This is introduced by Martens ([2010](https://arxiv.org/html/2510.09378v1#bib.bib19)) on classification and auto-encoder tasks, and extended to additional settings such as recurrent neural networks by Martens & Sutskever ([2011a](https://arxiv.org/html/2510.09378v1#bib.bib21)) and Cho et al. ([2015](https://arxiv.org/html/2510.09378v1#bib.bib4)). Garcia et al. ([2023](https://arxiv.org/html/2510.09378v1#bib.bib7)) amortizes the CG steps in Hessian-free optimization for deep linear and auto-encoder models. In contrast, our work focuses on the setting of LLMs, and we leverage optimizers that are specifically designed for LLMs (e.g. Adam and Muon) rather than CG to apply the Gauss-Newton step.

## 3 Background on existing optimizers

Input:

θ0\\theta\_{0}

, training set

𝒯\\mathcal{T}

, training iterations

TT

, batch size

bb

for _t\=0,1,…,T−1t=0,1,\\dots,T-1_ do

Linearize model:

fθt(1)(θ,x)≔f(θt,x)+∇f(θt,x)⊤(θ−θt)f^{(1)}\_{\\theta\_{t}}(\\theta,x)\\coloneqq f(\\theta\_{t},x)+\\nabla f(\\theta\_{t},x)^{\\top}(\\theta-\\theta\_{t})

Convexify loss:

With

B∼𝒯B\\sim\\mathcal{T}

,

ℒ~θt(θ)≔1b∑(x,y)∈Bℓ(fθt(1)(θ,x),y)\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta)\\coloneqq\\frac{1}{b}\\sum\_{(x,y)\\in B}\\ell(f^{(1)}\_{\\theta\_{t}}(\\theta,x),y)

Gauss-Newton loss:

ℒ~θt(2)(θ)≔ℒ~θt(θt)+∇ℒ~θt(θt)(θ−θt)+12(θ−θt)⊤∇2ℒ~θt(θt)(θ−θt)\\mathcal{\\widetilde{L}}^{(2)}\_{\\theta\_{t}}(\\theta)\\coloneqq\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta\_{t})+\\nabla\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta\_{t})(\\theta-\\theta\_{t})+\\frac{1}{2}(\\theta-\\theta\_{t})^{\\top}\\nabla^{2}\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta\_{t})(\\theta-\\theta\_{t})

Set θ^\\widehat{\\theta} to be an approximate solution to the _least squares_ problem:

minθ⁡ℒ~θt(2)(θ)\\min\_{\\theta}\\mathcal{\\widetilde{L}}^{(2)}\_{\\theta\_{t}}(\\theta)

Line search: Set

θt+1←θt+α⋆(θ^−θt)\\theta\_{t+1}\\leftarrow\\theta\_{t}+\\alpha^{\\star}(\\widehat{\\theta}-\\theta\_{t})

, where:

α∗←arg⁡minα⁡ℒ(θt+α(θ^−θt))\\alpha^{\*}\\leftarrow\\arg\\min\_{\\alpha}\\mathcal{L}(\\theta\_{t}+\\alpha(\\widehat{\\theta}-\\theta\_{t}))

Algorithm 1 Gauss-Newton method

Input:

θ0\\theta\_{0}

, training set

𝒯\\mathcal{T}

, training iterations

TT

, batch size

bb

for _t\=0,1,…,T−1t=0,1,\\dots,T-1_ do

Linearize model:

fθt(1)(θ,x)≔f(θt,x)+∇f(θt,x)⊤(θ−θt)f^{(1)}\_{\\theta\_{t}}(\\theta,x)\\coloneqq f(\\theta\_{t},x)+\\nabla f(\\theta\_{t},x)^{\\top}(\\theta-\\theta\_{t})

Convexify loss:

With

B∼𝒯B\\sim\\mathcal{T}

,

ℒ~θt(θ)≔1b∑(x,y)∈Bℓ(fθt(1)(θ,x),y)\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta)\\coloneqq\\frac{1}{b}\\sum\_{(x,y)\\in B}\\ell(f^{(1)}\_{\\theta\_{t}}(\\theta,x),y)

Set θt+1\\theta\_{t+1} to be an approximate solution to the _convex_ problem:

minθ⁡ℒ~θt(θ)\\min\_{\\theta}\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta)

Algorithm 2 GN-prox-linear

We will denote the weight matrix of a model layer at timestep tt by Wt∈ℝm×nW\_{t}\\in\\mathbb{R}^{m\\times n} and the corresponding gradient by GtG\_{t}. We use η\\eta to denote the learning rate.

The most widely used optimizer for LLMs is Adam (Kingma & Ba, [2017](https://arxiv.org/html/2510.09378v1#bib.bib15)). Adam maintains matrices for the first and second moment of the gradient GtG\_{t}, denoted MtM\_{t} and VtV\_{t} respectively. Adam performs the element-wise update

<table id="S3.Ex1"><tbody><tr><td></td><td><math alttext="W_{t+1}:=W_{t}-\eta\frac{M_{t}}{\sqrt{V_{t}}}" display="block" id="S3.Ex1.m1" intent=":literal"><semantics><mrow><msub><mi>W</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>:=</mo><mrow><msub><mi>W</mi><mi>t</mi></msub><mo>−</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em"></mo><mfrac><msub><mi>M</mi><mi>t</mi></msub><msqrt><msub><mi>V</mi><mi>t</mi></msub></msqrt></mfrac></mrow></mrow></mrow><annotation encoding="application/x-tex">W_{t+1}:=W_{t}-\eta\frac{M_{t}}{\sqrt{V_{t}}}</annotation></semantics></math></td><td></td></tr></tbody></table>

AdaGrad (Duchi et al., [2011](https://arxiv.org/html/2510.09378v1#bib.bib6)) maintains an accumulator over the vectorized gradient gt\=vec(Gt)∈ℝmng\_{t}=\\mathrm{vec}(G\_{t})\\in\\mathbb{R}^{mn}. The preconditioner HtH\_{t} and vectorized weights wtw\_{t} at timestep tt are updated as

<table id="S3.Ex2"><tbody><tr><td></td><td><math alttext="H_{t}:=H_{t-1}+g_{t}g_{t}^{\top};\quad w_{t}:=w_{t-1}-\eta H^{-1/2}_{t}g_{t}" display="block" id="S3.Ex2.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>H</mi><mi>t</mi></msub><mo>:=</mo><mrow><msub><mi>H</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><msub><mi>g</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em"></mo><msubsup><mi>g</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><mo rspace="1.167em">;</mo><mrow><msub><mi>w</mi><mi>t</mi></msub><mo>:=</mo><mrow><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em"></mo><msubsup><mi>H</mi><mi>t</mi><mrow><mo>−</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><msub><mi>g</mi><mi>t</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">H_{t}:=H_{t-1}+g_{t}g_{t}^{\top};\quad w_{t}:=w_{t-1}-\eta H^{-1/2}_{t}g_{t}</annotation></semantics></math></td><td></td></tr></tbody></table>

Shampoo (Gupta et al., [2018](https://arxiv.org/html/2510.09378v1#bib.bib8)) was originally motivated by AdaGrad, but can be viewed as an approximation of the Gauss-Newton component of the Hessian (Anil et al., [2021](https://arxiv.org/html/2510.09378v1#bib.bib1); Osawa et al., [2023](https://arxiv.org/html/2510.09378v1#bib.bib28); Morwani et al., [2024](https://arxiv.org/html/2510.09378v1#bib.bib25)). These methods leverage computationally efficient approximations of the layerwise Hessian to precondition the gradient update. Shampoo maintains a separate preconditioner for each dimension of the weight matrix: For weight matrix W∈ℝm×nW\\in\\mathbb{R}^{m\\times n}, Shampoo maintains a left matrix Lt∈ℝm×mL\_{t}\\in\\mathbb{R}^{m\\times m} and a right matrix Rt∈ℝn×nR\_{t}\\in\\mathbb{R}^{n\\times n}. The update rule is as follows:

<table id="S3.Ex3"><tbody><tr><td></td><td><math alttext="L_{t}:=L_{t-1}+G_{t}G_{t}^{\top};\quad R_{t}:=R_{t-1}+G_{t}^{\top}G_{t};\quad W_{t}:=W_{t-1}-\eta\,L_{t}^{-1/4}G_{t}R_{t}^{-1/4}" display="block" id="S3.Ex3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>L</mi><mi>t</mi></msub><mo>:=</mo><mrow><msub><mi>L</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><msub><mi>G</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em"></mo><msubsup><mi>G</mi><mi>t</mi><mo>⊤</mo></msubsup></mrow></mrow></mrow><mo rspace="1.167em">;</mo><mrow><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>:=</mo><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><msubsup><mi>G</mi><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em"></mo><msub><mi>G</mi><mi>t</mi></msub></mrow></mrow></mrow><mo rspace="1.167em">;</mo><mrow><msub><mi>W</mi><mi>t</mi></msub><mo>:=</mo><mrow><msub><mi>W</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mrow><mi>η</mi><mo lspace="0.170em" rspace="0em"></mo><msubsup><mi>L</mi><mi>t</mi><mrow><mo>−</mo><mrow><mn>1</mn><mo>/</mo><mn>4</mn></mrow></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><msub><mi>G</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em"></mo><msubsup><mi>R</mi><mi>t</mi><mrow><mo>−</mo><mrow><mn>1</mn><mo>/</mo><mn>4</mn></mrow></mrow></msubsup></mrow></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">L_{t}:=L_{t-1}+G_{t}G_{t}^{\top};\quad R_{t}:=R_{t-1}+G_{t}^{\top}G_{t};\quad W_{t}:=W_{t-1}-\eta\,L_{t}^{-1/4}G_{t}R_{t}^{-1/4}</annotation></semantics></math></td><td></td></tr></tbody></table>

SOAP (Vyas et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib34)) is a recent variant of Shampoo that runs AdamW (Loshchilov & Hutter, [2019](https://arxiv.org/html/2510.09378v1#bib.bib18)) in the eigenbasis provided by Shampoo.

Muon (Jordan et al., [2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)) tracks the first moment of the gradient, denoted MtM\_{t}, and performs an orthonormal update:

<table id="S3.Ex4"><tbody><tr><td></td><td><math alttext="O_{t}:=NS(M_{t})\quad W_{t+1}:=W_{t}-\eta O_{t}" display="block" id="S3.Ex4.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>O</mi><mi>t</mi></msub><mo>:=</mo><mrow><mi>N</mi><mo lspace="0em" rspace="0em"></mo><mi>S</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>M</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mspace width="1em"></mspace><mrow><msub><mi>W</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>:=</mo><mrow><msub><mi>W</mi><mi>t</mi></msub><mo>−</mo><mrow><mi>η</mi><mo lspace="0em" rspace="0em"></mo><msub><mi>O</mi><mi>t</mi></msub></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">O_{t}:=NS(M_{t})\quad W_{t+1}:=W_{t}-\eta O_{t}</annotation></semantics></math></td><td></td></tr></tbody></table>

where NSNS denotes a Newton-Schulz orthogonalization procedure. (See Jordan et al. ([2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)) for further description of the Newton-Schulz method). Muon without momentum can be seen as a version of Shampoo without preconditioner accumulation (Bernstein & Newhouse, [2024](https://arxiv.org/html/2510.09378v1#bib.bib2)).

These second-order methods<sup>1</sup> have been shown to scale effectively to larger batch sizes (Zhang et al., [2019](https://arxiv.org/html/2510.09378v1#bib.bib37); Vyas et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib34)). However, these are restricted by the need for computationally efficient per-layer preconditioners given the high computational and memory requirements for computing the full Gauss-Newton matrix.

## 4 Full second-order optimization

### 4.1 Notation

Let f(θ,x)f(\\theta,x) denote the model with parameters θ\\theta and input xx. Let ℒ(f(θ,x),y)\\mathcal{L}(f(\\theta,x),y) denote the (convex) loss function which takes the model output and the true labels yy. We will use either ∇θℒ\\nabla\_{\\theta}\\mathcal{L} or gg or simply ∇ℒ\\nabla\\mathcal{L} to denote the gradient with respect to θ\\theta, ∇f\\nabla\_{f} to denote the derivative of ℒ\\mathcal{L} with respect to ff and H:=∇θ2ℒH:=\\nabla^{2}\_{\\theta}\\mathcal{L} to denote the Hessian. We will use fθt(1)(θ,x)f^{(1)}\_{\\theta\_{t}}(\\theta,x) and ℒθt(2)(θ):=ℒθt(2)(f(θ,x),y)\\mathcal{L}^{(2)}\_{\\theta\_{t}}(\\theta):=\\mathcal{L}^{(2)}\_{\\theta\_{t}}(f(\\theta,x),y) to denote the first-order Taylor expansion of ff around θt\\theta\_{t} and the second-order Taylor expansion of ℒ\\mathcal{L} around θt\\theta\_{t} respectively. Similarly, we will also use ℒ(θ):=ℒ(f(θ,x),y)\\mathcal{L}(\\theta):=\\mathcal{L}(f(\\theta,x),y) when ff, xx, and yy are clear from context. For simplicity, we will assume that we are working with cross-entropy loss throughout this work, however, our presentation holds for any general convex loss function.

### 4.2 Newton’s method & the Gauss-Newton matrix

Full second-order optimization requires full access to the Hessian HH, which can be used to precondition the gradient in each parameter update. This is known as Newton’s method, and results in the following update rule:

<table id="S4.Ex5"><tbody><tr><td></td><td><math alttext="\theta^{*}=\theta-H^{-1}g" display="block" id="S4.Ex5.m1" intent=":literal"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><mrow><mi>θ</mi><mo>−</mo><mrow><msup><mi>H</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em"></mo><mi>g</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta^{*}=\theta-H^{-1}g</annotation></semantics></math></td><td></td></tr></tbody></table>

In practice, for neural networks, the Hessian is not guaranteed to be positive semi-definite (PSD), and therefore Newton’s method does not guarantee that the loss decreases in each iteration or even converges. As a consequence, it is common to instead use the Gauss-Newton matrix.

The Gauss-Newton matrix is defined to be the first term of the following decomposition of the Hessian, where z≔f(x)z\\coloneqq f(x) denotes the pre-softmax outputs of the model ff and aa goes over the output dimensions of ff:

<table id="S4.Ex6"><tbody><tr><td></td><td><math alttext="\nabla_{\theta}^{2}\mathcal{L}(\theta)=\underbrace{\nabla_{\theta}f(\theta)^{\mathsf{T}}\nabla_{z}^{2}\mathcal{L}(\theta)\nabla_{\theta}f(\theta)}_{\text{Gauss-Newton matrix}}+\sum_{a}\frac{\delta\mathcal{L}}{\delta z_{a}}\nabla^{2}_{\theta}\left[f(\theta)\right]_{a}" display="block" id="S4.Ex6.m1" intent=":literal"><semantics><mrow><msubsup><mo>∇</mo><mi>θ</mi><mn>2</mn></msubsup><mi>ℒ</mi><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><munder><munder accentunder="true"><mrow><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><mi>f</mi></mrow><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0.167em" rspace="0em"></mo><mrow><msubsup><mo rspace="0.167em">∇</mo><mi>z</mi><mn>2</mn></msubsup><mi>ℒ</mi></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em"></mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><mi>f</mi></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="true">⏟</mo></munder><mtext>Gauss-Newton matrix</mtext></munder><mo rspace="0.055em">+</mo><munder><mo movablelimits="false">∑</mo><mi>a</mi></munder><mfrac><mrow><mi>δ</mi><mo lspace="0em" rspace="0em"></mo><mi>ℒ</mi></mrow><mrow><mi>δ</mi><mo lspace="0em" rspace="0em"></mo><msub><mi>z</mi><mi>a</mi></msub></mrow></mfrac><msubsup><mo lspace="0.167em">∇</mo><mi>θ</mi><mn>2</mn></msubsup><msub><mrow><mo>[</mo><mi>f</mi><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">\nabla_{\theta}^{2}\mathcal{L}(\theta)=\underbrace{\nabla_{\theta}f(\theta)^{\mathsf{T}}\nabla_{z}^{2}\mathcal{L}(\theta)\nabla_{\theta}f(\theta)}_{\text{Gauss-Newton matrix}}+\sum_{a}\frac{\delta\mathcal{L}}{\delta z_{a}}\nabla^{2}_{\theta}\left[f(\theta)\right]_{a}</annotation></semantics></math></td><td></td></tr></tbody></table>

That is, the Gauss-Newton matrix is defined as G≔∇θf(θ)𝖳∇z2ℒ(θ)∇θf(θ)G\\coloneqq\\nabla\_{\\theta}f(\\theta)^{\\mathsf{T}}\\nabla\_{z}^{2}\\mathcal{L}(\\theta)\\nabla\_{\\theta}f(\\theta). Intuitively, the Gauss-Newton captures the curvature of the loss function, but drops the curvature of the model. Unlike the Hessian, the Gauss-Newton matrix is PSD for MSE and cross-entropy loss (Martens, [2020](https://arxiv.org/html/2510.09378v1#bib.bib20)). This avoids untrustworthy updates as negative curvature implies unbounded decrease in loss (Martens, [2020](https://arxiv.org/html/2510.09378v1#bib.bib20)). Indeed, methods using the Gauss-Newton matrix rather than the full Hessian have been found to lead to better optimization (Martens, [2010](https://arxiv.org/html/2510.09378v1#bib.bib19); Martens & Sutskever, [2012](https://arxiv.org/html/2510.09378v1#bib.bib23); Vinyals & Povey, [2012](https://arxiv.org/html/2510.09378v1#bib.bib33)).

### 4.3 Memory-feasible Gauss-Newton implementation

To test the limits of second-order optimizers, we want to apply the full Gauss-Newton term as the preconditioner. Formally, for gradient gg, Gauss-Newton matrix GG and current parameters θ\\theta, the Gauss-Newton update is

<table id="S4.E1"><tbody><tr><td></td><td><math alttext="\theta^{*}=\theta-G^{-1}g" display="block" id="S4.E1.m1" intent=":literal"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><mrow><mi>θ</mi><mo>−</mo><mrow><msup><mi>G</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em"></mo><mi>g</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta^{*}=\theta-G^{-1}g</annotation></semantics></math></td><td></td><td rowspan="1"><span>(1)</span></td></tr></tbody></table>

However, given that computing the Gauss-Newton matrix directly is infeasible, we instead run a functionally equivalent method that leverages Jacobian-vector products (JVPs) to avoid explicitly storing the Hessian. Specifically, we optimize the second-order Taylor approximation of the loss function ℒ\\mathcal{L} with a first-order Taylor approximation of the model ff. The minimization of the loss in this setting is equivalent to using the Gauss-Newton matrix as a preconditioner (Martens & Sutskever, [2011b](https://arxiv.org/html/2510.09378v1#bib.bib22)). The proof is provided in Appendix [A](https://arxiv.org/html/2510.09378v1#A1 "Appendix A Proof of equivalence to Gauss-Newton method ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

We are now ready to define our Gauss-Newton method. Let ℒ~θt(θ)≔ℒ(fθt(1)(θ,x),y)\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta)\\coloneqq\\mathcal{L}(f^{(1)}\_{\\theta\_{t}}(\\theta,x),y) be the loss function on the first-order Taylor expansion of ff around current parameters θt\\theta\_{t}. Let ℒ~θt(2)(θ)\\mathcal{\\widetilde{L}}^{(2)}\_{\\theta\_{t}}(\\theta) denote the second-order Taylor expansion of ℒ~\\mathcal{\\widetilde{L}} around θt\\theta\_{t}.

Given current parameters θt\\theta\_{t}, we define the Gauss-Newton update (Algorithm [1](https://arxiv.org/html/2510.09378v1#algorithm1 "Algorithm 1 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")) as

<table id="S4.Ex7"><tbody><tr><td></td><td><math alttext="\theta^{*}=\text{argmin}_{\theta}\text{ }\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)" display="block" id="S4.Ex7.m1" intent=":literal"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><mrow><msub><mtext>argmin</mtext><mi>θ</mi></msub><mo lspace="0em" rspace="0em"></mo><mtext>&nbsp;</mtext><mo lspace="0em" rspace="0em"></mo><msubsup><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta^{*}=\text{argmin}_{\theta}\text{ }\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)</annotation></semantics></math></td><td></td></tr></tbody></table>

With this definition, there remains the problem of finding the minimizing θ∗\\theta^{\*}. As it is difficult to solve for the minimum directly, we instead use a separate optimizer to minimize ℒ~θt(2)(θ)\\mathcal{\\widetilde{L}}^{(2)}\_{\\theta\_{t}}(\\theta). In our experiments we use Muon (Jordan et al., [2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)) as this “inner optimizer” as we found it to outperform AdamW. More details on this inner optimization procedure are given in Section [5](https://arxiv.org/html/2510.09378v1#S5 "5 Experiment details ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

For the GN-prox-linear algorithm (Algorithm [2](https://arxiv.org/html/2510.09378v1#algorithm2 "Algorithm 2 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")), we instead define the updated iterate as

<table id="S4.Ex8"><tbody><tr><td></td><td><math alttext="\theta^{*}=\text{argmin}_{\theta}\text{ }\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)." display="block" id="S4.Ex8.m1" intent=":literal"><semantics><mrow><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><mrow><msub><mtext>argmin</mtext><mi>θ</mi></msub><mo lspace="0em" rspace="0em"></mo><mtext>&nbsp;</mtext><mo lspace="0em" rspace="0em"></mo><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\theta^{*}=\text{argmin}_{\theta}\text{ }\mathcal{\widetilde{L}}_{\theta_{t}}(\theta).</annotation></semantics></math></td><td></td></tr></tbody></table>

The results for GN-prox-linear algorithm are provided in Section [6.2](https://arxiv.org/html/2510.09378v1#S6.SS2 "6.2 GN-prox-linear method ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

## 5 Experiment details

##### Training details

We train 45M and 150M parameter LLaMA models (Touvron et al., [2023](https://arxiv.org/html/2510.09378v1#bib.bib32)) on the C4 dataset (Raffel et al., [2020](https://arxiv.org/html/2510.09378v1#bib.bib29)). Full details on models and hyperparameter sweeps are given in Appendix [D](https://arxiv.org/html/2510.09378v1#A4 "Appendix D Model details ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") and Appendix [G](https://arxiv.org/html/2510.09378v1#A7 "Appendix G Details on hyperparameter tuning ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") respectively.

##### Baselines

We run AdamW (Loshchilov & Hutter, [2019](https://arxiv.org/html/2510.09378v1#bib.bib18)), Muon (Jordan et al., [2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)), and SOAP (Vyas et al., [2025](https://arxiv.org/html/2510.09378v1#bib.bib34)) as baselines. For 45M models, for each method at each batch size, we run a hyperparameter sweep over learning rate, weight decay, and weight averaging decay if applicable. We additionally sweep the β2\\beta\_{2} parameter for Adam, the μ\\mu parameter for Muon, and (β1,β2)(\\beta\_{1},\\beta\_{2}) for SOAP. For 150M parameter models we run a more limited hyperparameter sweep over learning rate and β\\beta and μ\\mu parameters. To make sure runs are well-initialized, we start all runs after an AdamW warmup consisting of 5%5\\% of the Chinchilla-optimal number of tokens (Hoffmann et al., [2022](https://arxiv.org/html/2510.09378v1#bib.bib9)). More details on hyperparameter tuning are given in Appendix [G](https://arxiv.org/html/2510.09378v1#A7 "Appendix G Details on hyperparameter tuning ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

##### Gauss-Newton

For each training step, we take a first-order Taylor approximation of the model around the current parameters. We initialize the parameters of the Taylor approximation to be the pre-linesearch parameters from the previous iteration (see Section [5.1](https://arxiv.org/html/2510.09378v1#S5.SS1 "5.1 Optimization strategies ‣ 5 Experiment details ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). We then take a second-order Taylor approximation around the cross-entropy loss on the Taylorized model, also around the current model parameters. We use Muon (Jordan et al., [2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)) with batch size binnerb\_{inner} to minimize the Taylorized loss. We take NN steps of Muon and then update the model parameters using a line search. We refer to the global batch size (in number of sequences) as b\=N×binnerb=N\\times b\_{inner} since this is the total amount of data seen per parameter update on the true model. We use binner\=32b\_{inner}=32 for the 45M models and binner\=128b\_{inner}=128 for the 150M models with sequence length 1024, and vary NN to control the overall batch size bb. We start all runs from the same AdamW post-warmup checkpoint. To compute the necessary Taylor approximations we use the neural-tangents library from Novak et al. ([2020](https://arxiv.org/html/2510.09378v1#bib.bib27)). See Algorithm [1](https://arxiv.org/html/2510.09378v1#algorithm1 "Algorithm 1 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") for details.

##### Upper bound for Gauss-Newton method

In our experiments, we use Muon (Jordan et al., [2024b](https://arxiv.org/html/2510.09378v1#bib.bib13)) with batch size binnerb\_{inner} to solve the least squares problem in Algorithm [1](https://arxiv.org/html/2510.09378v1#algorithm1 "Algorithm 1 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). Therefore, Muon with batch size binnerb\_{inner} trained on the true model and loss marks the upper bound for the Gauss-Newton method: since Muon in our method optimizes over the respective Taylor approximations, it is upper bounded by the performance of Muon with the same batch size on the true model and loss. We include results for Muon with batch size binnerb\_{inner} in order to judge the relative performance of the Gauss-Newton method.

##### GN-prox-linear

For each training step, we minimize the loss of the first order Taylor expansion of the model around the current parameters (as mentioned in Algorithm [2](https://arxiv.org/html/2510.09378v1#algorithm2 "Algorithm 2 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). This method evaluates whether incorporating higher-order terms in the loss function yields improvements compared to the Gauss–Newton method. The results for this method are discussed in Section [6.2](https://arxiv.org/html/2510.09378v1#S6.SS2 "6.2 GN-prox-linear method ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

### 5.1 Optimization strategies

We perform extensive hyperparameter sweeps, learning rate scheduling strategies, and regularization strategies to test the limits of the Gauss-Newton method.

##### Learning rate schedules

We experiment with three learning rate schedules for Gauss-Newton runs, which we refer to as “global cosine,” “global+inner cosine,” and “constant+inner cosine.” We depict each learning rate schedule in Figure [4](https://arxiv.org/html/2510.09378v1#A3.F4 "Figure 4 ‣ C.1 Learning rate schedules ‣ Appendix C Additional details on optimization strategies ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

##### Regularization

We experiment with several types of regularization strategies to improve the stability of the Gauss-Newton runs at high learning rate. These fall into two categories of inner optimization and outer optimization regularization strategies. For inner optimization strategies (regularization involving the inner optimization loop to solve the least squares problem), we add weight decay to the optimizer as well as a weight decay term to the loss, which adds regularization on the ℓ2\\ell\_{2} norm of the magnitude of the parameter update. For outer optimization, we experiment with line search to control the size of the parameter update.

##### Inner optimizer

We note that the choice of inner optimizer has a significant impact on the upper bound of the Gauss-Newton method; we consider our results with respect to the performance of the inner optimizer. Regardless, we find that Muon outperforms AdamW as the inner optimizer for the Gauss-Newton runs.

##### Takeaways from optimization strategies

We found that learning rate schedule and line search had major impact on the stability of training for the Gauss-Newton method. As for learning rate schedules, we find that the global cosine schedule outperforms the global+inner cosine schedule at small to medium batch size, but the constant+inner cosine schedule can be helpful for runs at large batch size for Gauss-Newton. Additionally, we found line search to be essential for stable convergence for Gauss-Newton runs. However, we found that when using line search, it helps to set the initial parameters for the next inner minimization to be the pre-linesearch parameters from the previous step. These findings coincide with those of Martens ([2010](https://arxiv.org/html/2510.09378v1#bib.bib19)), which finds that sharing information across iterations and backtracking improve performance of a conjugate gradient-based Hessian-free optimization strategy. The importance of inner optimizer and sharing information across iterations seem to imply that we are not finding the precise Gauss-Newton update at each step – it is possible that with further optimization the Gauss-Newton method could achieve even better performance.

## 6 Experiments

### 6.1 Gauss-Newton Experiments

#### 6.1.1 Iteration complexity

We measure the iteration complexity of each method by measuring the number of steps it takes to reach loss 3.25 with extremely large batch size. Specifically, for each method, we use a batch size significantly beyond that method’s critical batch size (McCandlish et al., [2018](https://arxiv.org/html/2510.09378v1#bib.bib24); Shallue et al., [2019](https://arxiv.org/html/2510.09378v1#bib.bib31)) such that further increasing batch size does not reduce the number of training steps needed to achieve a given performance.<sup>2</sup> Following our critical batch size findings in Section [6.1.2](https://arxiv.org/html/2510.09378v1#S6.SS1.SSS2 "6.1.2 Batch size scaling ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"), we use a batch size of 40M tokens for AdamW and Muon as gains disappear almost completely beyond this amount, and a batch size of 240M tokens for SOAP and Gauss-Newton. We choose this threshold and these batch sizes to enter the regime in which additional batch size increases no longer reduce the required steps, while keeping runs feasible.

We find that the Gauss-Newton method can make fast progress in the large batch size regime, particularly in the first few steps of training. After 10 steps, the loss for the Gauss-Newton model is below 3.75, while other methods have made marginal progress from the starting loss. The optimal learning rates for AdamW, Muon, and SOAP lead to initial instability but faster convergence overall – see Appendix [G](https://arxiv.org/html/2510.09378v1#A7 "Appendix G Details on hyperparameter tuning ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") for details on choosing hyperparameters. The Gauss-Newton method is able to reach loss 3.25 in 54 steps, a 5.4x gain over SOAP and 16x gain over Muon. The results are shown in Figure [1](https://arxiv.org/html/2510.09378v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton").

#### 6.1.2 Batch size scaling

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/x2.png)

Figure 2: Left: Batch size vs final validation loss for models trained for Chinchilla-optimal number of tokens. The dotted line marks the loss achieved by a model trained with Muon with batch size 128k. This represents the upper bound of performance for our Gauss-Newton method. Right: Critical batch size scaling. The dotted line marks the optimal scaling trend, where no sample efficiency is lost as batch size increases.

While iteration complexity captures a purely sequential perspective of training efficiency, it is also important to consider the sample efficiency. In an ideal setting, the number of samples seen at each step would vary proportionally to the number of steps, such that there is always a constant number of samples required overall. However, it is known that sample efficiency is lost once the batch size is scaled past a given method’s critical batch size (McCandlish et al., [2018](https://arxiv.org/html/2510.09378v1#bib.bib24); Shallue et al., [2019](https://arxiv.org/html/2510.09378v1#bib.bib31)). That is, the total number of samples needed to achieve a given loss will grow once the critical batch size is exceeded. Therefore, we study the batch size scaling behavior of the Gauss-Newton method to understand how much we lose in sample and computational efficiency when we minimize the number of sequential iterations. We run experiments in two settings: first, we train models for a fixed amount of data over a range of batch sizes and measure the final loss. Second, we measure how the number of steps to achieve a given loss changes with increasing batch size. We run experiments for 45M- and 150M-parameter models; see Appendix [F](https://arxiv.org/html/2510.09378v1#A6 "Appendix F Experiments on 45M parameter models ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") for results at 45M-parameter scale.

##### Training for fixed token count

We train 150M-parameter models for 3B tokens following Chinchilla-optimal scaling laws (Hoffmann et al., [2022](https://arxiv.org/html/2510.09378v1#bib.bib9)), ranging batch size from 1.2M to 120M tokens. We observe similar performance between SOAP and Gauss-Newton up to batch size 4M, while substantial gains are achieved by Gauss-Newton for larger batch size (Figure [2](https://arxiv.org/html/2510.09378v1#S6.F2 "Figure 2 ‣ 6.1.2 Batch size scaling ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). Of existing methods, SOAP performs best, followed by Muon and then AdamW. Especially noteworthy is the performance of the Gauss-Newton method at batch size 120M, which uses only 20 steps of optimization. Here we are able to achieve loss 3.45 with Gauss-Newton. For comparison, AdamW achieves loss 3.4 with batch size 1.2M, and degrades to loss above 4.4 with batch size 120M.

##### Training to reach a target validation loss

Following the methodology of Zhang et al. ([2025](https://arxiv.org/html/2510.09378v1#bib.bib38)), we plot the number of steps required for each optimization method to reach the target validation loss of 3.4 as a function of batch size. The point at which the curve for each model plateaus defines its respective critical batch size (McCandlish et al., [2018](https://arxiv.org/html/2510.09378v1#bib.bib24); Shallue et al., [2019](https://arxiv.org/html/2510.09378v1#bib.bib31); Jain et al., [2018b](https://arxiv.org/html/2510.09378v1#bib.bib11)). We find that AdamW levels off near batch size of 4M with little further reduction. SOAP and Muon continue to decrease up to batch size of 12M but with diminishing reductions, and show little additional decrease by 40M. Meanwhile, the Gauss-Newton method continues to decrease through 40M, indicating better sample efficiency at large batch sizes.

### 6.2 GN-prox-linear method

We define a variation of our method that corresponds to another convex problem that retains the full loss function on the linearized model instead of using the second-order approximation. This method follows the same procedure as the Gauss-Newton method in Section [4](https://arxiv.org/html/2510.09378v1#S4 "4 Full second-order optimization ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") but directly minimizes the loss on the linearized model, denoted by ℒ~θt(θ)\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta):

<table id="S6.Ex9"><tbody><tr><td></td><td><math alttext="\theta^{*}=\text{argmin}_{\theta}\text{ }\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)" display="block" id="S6.Ex9.m1" intent=":literal"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><mrow><msub><mtext>argmin</mtext><mi>θ</mi></msub><mo lspace="0em" rspace="0em"></mo><mtext>&nbsp;</mtext><mo lspace="0em" rspace="0em"></mo><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta^{*}=\text{argmin}_{\theta}\text{ }\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)</annotation></semantics></math></td><td></td></tr></tbody></table>

See Algorithm [2](https://arxiv.org/html/2510.09378v1#algorithm2 "Algorithm 2 ‣ 3 Background on existing optimizers ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") for details. Note that for any convex loss function (including cross-entropy loss), the above optimization problem is still convex. Moreover, this problem is related to the richly studied literature of kernelized classification (Shalev-Shwartz & Ben-David, [2014](https://arxiv.org/html/2510.09378v1#bib.bib30)) (albeit with cross-entropy loss, instead of the max-margin loss). This GN-prox-linear method is notably not a second-order method. Rather, it allows us to study the effect of the higher order terms of the loss as compared to the Gauss-Newton update rule.

We train in the same Chinchilla-optimal setting on 150M parameter models and perform the same hyperparameter sweeps as for Gauss-Newton (See Appendix [G](https://arxiv.org/html/2510.09378v1#A7 "Appendix G Details on hyperparameter tuning ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). We find that the inclusion of higher order loss terms has little effect on performance as compared to Gauss-Newton; results are shown in Figure [3](https://arxiv.org/html/2510.09378v1#S6.F3 "Figure 3 ‣ 6.3 Layerwise Gauss-Newton ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). However, unlike Gauss-Newton, we found that the global cosine schedule for the inner optimizer outperformed the constant+inner cosine schedule. Additionally, line search was not necessary for the GN-prox-linear method.

### 6.3 Layerwise Gauss-Newton

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/x3.png)

Figure 3: Left: Comparison of Gauss-Newton to the layerwise implementation for Chinchilla-optimal token count for 150M parameter models. The layerwise method achieves almost matching performance to that of the full Gauss-Newton. Right: The Gauss-Newton update closely matches the GN-prox-linear method that has access to higher order loss terms.

Many existing second-order optimizers use layerwise approximations of the Hessian for computational and memory feasibility. This prompts a further study on whether the full Hessian is necessary to achieve the performance gains discussed in Section [5](https://arxiv.org/html/2510.09378v1#S5 "5 Experiment details ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). Specifically, we want to understand the importance of the cross-layer Hessian information.

We define a layerwise version of our Gauss-Newton method, in which we take a Taylor expansion around each model layer and optimize the second-order Taylor expansion of the loss separately for each layer.

Formally, let θl,t\\theta\_{l,t} be the set of parameters at time tt for layer ll of the network. For layer ll, define fθl,t(1)(θl)f^{(1)}\_{\\theta\_{l,t}}(\\theta\_{l}) as the first-order Taylor expansion of ff with respect to only the parameters θl\\theta\_{l}, expanded around the current parameters θl,t\\theta\_{l,t}, while keeping the parameters of all other layers fixed at their current values.

At each timestep tt, take ℒ~θl,t(2)(θl)\\mathcal{\\widetilde{L}}^{(2)}\_{\\theta\_{l,t}}(\\theta\_{l}) to be the second-order Taylor expansion of the loss on fθl,t(1)(θl)f\_{\\theta\_{l,t}}^{(1)}(\\theta\_{l}). Then for each layer, we solve for

<table id="S6.Ex10"><tbody><tr><td></td><td><math alttext="{\theta_{l,t+1}}=\text{argmin}_{\theta_{l}}\mathcal{\widetilde{L}}^{(2)}_{\theta_{l,t}}(\theta_{l})" display="block" id="S6.Ex10.m1" intent=":literal"><semantics><mrow><msub><mi>θ</mi><mrow><mi>l</mi><mo>,</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub><mo>=</mo><mrow><msub><mtext>argmin</mtext><msub><mi>θ</mi><mi>l</mi></msub></msub><mo lspace="0em" rspace="0em"></mo><msubsup><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mrow><mi>l</mi><mo>,</mo><mi>t</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>l</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">{\theta_{l,t+1}}=\text{argmin}_{\theta_{l}}\mathcal{\widetilde{L}}^{(2)}_{\theta_{l,t}}(\theta_{l})</annotation></semantics></math></td><td></td></tr></tbody></table>

After independently computing updates for each layer, we merge the updated layer parameters. We then apply a line search over the merged parameter set to obtain the final parameter update at step tt.

Due to compute costs, we train 150M parameter layerwise Gauss-Newton models only for the largest three batch sizes. We follow the same training setting for fixed token count as specified in Section [6.1.2](https://arxiv.org/html/2510.09378v1#S6.SS1.SSS2 "6.1.2 Batch size scaling ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). For layerwise experiments we set hyperparameters to match those of the best Gauss-Newton configuration at each batch size. However, we include smaller step size options for the line search as we find this is necessary for stable convergence. We provide more details on line search step sizes in Appendix [C](https://arxiv.org/html/2510.09378v1#A3 "Appendix C Additional details on optimization strategies ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton"). We find that the layerwise Gauss-Newton method also achieves comparable performance through batch size of 40M tokens (Figure [3](https://arxiv.org/html/2510.09378v1#S6.F3 "Figure 3 ‣ 6.3 Layerwise Gauss-Newton ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). We additionally train a layerwise Gauss-Newton model with batch size of 120M tokens to loss 3.25 to compare its iteration complexity to that of full Gauss-Newton (See Sec [6.1.1](https://arxiv.org/html/2510.09378v1#S6.SS1.SSS1 "6.1.1 Iteration complexity ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). We find that the layerwise Gauss-Newton takes only 1.4x more steps to reach the target loss compared to the full Gauss-Newton method and provides a 3.4x gain over SOAP (Figure [1](https://arxiv.org/html/2510.09378v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")).

## 7 Discussion and Conclusion

In this work, we study whether full second-order optimization – specifically, using the full Gauss-Newton matrix as a preconditioner – can offer further benefits for training large language models as compared to existing methods. In particular, we focus on the large batch size regime, following Jain et al. ([2018a](https://arxiv.org/html/2510.09378v1#bib.bib10)) and Zhang et al. ([2019](https://arxiv.org/html/2510.09378v1#bib.bib37)) which show that the benefits of preconditioning may not appear at small batch size. While our current implementation is roughly 4-5x slower than standard training (e.g. with AdamW or Muon), we view this as a proof of concept demonstrating the potential of exact second-order methods. Our results indicate that further development in second-order methods could lead to substantial benefits in convergence and ability to scale to larger batch size.

While we perform extensive hyperparameter sweeps and regularization strategies, we acknowledge that there could be other optimization strategies to further improve the performance of the Gauss-Newton method. In addition, our work is limited to applying the inverse of the Gauss-Newton matrix as the preconditioner (G−1G^{-1}). There may be better ways to apply full second-order optimization for large language models. We encourage future work in this area and hope our findings are informative.

We also compare Gauss-Newton to the GN-prox-linear method to study whether there is benefit to including higher order loss terms beyond second-order. Our results suggest that Gauss-Newton can achieve performance similar to this method, indicating that higher-order loss terms are not necessary to achieve gains in performance over current methods. In addition, our layerwise Gauss-Newton experiments suggest that better approximations to the per-layer Hessian may be sufficient to achieve substantial performance benefits over current methods. We encourage future work in developing computationally efficient and practical optimization methods in this direction.

## 8 Acknowledgements

We thank Jonathan Frankle and Kwangjun Ahn for helpful discussions. SK, NA, and DM acknowledge support from the Office of Naval Research under award N0001422-1-2377 and the National Science Foundation Grant under award #IIS 2229881. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. NA, NV, and DM are supported by a Simons Investigator Fellowship, NSF grant DMS-2134157, DARPA grant W911NF2010021,and DOE grant DE-SC0022199.

## References

- Anil et al. (2021) Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and Yoram Singer. Towards practical second order optimization for deep learning, 2021. URL [https://openreview.net/forum?id=Sc8cY4Jpi3s](https://openreview.net/forum?id=Sc8cY4Jpi3s).
- Bernstein & Newhouse (2024) Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology, 2024. URL [https://arxiv.org/abs/2409.20325](https://arxiv.org/abs/2409.20325).
- Burke (1985) James V. Burke. Descent methods for composite nondifferentiable optimization problems. _Math. Program._, 33(3):260–279, December 1985. ISSN 0025-5610. doi: 10.1007/BF01584377. URL [https://doi.org/10.1007/BF01584377](https://doi.org/10.1007/BF01584377).
- Cho et al. (2015) Minhyung Cho, Chandra Dhir, and Jaehyung Lee. Hessian-free optimization for learning deep multidimensional recurrent neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015. URL [https://proceedings.neurips.cc/paper_files/paper/2015/file/a86c450b76fb8c371afead6410d55534-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/a86c450b76fb8c371afead6410d55534-Paper.pdf).
- Drusvyatskiy (2017) Dmitriy Drusvyatskiy. The proximal point method revisited, 2017. URL [https://arxiv.org/abs/1712.06038](https://arxiv.org/abs/1712.06038).
- Duchi et al. (2011) John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of Machine Learning Research_, 12(61):2121–2159, 2011. URL [http://jmlr.org/papers/v12/duchi11a.html](http://jmlr.org/papers/v12/duchi11a.html).
- Garcia et al. (2023) Jezabel R Garcia, Federica Freddi, Stathi Fotiadis, Maolin Li, Sattar Vakili, Alberto Bernacchia, and Guillaume Hennequin. Fisher-legendre (fishleg) optimization of deep neural networks. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=c9lAOPvQHS](https://openreview.net/forum?id=c9lAOPvQHS).
- Gupta et al. (2018) Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization, 2018. URL [https://arxiv.org/abs/1802.09568](https://arxiv.org/abs/1802.09568).
- Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).
- Jain et al. (2018a) Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent for least squares regression, 2018a. URL [https://arxiv.org/abs/1704.08227](https://arxiv.org/abs/1704.08227).
- Jain et al. (2018b) Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification. _Journal of machine learning research_, 18(223):1–42, 2018b.
- Jordan et al. (2024a) Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, You Jiacheng, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024a. URL [https://github.com/KellerJordan/modded-nanogpt](https://github.com/KellerJordan/modded-nanogpt).
- Jordan et al. (2024b) Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024b. URL [https://kellerjordan.github.io/posts/muon/](https://kellerjordan.github.io/posts/muon/).
- Kasimbeg et al. (2025) Priya Kasimbeg, Frank Schneider, Runa Eschenhagen, Juhan Bae, Chandramouli Shama Sastry, Mark Saroufim, Feng Boyuan, Less Wright, Edward Z. Yang, Zachary Nado, Sourabh Medapati, Philipp Hennig, Michael Rabbat, and George E. Dahl. Accelerating neural network training: An analysis of the AlgoPerf competition. In _The Thirteenth International Conference on Learning Representations_, 2025. URL [https://openreview.net/forum?id=CtM5xjRSfm](https://openreview.net/forum?id=CtM5xjRSfm).
- Kingma & Ba (2017) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. URL [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).
- Liu et al. (2024) Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-order optimizer for language model pre-training, 2024. URL [https://arxiv.org/abs/2305.14342](https://arxiv.org/abs/2305.14342).
- Liu et al. (2025) Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu, Yuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu, Yutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training, 2025. URL [https://arxiv.org/abs/2502.16982](https://arxiv.org/abs/2502.16982).
- Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
- Martens (2010) James Martens. Deep learning via hessian-free optimization. In _Proceedings of the 27th International Conference on International Conference on Machine Learning_, ICML’10, pp. 735–742, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077.
- Martens (2020) James Martens. New insights and perspectives on the natural gradient method. _Journal of Machine Learning Research_, 21(146):1–76, 2020. URL [http://jmlr.org/papers/v21/17-678.html](http://jmlr.org/papers/v21/17-678.html).
- Martens & Sutskever (2011a) James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_, ICML’11, pp. 1033–1040, Madison, WI, USA, 2011a. Omnipress. ISBN 9781450306195.
- Martens & Sutskever (2011b) James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization. In _Proceedings of the 28th International Conference on International Conference on Machine Learning_, ICML’11, pp. 1033–1040, Madison, WI, USA, 2011b. Omnipress. ISBN 9781450306195.
- Martens & Sutskever (2012) James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free optimization. In _Neural Networks: Tricks of the Trade: Second Edition_, pp. 479–535. Springer, 2012.
- McCandlish et al. (2018) Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training, 2018. URL [https://arxiv.org/abs/1812.06162](https://arxiv.org/abs/1812.06162).
- Morwani et al. (2024) Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham Kakade, and Lucas Janson. A new perspective on shampoo’s preconditioner, 2024. URL [https://arxiv.org/abs/2406.17748](https://arxiv.org/abs/2406.17748).
- Nesterov (2018) Yurii Nesterov. _Lectures on Convex Optimization_, volume 137 of _Springer Optimization and Its Applications_. Springer, 2018. ISBN 978-3-319-91577-7. doi: 10.1007/978-3-319-91578-4.
- Novak et al. (2020) Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein, and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In _International Conference on Learning Representations_, 2020. URL [https://github.com/google/neural-tangents](https://github.com/google/neural-tangents).
- Osawa et al. (2023) Kazuki Osawa, Satoki Ishikawa, Rio Yokota, Shigang Li, and Torsten Hoefler. Asdl: A unified interface for gradient preconditioning in pytorch, 2023. URL [https://arxiv.org/abs/2305.04684](https://arxiv.org/abs/2305.04684).
- Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 21(140):1–67, 2020. URL [http://jmlr.org/papers/v21/20-074.html](http://jmlr.org/papers/v21/20-074.html).
- Shalev-Shwartz & Ben-David (2014) Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, USA, 2014. ISBN 1107057132.
- Shallue et al. (2019) Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on neural network training, 2019. URL [https://arxiv.org/abs/1811.03600](https://arxiv.org/abs/1811.03600).
- Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).
- Vinyals & Povey (2012) Oriol Vinyals and Daniel Povey. Krylov subspace descent for deep learning. In _Artificial intelligence and statistics_, pp. 1261–1268. PMLR, 2012.
- Vyas et al. (2025) Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade. Soap: Improving and stabilizing shampoo using adam, 2025. URL [https://arxiv.org/abs/2409.11321](https://arxiv.org/abs/2409.11321).
- Wang et al. (2025) Sifan Wang, Ananyae Kumar Bhartari, Bowen Li, and Paris Perdikaris. Gradient alignment in physics-informed neural networks: A second-order optimization perspective, 2025. URL [https://arxiv.org/abs/2502.00604](https://arxiv.org/abs/2502.00604).
- Yao et al. (2021) Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, and Michael W. Mahoney. Adahessian: An adaptive second order optimizer for machine learning, 2021. URL [https://arxiv.org/abs/2006.00719](https://arxiv.org/abs/2006.00719).
- Zhang et al. (2019) Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, Christopher J. Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model, 2019. URL [https://arxiv.org/abs/1907.04164](https://arxiv.org/abs/1907.04164).
- Zhang et al. (2025) Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean Foster, and Sham Kakade. How does critical batch size scale in pre-training?, 2025. URL [https://arxiv.org/abs/2410.21676](https://arxiv.org/abs/2410.21676).
- Zhao et al. (2025) Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, and Sham Kakade. Deconstructing what makes a good optimizer for language models, 2025. URL [https://arxiv.org/abs/2407.07972](https://arxiv.org/abs/2407.07972).

## Appendix A Proof of equivalence to Gauss-Newton method

In our method, we compute the Gauss-Newton update by minimizing the second-order Taylor approximation of the loss around the first-order Taylor approximation of the function.

Taking the first-order Taylor approximation of ff around θt\\theta\_{t},

<table id="A1.Ex11"><tbody><tr><td></td><td><math alttext="f^{(1)}_{\theta_{t}}(\theta,x)\coloneqq f(\theta_{t},x)+\nabla f(\theta_{t},x)^{\mathsf{T}}(\theta-\theta_{t})" display="block" id="A1.Ex11.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≔</mo><mrow><mrow><mi>f</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">f^{(1)}_{\theta_{t}}(\theta,x)\coloneqq f(\theta_{t},x)+\nabla f(\theta_{t},x)^{\mathsf{T}}(\theta-\theta_{t})</annotation></semantics></math></td><td></td></tr></tbody></table>

Let ℒ~θt(θ)≔ℒ(fθt(1)(θ,x),y)\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta)\\coloneqq\\mathcal{L}(f^{(1)}\_{\\theta\_{t}}(\\theta,x),y) be the loss function on the Taylor expansion of ff around θt\\theta\_{t}.

Then the second-order Taylor approximation of ℒ~θt(θ)\\mathcal{\\widetilde{L}}\_{\\theta\_{t}}(\\theta) around θt\\theta\_{t} gives

<table id="A1.Ex12"><tbody><tr><td></td><td><math alttext="\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)\coloneqq\mathcal{\widetilde{L}}_{\theta_{t}}(\theta_{t})+\nabla\mathcal{\widetilde{L}}_{\theta_{t}}(\theta_{t})^{\mathsf{T}}(\theta-\theta_{t})+\frac{1}{2}(\theta-\theta_{t})^{\mathsf{T}}\nabla^{2}\mathcal{\widetilde{L}}_{\theta_{t}}(\theta_{t})(\theta-\theta_{t})" display="block" id="A1.Ex12.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>≔</mo><mrow><mrow><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo rspace="0.167em">∇</mo><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub></mrow><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0.167em" rspace="0em"></mo><mrow><msup><mo rspace="0.167em">∇</mo><mn>2</mn></msup><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)\coloneqq\mathcal{\widetilde{L}}_{\theta_{t}}(\theta_{t})+\nabla\mathcal{\widetilde{L}}_{\theta_{t}}(\theta_{t})^{\mathsf{T}}(\theta-\theta_{t})+\frac{1}{2}(\theta-\theta_{t})^{\mathsf{T}}\nabla^{2}\mathcal{\widetilde{L}}_{\theta_{t}}(\theta_{t})(\theta-\theta_{t})</annotation></semantics></math></td><td></td></tr></tbody></table>

Denoting z\=fθt(1)(θ,x)z=f^{(1)}\_{\\theta\_{t}}(\\theta,x) and applying the chain rule for the gradients, we have

<table id="A1.Ex13"><tbody><tr><td></td><td><math alttext="\nabla\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)=\mathcal{L}^{\prime}(f^{(1)}_{\theta_{t}}(\theta,x),y)\nabla f^{(1)}_{\theta_{t}}(\theta,x)" display="block" id="A1.Ex13.m1" intent=":literal"><semantics><mrow><mrow><mrow><mo rspace="0.167em">∇</mo><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msup><mi>ℒ</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em"></mo><mrow><mo rspace="0.167em">∇</mo><msubsup><mi>f</mi><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\nabla\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)=\mathcal{L}^{\prime}(f^{(1)}_{\theta_{t}}(\theta,x),y)\nabla f^{(1)}_{\theta_{t}}(\theta,x)</annotation></semantics></math></td><td></td></tr></tbody></table>

<table id="A1.Ex14"><tbody><tr><td></td><td><math alttext="\nabla^{2}\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)=\nabla f^{(1)}_{\theta_{t}}(\theta,x)^{\top}\nabla^{2}_{z}\mathcal{L}(f^{(1)}_{\theta_{t}}(\theta,x),y)\nabla f^{(1)}_{\theta_{t}}(\theta,x)" display="block" id="A1.Ex14.m1" intent=":literal"><semantics><mrow><mrow><mrow><msup><mo>∇</mo><mn>2</mn></msup><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mo rspace="0.167em">∇</mo><msubsup><mi>f</mi><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0.167em" rspace="0em"></mo><mrow><msubsup><mo rspace="0.167em">∇</mo><mi>z</mi><mn>2</mn></msubsup><mi>ℒ</mi></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>f</mi><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em"></mo><mrow><mo rspace="0.167em">∇</mo><msubsup><mi>f</mi><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\nabla^{2}\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)=\nabla f^{(1)}_{\theta_{t}}(\theta,x)^{\top}\nabla^{2}_{z}\mathcal{L}(f^{(1)}_{\theta_{t}}(\theta,x),y)\nabla f^{(1)}_{\theta_{t}}(\theta,x)</annotation></semantics></math></td><td></td></tr></tbody></table>

Then substituting and evaluating at θt\\theta\_{t}:

<table id="A1.Ex15"><tbody><tr><td></td><td><math alttext="\nabla\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)\rvert_{\theta=\theta_{t}}=\mathcal{L}^{\prime}(f(\theta_{t},x),y)\nabla f(\theta_{t},x)" display="block" id="A1.Ex15.m1" intent=":literal"><semantics><mrow><mo rspace="0.167em">∇</mo><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">|</mo><msub><mi></mi><mrow><mi>θ</mi><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><mi>ℒ</mi><msup><mi></mi><mo>′</mo></msup><mo stretchy="false">(</mo><mi>f</mi><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>y</mi><mo rspace="0.167em" stretchy="false">)</mo><mo rspace="0.167em">∇</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)\rvert_{\theta=\theta_{t}}=\mathcal{L}^{\prime}(f(\theta_{t},x),y)\nabla f(\theta_{t},x)</annotation></semantics></math></td><td></td></tr></tbody></table>

<table id="A1.Ex16"><tbody><tr><td></td><td><math alttext="\nabla^{2}\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)\rvert_{\theta=\theta_{t}}=\nabla_{\theta}f(\theta_{t},x)^{\top}\nabla^{2}_{z}\mathcal{L}(f(\theta_{t},x),y)\nabla_{\theta}f(\theta_{t},x)" display="block" id="A1.Ex16.m1" intent=":literal"><semantics><mrow><msup><mo>∇</mo><mn>2</mn></msup><msub><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub></msub><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">|</mo><msub><mi></mi><mrow><mi>θ</mi><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow></msub><mo>=</mo><mo rspace="0.167em">∇</mo><msub><mi></mi><mi>θ</mi></msub><mi>f</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mi></mi><mo>⊤</mo></msup><mo lspace="0.167em" rspace="0.167em">∇</mo><msup><mi></mi><mn>2</mn></msup><msub><mi></mi><mi>z</mi></msub><mi>ℒ</mi><mo stretchy="false">(</mo><mi>f</mi><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>,</mo><mi>y</mi><mo rspace="0.167em" stretchy="false">)</mo><mo rspace="0.167em">∇</mo><msub><mi></mi><mi>θ</mi></msub><mi>f</mi><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla^{2}\mathcal{\widetilde{L}}_{\theta_{t}}(\theta)\rvert_{\theta=\theta_{t}}=\nabla_{\theta}f(\theta_{t},x)^{\top}\nabla^{2}_{z}\mathcal{L}(f(\theta_{t},x),y)\nabla_{\theta}f(\theta_{t},x)</annotation></semantics></math></td><td></td></tr></tbody></table>

Then

<table id="A7.EGx1"><tbody id="A1.Ex17"><tr><td></td><td><math alttext="\displaystyle\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)" display="inline" id="A1.Ex17.m1" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)</annotation></semantics></math></td><td><math alttext="\displaystyle=\mathcal{L}(f(\theta_{t},x),y)+\mathcal{L}^{\prime}(f(\theta_{t},x),y)\nabla f(\theta_{t},x)^{\mathsf{T}}(\theta-\theta_{t})" display="inline" id="A1.Ex17.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><mi>ℒ</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msup><mi>ℒ</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em"></mo><mrow><mo rspace="0.167em">∇</mo><mi>f</mi></mrow><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\mathcal{L}(f(\theta_{t},x),y)+\mathcal{L}^{\prime}(f(\theta_{t},x),y)\nabla f(\theta_{t},x)^{\mathsf{T}}(\theta-\theta_{t})</annotation></semantics></math></td><td></td></tr></tbody><tbody id="A1.Ex18"><tr><td></td><td></td><td><math alttext="\displaystyle\quad+\frac{1}{2}(\theta-\theta_{t})^{\mathsf{T}}\nabla_{\theta}f(\theta_{t},x)^{\top}\nabla^{2}_{z}\mathcal{L}(f(\theta_{t},x),y)\nabla_{\theta}f(\theta_{t},x)(\theta-\theta_{t})" display="inline" id="A1.Ex18.m1" intent=":literal"><semantics><mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0.167em" rspace="0em"></mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><mi>f</mi></mrow><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo>⊤</mo></msup><mo lspace="0.167em" rspace="0em"></mo><mrow><msubsup><mo rspace="0.167em">∇</mo><mi>z</mi><mn>2</mn></msubsup><mi>ℒ</mi></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em"></mo><mrow><msub><mo rspace="0.167em">∇</mo><mi>θ</mi></msub><mi>f</mi></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\quad+\frac{1}{2}(\theta-\theta_{t})^{\mathsf{T}}\nabla_{\theta}f(\theta_{t},x)^{\top}\nabla^{2}_{z}\mathcal{L}(f(\theta_{t},x),y)\nabla_{\theta}f(\theta_{t},x)(\theta-\theta_{t})</annotation></semantics></math></td><td></td></tr></tbody></table>

Let gg denote the gradient of the loss at θt\\theta\_{t}, i.e. ℒ′(f(θt,x),y)∇θf(θt,x)\\mathcal{L}^{\\prime}(f(\\theta\_{t},x),y)\\nabla\_{\\theta}f(\\theta\_{t},x). Let G denote the Gauss-Newton term ∇θf(θt,x)⊤∇z2ℒ(f(θt,x),y)∇θf(θt,x)\\nabla\_{\\theta}f(\\theta\_{t},x)^{\\top}\\nabla^{2}\_{z}\\mathcal{L}(f(\\theta\_{t},x),y)\\nabla\_{\\theta}f(\\theta\_{t},x). Then we can write

<table id="A1.Ex19"><tbody><tr><td></td><td><math alttext="\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)=\mathcal{L}(f(\theta_{t},x),y)+g(\theta-\theta_{t})+\frac{1}{2}(\theta-\theta_{t})^{\mathsf{T}}G(\theta-\theta_{t})" display="block" id="A1.Ex19.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mover accent="true"><mi>ℒ</mi><mo>~</mo></mover><msub><mi>θ</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mi>ℒ</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>f</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em"></mo><msup><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mi>𝖳</mi></msup><mo lspace="0em" rspace="0em"></mo><mi>G</mi><mo lspace="0em" rspace="0em"></mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo>−</mo><msub><mi>θ</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{\widetilde{L}}^{(2)}_{\theta_{t}}(\theta)=\mathcal{L}(f(\theta_{t},x),y)+g(\theta-\theta_{t})+\frac{1}{2}(\theta-\theta_{t})^{\mathsf{T}}G(\theta-\theta_{t})</annotation></semantics></math></td><td></td></tr></tbody></table>

Since the Gauss-Newton matrix is PSD, we can find θ∗\\theta^{\*} to minimize h(2)h^{(2)} by setting its gradient to zero:

<table id="A1.Ex20"><tbody><tr><td></td><td><math alttext="g+(\theta^{*}-\theta_{0})G=0" display="block" id="A1.Ex20.m1" intent=":literal"><semantics><mrow><mrow><mi>g</mi><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>−</mo><msub><mi>θ</mi><mn>0</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em"></mo><mi>G</mi></mrow></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">g+(\theta^{*}-\theta_{0})G=0</annotation></semantics></math></td><td></td></tr></tbody></table>

which results in the update rule

<table id="A1.Ex21"><tbody><tr><td></td><td><math alttext="\theta^{*}=\theta_{0}-G^{-1}g" display="block" id="A1.Ex21.m1" intent=":literal"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup><mo>=</mo><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>−</mo><mrow><msup><mi>G</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em"></mo><mi>g</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta^{*}=\theta_{0}-G^{-1}g</annotation></semantics></math></td><td></td></tr></tbody></table>

## Appendix B Additional related work

Also related to our work are optimizers that leverage diagonal approximations to the Hessian, such as AdaHessian (Yao et al., [2021](https://arxiv.org/html/2510.09378v1#bib.bib36)) and Sophia (Liu et al., [2024](https://arxiv.org/html/2510.09378v1#bib.bib16)). These propose lightweight approximations to the diagonal Hessian rather than layerwise approximations as in Shampoo and SOAP. However, Zhao et al. ([2025](https://arxiv.org/html/2510.09378v1#bib.bib39)) show that Sophia performs comparably to AdamW, suggesting the need to go beyond the diagonal Hessian.

## Appendix C Additional details on optimization strategies

### C.1 Learning rate schedules

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/x4.png)

Figure 4: Three learning rate schedules used for the Gauss-Newton and GN-prox-linear runs. From left to right: global cosine, global+inner cosine, and constant+inner cosine. Each inner cosine period lasts the duration of the optimization over the current Taylor expansion; outer step refers to each parameter update on the model.

We experiment with three learning rate schedules: “global cosine,” “global+inner cosine,” and “constant+inner cosine.” From preliminary experiments we find that global+inner cosine did not generally outperform the global cosine and constant+inner cosine options, so we do not use global+inner cosine in our main experiments.

### C.2 Line search step sizes

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/x5.png)

Figure 5: Resulting step sizes used from line search for Gauss-Newton and layerwise Gauss-Newton.

We find that line search is necessary for the performance of Gauss-Newton and layerwise Gauss-Newton runs. For the line search, we evaluate the true model loss on step sizes 2−i22^{\\frac{-i}{2}} with i∈{0,…,4}i\\in\\{0,\\dots,4\\} for full Gauss-Newton and i∈{0,…,9}i\\in\\{0,\\dots,9\\} for layerwise Gauss-Newton. Figure [5](https://arxiv.org/html/2510.09378v1#A3.F5 "Figure 5 ‣ C.2 Line search step sizes ‣ Appendix C Additional details on optimization strategies ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") shows the step sizes at each step for iteration complexity runs (See Section [6.1.1](https://arxiv.org/html/2510.09378v1#S6.SS1.SSS1 "6.1.1 Iteration complexity ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")).

## Appendix D Model details

Table 1: Model configurations for the 45M and 150M parameter LLaMA-based models.

## Appendix E Compute resources

All 45M parameter model runs are trained on 1 Nvidia 80GB H100. 45M runs for AdamW, Muon, and SOAP trained for 2-3 hours and Gauss-Newton and GN-prox-linear trained for 1-2 days. 150M parameter model runs for batch size scaling experiments for AdamW, Muon, and SOAP are each trained using 1 H100 for roughly 6-20 hours. 150M parameter model runs for batch size scaling experiments for Gauss-Newton and GN-prox-linear are each trained for 1-3 days with 4 80GB Nvidia H100s using distributed data parallel (DDP). Layerwise Gauss-Newton runs each trained for 3-7 days with 4 H100s. Iteration complexity runs trained for 2-3 days for AdamW and Muon and 15-30 days for SOAP, Gauss-Newton, and Layerwise Gauss-Newton.

## Appendix F Experiments on 45M parameter models

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2510.09378v1/x6.png)

Figure 6: Left: Batch size vs final validation loss for 45M parameter models. All models are trained for 900M tokens on the C4 dataset (Raffel et al., [2020](https://arxiv.org/html/2510.09378v1#bib.bib29)) following Chinchilla scaling laws (Hoffmann et al., [2022](https://arxiv.org/html/2510.09378v1#bib.bib9)). The dotted line marks the loss achieved by a model trained with Muon with a batch size of 32k tokens. This represents the upper bound of performance for our Gauss-Newton and GN-prox-linear methods as we use Muon with batch size of 32k tokens as the inner optimizer to compute the parameter update in each step. Right: Critical batch size scaling for 45M parameter models. The dotted line marks the optimal scaling trend, where no sample efficiency is lost as batch size increases.

## Appendix G Details on hyperparameter tuning

Table 2: Hyperparameter configurations used for 45M models. Values in parentheses were not used for every sweep. For the critical batch size plot (Figure [6](https://arxiv.org/html/2510.09378v1#A6.F6 "Figure 6 ‣ Appendix F Experiments on 45M parameter models ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton") right) only the constant+EWA learning rate schedule was used. We start each run after an AdamW warmup of 5% for 45M parameter models; additional warmup refers to warmup starting from this checkpoint. For baselines, weight averaging is used only with the constant schedule. All Gauss-Newton runs without line search use weight averaging; runs with line search use no weight averaging. Inner loop warmup applies only to the constant+inner cosine schedule.

<table><tbody><tr><td colspan="2"><span>Shared Hyperparameters</span></td></tr><tr><td>Model Size</td><td>45M</td></tr><tr><td>Batch Size</td><td>400k, 1.2m, 4m, 12m, 40m</td></tr><tr><td>Context Length</td><td>1024</td></tr><tr><td colspan="2"><span>AdamW</span></td></tr><tr><td>Learning Rate</td><td>0.001, 0.003, 0.01, 0.03</td></tr><tr><td>Weight Decay</td><td>0, (0.001, 0.01)</td></tr><tr><td>Additional Warmup Fraction</td><td>0, (0.1)</td></tr><tr><td>Momentum <math alttext="\beta_{1}" display="inline" id="A7.T2.m1" intent=":literal"><semantics><msub><mi>β</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_{1}</annotation></semantics></math></td><td>0.9</td></tr><tr><td>Adam <math alttext="\beta_{2}" display="inline" id="A7.T2.m2" intent=":literal"><semantics><msub><mi>β</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\beta_{2}</annotation></semantics></math></td><td>0.95, 0.99, 0.999</td></tr><tr><td>LR Scheduler</td><td>constant+EWA, cosine</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T2.m3" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>0.9, 0.99</td></tr><tr><td colspan="2"><span>Muon</span></td></tr><tr><td>Learning Rate</td><td>0.03, 0.1, (0.3)</td></tr><tr><td>Additional Warmup Fraction</td><td>0</td></tr><tr><td>Momentum <math alttext="\mu" display="inline" id="A7.T2.m4" intent=":literal"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math></td><td>0.9, 0.95, 0.99</td></tr><tr><td>LR Scheduler</td><td>constant+EWA, cosine</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T2.m5" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>(0.3), 0.5, 0.7, (0.9)</td></tr><tr><td colspan="2"><span>SOAP</span></td></tr><tr><td>Preconditioning Frequency</td><td>1</td></tr><tr><td>Learning Rate</td><td>0.01, 0.03, (0.1)</td></tr><tr><td>Additional Warmup Fraction</td><td>0</td></tr><tr><td>Momentum <math alttext="\beta_{1}" display="inline" id="A7.T2.m6" intent=":literal"><semantics><msub><mi>β</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_{1}</annotation></semantics></math></td><td>0.7, 0.8, 0.9</td></tr><tr><td>Adam <math alttext="\beta_{2}" display="inline" id="A7.T2.m7" intent=":literal"><semantics><msub><mi>β</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\beta_{2}</annotation></semantics></math></td><td>0.7, 0.8, 0.9</td></tr><tr><td>LR Scheduler</td><td>constant+EWA, cosine</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T2.m8" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>(0.5), 0.7, 0.8, (0.9)</td></tr><tr><td colspan="2"><span>Gauss-Newton</span></td></tr><tr><td>Learning Rate</td><td>(0.001), 0.003, 0.01, 0.03, (0.1)</td></tr><tr><td>Inner Loop Warmup Fraction</td><td>0, (0.2)</td></tr><tr><td>Global Warmup Fraction</td><td>0, (0.2)</td></tr><tr><td>Momentum <math alttext="\mu" display="inline" id="A7.T2.m9" intent=":literal"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math></td><td>0.95</td></tr><tr><td>Optimizer weight decay</td><td>(0), 0.001</td></tr><tr><td>Loss weight decay</td><td>0, 0.01, (0.1)</td></tr><tr><td>Parameter weight decay</td><td>0, (0.01, 0.03, 0.1)</td></tr><tr><td>LR Scheduler</td><td>constant+inner cosine, global cosine</td></tr><tr><td>Linesearch</td><td>True, False</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T2.m10" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>(0.99), 0.999</td></tr></tbody></table>

Table 3: Hyperparameter configurations used for 150M models for batch size scaling experiments (Section [6.1.2](https://arxiv.org/html/2510.09378v1#S6.SS1.SSS2 "6.1.2 Batch size scaling ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")). For Muon, constant+EWA learning rate schedule was included in the sweep for the three largest batch sizes. For Gauss-Newton, due to high compute costs we hand-tune over the range of provided values rather than conducting the entire sweep at each batch size. Inner loop warmup applies only to the constant+inner cosine schedule. We start each run after an AdamW warmup of 5% for 150M parameter models; additional warmup refers to warmup starting from this checkpoint. For baselines, weight averaging is used only with the constant schedule. All Gauss-Newton runs without line search use weight averaging with τ\=0.999\\tau=0.999; runs with line search are default to no weight averaging or are swept with lower values of τ\\tau. For iteration complexity experiments (Section [6.1.1](https://arxiv.org/html/2510.09378v1#S6.SS1.SSS1 "6.1.1 Iteration complexity ‣ 6.1 Gauss-Newton Experiments ‣ 6 Experiments ‣ The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton")): For AdamW and Muon we take the best hyperparameters from sweeps at 40M batch size in the batch size experiments. For SOAP and Gauss-Newton we use the best hyperparameters from sweeps at 120M batch size and run a limited sweep over learning rate.

<table><tbody><tr><td colspan="2"><span>Shared Hyperparameters</span></td></tr><tr><td>Model Size</td><td>150M</td></tr><tr><td>Batch Size</td><td>1.2m, 4m, 12m, 40m, 120m</td></tr><tr><td>Context Length</td><td>1024</td></tr><tr><td colspan="2"><span>AdamW</span></td></tr><tr><td>Learning Rate</td><td>0.001, 0.003, 0.01</td></tr><tr><td>Weight Decay</td><td>0.001, 0.1</td></tr><tr><td>Additional Warmup Fraction</td><td>0, (0.1)</td></tr><tr><td>Momentum <math alttext="\beta_{1}" display="inline" id="A7.T3.m5" intent=":literal"><semantics><msub><mi>β</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_{1}</annotation></semantics></math></td><td>0.9</td></tr><tr><td>Adam <math alttext="\beta_{2}" display="inline" id="A7.T3.m6" intent=":literal"><semantics><msub><mi>β</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\beta_{2}</annotation></semantics></math></td><td>0.95, 0.99</td></tr><tr><td>LR Scheduler</td><td>cosine</td></tr><tr><td colspan="2"><span>Muon</span></td></tr><tr><td>Learning Rate</td><td>0.01, 0.03, 0.1</td></tr><tr><td>Weight Decay</td><td>0</td></tr><tr><td>Additional Warmup Fraction</td><td>0</td></tr><tr><td>Momentum <math alttext="\mu" display="inline" id="A7.T3.m7" intent=":literal"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math></td><td>0.9, 0.95, 0.99</td></tr><tr><td>LR Scheduler</td><td>cosine, (constant+EWA)</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T3.m8" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>0.7, 0.9</td></tr><tr><td colspan="2"><span>SOAP</span></td></tr><tr><td>Preconditioning Frequency</td><td>1</td></tr><tr><td>Learning Rate</td><td>0.01, 0.03, 0.1</td></tr><tr><td>Weight Decay</td><td>0</td></tr><tr><td>Additional Warmup Fraction</td><td>0</td></tr><tr><td>Momentum <math alttext="\beta_{1}" display="inline" id="A7.T3.m9" intent=":literal"><semantics><msub><mi>β</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_{1}</annotation></semantics></math></td><td>(0.7), 0.9, (0.95)</td></tr><tr><td>Adam <math alttext="\beta_{2}" display="inline" id="A7.T3.m10" intent=":literal"><semantics><msub><mi>β</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\beta_{2}</annotation></semantics></math></td><td>0.7, 0.9, 0.95</td></tr><tr><td>LR Scheduler</td><td>cosine, (constant+EWA)</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T3.m11" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>0.7, 0.9</td></tr><tr><td colspan="2"><span>Gauss-Newton</span></td></tr><tr><td>Inner Loop Learning Rate</td><td>0.003, 0.01, 0.03, 0.1</td></tr><tr><td>Inner Loop Warmup Fraction</td><td>0, 0.2</td></tr><tr><td>Global Warmup Fraction</td><td>0</td></tr><tr><td>Inner Loop Weight Decay</td><td>0, 0.01, 0.1</td></tr><tr><td>Optimizer Weight Decay</td><td>0.001</td></tr><tr><td>Momentum <math alttext="\mu" display="inline" id="A7.T3.m12" intent=":literal"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math></td><td>0.95</td></tr><tr><td>LR Scheduler</td><td>constant+inner cosine, global cosine</td></tr><tr><td>Linesearch</td><td>True, False</td></tr><tr><td>EWA Decay Rate <math alttext="\tau" display="inline" id="A7.T3.m13" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math></td><td>(0.9, 0.99), 0.999</td></tr></tbody></table>
