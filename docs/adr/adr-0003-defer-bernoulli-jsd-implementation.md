---
title: "ADR-0003: 暂缓为元学习损失函数实现独立伯努利JSD"
status: "Accepted"
date: "2025-10-06"
authors: "Ω Researcher, Tiny-ONN 课题组"
tags: ["architecture", "decision", "meta-learning", "loss-function"]
supersedes: ""
superseded_by: ""
---

# ADR-0003: 暂缓为元学习损失函数实现独立伯努利 JSD（IB-JSD）

## 状态 (Status)

Proposed | **Accepted** | Rejected | Superseded | Deprecated

## 背景 (Context)

当前 `Tiny-ONN` 模型的元学习损失函数 (`L_meta`) 旨在通过詹森-香农散度 (JSD) 对齐模型的路由激活分布与目标“专家效用”分布。目前的实现隐式地将 `d` 个专家的激活决策建模为一个单一的**多项式分布**，这与模型“独立概念激活”的核心设计哲学存在理论不一致。一个更理论纯粹的方案是采用 `d` 个并行的**独立伯努利分布**来计算 JSD，但这将改变训练动态并使现有检查点失效。

## 决策 (Decision)

我们决定**暂缓**从当前的多项式（伪）JSD 实现迁移至IB-JSD。我们将继续使用现有的损失函数，并接受其作为一种有价值的隐式正则化器。

## 后果 (Consequences)

### 积极 (Positive)

- **POS-001**: 维持了当前稳定且已知的训练动态，避免了引入新不稳定性的风险。
- **POS-002**: 保留了当前实现中“赢家通吃”动态所带来的隐式正则化效果，它能有效推动专家功能正交化，而无需额外成本。
- **POS-003**: 避免了因破坏性变更导致所有现有检查点失效，以及重新进行昂贵超参数搜索的巨大工程开销。

### 消极 (Negative)

- **NEG-001**: 模型在理论上解决需要真正“横向组合”的特定任务的能力可能受限。
- **NEG-002**: 保留了模型核心假设（专家独立）与损失函数实现（专家互斥）之间的理论不一致性，这被记录为一项技术债务。
- **NEG-003**: 可能会掩盖模型在学习组合性表示方面的潜在瓶颈，延迟对更优架构的探索。

## 考虑的备选方案 (Alternatives Considered)

### 立即实施IB-JSD

- **ALT-001**: **描述 (Description)**: 将损失函数修改为计算 `d` 个独立的伯努利分布对 (`p_i`, `q_i`) 的 JSD，然后进行聚合。这在理论上更符合模型设计哲学。
- **ALT-002**: **拒绝理由 (Rejection Reason)**:
  1. **功能冗余**: 当前模型的深度可能已经通过“纵向协同”提供了足够的计算能力，使得“横向组合”并非必要。
  2. **工程成本**: 此方案是破坏性变更，会废弃所有检查点并需要大量资源进行重新调优。
  3. **失去正则化**: 放弃了当前实现中有价值的、能促进专家分化的“赢家通吃”隐式正则化。

## 实施注意事项 (Implementation Notes)

- **IMP-001**: 无需立即实施。
- **IMP-002**: 本次决策应被视为一项已知的技术债务。如果未来实验证据表明模型性能的瓶颈确实源于缺乏横向组合能力，应重新审视并优先实施此备选方案。
- **IMP-003**: 建议在 `Observer` 中增加对专家原型间相似度的监控，以便在未来更客观地评估专家功能冗余问题。

## 参考文献 (References)

- **REF-001**: `docs/rules/DFC-Theory.md`
- **REF-002**: 相关理论会议的内部讨论记录。
