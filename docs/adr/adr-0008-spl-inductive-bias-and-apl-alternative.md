---
标题: "ADR-0008: SPL的归纳偏差作为交叉注意力与APL替代方案"
状态: "草稿"
日期: "2025-10-10"
作者: "林睿, Ω 研究员"
标签: ["架构", "决策", "注意力机制", "spl", "apl"]
取代: ""
被取代: ""
---

# ADR-0008: SPL 的归纳偏差作为交叉注意力与 APL 替代方案

## 状态 (Status)

**Draft** | Proposed | Accepted | Rejected | Superseded | Deprecated

## 背景 (Context)

我们对 `稀疏原型线性 (SparseProtoLinear, SPL)` 核心机制的研究得出了一个关键见解：其基本操作可以被形式化理解为一种**交叉注意力**。输入向量 `x` 充当**查询 (Query)**，原型矩阵 `p` 充当**键 (Key)**集。由此产生的 `match_values` 是注意力分数，决定了哪些计算操作符 `μ`（**值 (Values)**）被激活。

这种交叉注意力的框架揭示了一种强大但可能具有局限性的归纳偏差：它在整个输入特征向量和整个原型向量之间执行“整体到整体”的匹配。这种偏差是有益的，因为它在“感知空间”（输入 `x`）和“操作符空间”（原型 `p` 和操作符 `μ`）之间保持了清晰的分离，这对于 ARC 任务所需的组合推理可能至关重要。然而，这种相同的偏差阻碍了模型捕获更细粒度的关系，例如“输入 `x` 的一个子空间与原型 `p` 的一个子空间相关”。

本 ADR 记录了对这种归纳偏差的分析，并探索了一种重要的架构替代方案，即 `注意力原型线性 (AttentiveProtoLinear, APL)`，旨在超越这一限制。

## 核心问题 (The Core Problem)

我们能否通过将其归纳偏差从“整体概念选择”（交叉注意力）演变为“细粒度概念构建”（跨融合空间的自注意力），来增强我们核心计算原语的表达能力？这种改变会带来哪些理论和计算上的权衡？

## 备选方案 1: `APL` (AttentiveProtoLinear) - 一种新范式

本提案旨在用一种在融合的输入-原型空间上运行的、更强大的自注意力机制来取代交叉注意力机制。

### 形式化定义 (Formal Definition)

1. **扩展与拼接**: 输入 `x`（形状 `b, s, d`）和原型矩阵 `p`（形状 `n, d`）被融合为一个联合序列。
   `joint_sequence = cat([x.unsqueeze(2), p.expand(b, s, n, d)], dim=2)` -> 形状 `(b, s, 1+n, d)`
2. **融合自注意力**: 在这个长度为 `1+n` 的短融合序列上执行标准的 `缩放点积注意力 (scaled_dot_product_attention)`。
   `q, k, v = W_q(joint_seq), W_k(joint_seq), W_v(joint_seq)`
   `attended_sequence = sdpa(q, k, v)` -> 形状 `(b, s, 1+n, d)`
3. **提取**: 从输出中提取情境化的输入 `x_refined` 和原型 `p_refined`。
   `x_refined = attended_sequence[:, :, 0, :]`
   `p_refined = attended_sequence[:, :, 1:, :]`
4. **最终交互**: 后续的路由和计算步骤使用这些经过提炼和情境化的表示。

### 分析 (Analysis)

- **表达能力**: `APL` 将范式从**选择**离散原型转变为**构建**针对每个标记的连续、特定于输入的虚拟操作符。这代表了表达能力上的根本性飞跃。
- **计算成本**: 复杂度主要由融合自注意力决定，即 `O(b * s * n_protos^2 * d)`。虽然显著，但这是一个可控的多项式增长，而非指数爆炸，并符合我们项目利用密集计算来获得更好表示的理念。

## 备选方案 2: 坚守并深化 `SPL` (Adhere to and Deepen SPL)

本提案主张保留 `SPL` 的交叉注意力归纳偏差，基于的洞察是它可能才是适用于 ARC 的正确偏差。

### 论点 (Argument)

感知空间 (`x`) 和操作符空间 (`μ`) 的分离并非缺陷，而是一个特性。它迫使模型学习其“所见”和“所做”之间清晰、解耦的映射。`APL` 方法通过提前融合这些空间，可能会导致难以优化和解释的纠缠表示。因此，挑战不在于替换 `SPL`，而在于增强其在现有交叉注意力框架内捕获更细粒度关系的能力。

## 决策的递归性困境 (The Dilemma of Recursive Application)

在讨论过程中发现了一个深刻的元问题：如果我们认为固定的线性变换（如 `APL` 中的 `W_q, W_k, W_v`）是一个瓶颈，那么它们是否也应该被动态的 `SPL`/`APL` 模块取代？这将导致无限递归。

`A -> APL(A) -> APL(APL(A)) ...`

这意味着任何可行的架构都必须在某种程度上接受一组固定的、原始的计算操作符。`APL` 提案只是将这种“基本现实”向更深一层推进，推入其内部投影矩阵。这种困境必须成为未来任何架构决策的核心考虑因素。它表明，当前 `SPL` 的简单性——它使用直接、非参数化的交互——可能是一种美德。

## 决策 (Decision)

**暂不立即实施架构更改。** 本 ADR 的目的是正式记录所产生的深刻理论问题。

1. 我们认识到 `SPL` 的交叉注意力本质，并承认其归纳偏差对于 ARC 来说可能是一个关键且正确的特性。
2. 我们已将 `APL` 正式化为一种高风险、高回报的替代方案，它从根本上改变了这种偏差。
3. 我们确定了“递归问题”，它挑战了用动态变换取代固定变换的基本假设。

在承诺进行成本高昂的 `APL` 实现之前，需要进一步的讨论和理论分析。当前 `SPL` 架构的性能尚未得到充分探索，而且其理论基础现在被认为比之前假设的更牢固。我们眼前的重点应回到从现有、更简单且理论上合理的 `SPL` 架构中挖掘最大潜力。
