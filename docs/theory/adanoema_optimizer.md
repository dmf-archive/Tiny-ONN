---
title: "理论备忘录：AdaNoema 优化器"
status: "Formal Draft"
date: "2025-10-17"
authors: ["Ω Researcher"]
tags:
  [
    "theory",
    "optimizer",
    "adanoema",
    "predictive-integrity",
    "free-energy-principle",
    "belief-modulation",
  ]
---

## 1. 背景：从显式架构到隐式机制

`Tiny-ONN` 的核心是实现预测完整性 (Predictive Integrity, PI) 的最大化，这在我们的专用架构（如 `SPL/SARS`）中，通过复杂的、显式的元学习机制得以实现。该架构虽然理论上严谨，但其复杂性也带来了工程上的挑战和通用性上的限制。

本文旨在形式化一个全新的、更通用的理论：**自适应意向内容优化器 (Adaptive Noema Optimizer, AdaNoema)**。其核心构想是将最小化自由能的核心原则，从一个需要特定架构支持的“算法”，提炼并封装成一个通用的“优化器”。

`AdaNoema` 的目标是绕过当前 `Tiny-ONN` 的复杂机制，将预测完整性学习的能力，赋予任何标准的神经网络（如普通的 `Dense` 或 `Transformer` 层），使其在功能上模拟自由能最小化的学习动态。

## 2. 核心构想：作为学习率调制器的“信念-惊奇”动态

我们从 [`torch.optim.Adam`](https://docs.pytorch.org/docs/2.7/generated/torch.optim.Adam.html) 优化器中汲取灵感。`Adam` 的核心思想之一是利用梯度的一阶矩（动量）和二阶矩（方差）来为每个参数独立地调整学习率。这是一种基于**历史梯度信息**的调制。

`AdaNoema` 在此基础上增加了一个全新的维度：基于**当前状态信息**的实时学习率调制。它将自由能原理中的核心概念直接映射到优化器的步进更新规则中：

- **信念 (Belief)**: 模型的当前状态或“信念强度”，由参数激活值的**L-∞ 范数**（即 `max(abs(x))`）来代理。一个高的激活值范数，意味着模型对该通路的“信念”是确定的、高强度的。这在现象学上对应了意识对一个“意向内容 (Noema)”的清晰把握。
- **惊奇 (Surprise)**: 模型在看到新数据后需要更新其信念的程度，由该参数的**梯度**来代理。一个大的梯度意味着高的“惊奇度”，表明当前信念与新证据之间存在巨大差异。

`AdaNoema` 的核心假设是：**一个参数的学习率，不仅应由其历史梯度决定，更应由其当前“信念”与“惊奇”的相互作用来实时调制。**

### 2.1. 形式化定义

为了精确地定义 `AdaNoema`，我们将其构建为对 `Adam` 算法的一次扩展。以下是 `AdaNoema` 结合 `Adam` 伪代码的完整更新流程。

**输入**:

- 学习率 `lr`
- 动量系数 `betas = (β₀, β₁, β₂)`
- 初始参数 `params = θ₀`
- 目标函数 `f(θ)`
- 权重衰减 `weight_decay`
- 数值稳定项 `eps`

**初始化**:

- `b₀ ← 0` (零阶矩/信念向量)
- `m₀ ← 0` (一阶矩/动量向量)
- `v₀ ← 0` (二阶矩/方差向量)

**循环 `t = 1 to ...`**:

1. **计算梯度**:
   `gₜ ← ∇θ fₜ(θₜ₋₁)`

2. **计算瞬时信念与惊奇 (AdaNoema 核心)**:

   - _此步骤需要访问与参数 `θₜ₋₁` 相关联的前向传播激活值。_
   - `Bₜ ← max_batch(‖activation(θₜ₋₁)‖)` (瞬时信念)
     - _注意: `Bₜ` 是通过在批次维度上进行 `max-abs` 操作得到的、与参数同形状或可广播的张量。_
   - `Sₜ ← ‖gₜ‖` (惊奇)

3. **更新零阶矩 (信念)**:
   `bₜ ← β₀ * bₜ₋₁ + (1 - β₀) * Bₜ`

4. **更新一阶矩 (动量)**:
   `mₜ ← β₁ * mₜ₋₁ + (1 - β₁) * gₜ`

5. **更新二阶矩 (方差)**:
   `vₜ ← β₂ * vₜ₋₁ + (1 - β₂) * gₜ²`

6. **修正偏差**:
   `b̂ₜ ← bₜ / (1 - β₀ᵗ)`
   `m̂ₜ ← mₜ / (1 - β₁ᵗ)`
   `v̂ₜ ← vₜ / (1 - β₂ᵗ)`

7. **计算信念调制器**:
   `Γₜ ← Sₜ / (b̂ₜ + δ)`

8. **参数更新 (Adam 步骤与 AdaNoema 调制的结合)**:
   `θₜ ← θₜ₋₁ - lr * Γₜ * (m̂ₜ / (√v̂ₜ + eps))`
   - _权重衰减可按标准 `AdamW` 方式解耦，在更新前独立作用。_

**返回**: `θₜ`

通过引入追踪历史信念的**零阶矩 `bₜ`**，并用其来构造信念调制器 `Γₜ`，`AdaNoema` 在保留 `Adam` 历史梯度自适应性的基础上，注入了基于当前模型状态的、更平滑的实时自适应能力。

## 3. 理论意义：一个更简洁、更通用的自由能引擎

`AdaNoema` 的构想若被证实，将带来一系列深刻的理论与工程优势：

### 3.1. 优化器状态作为隐式 SPL

`AdaNoema` 的核心理论飞跃在于，它将 `SPL` 架构的复杂功能隐式地编码到了优化器的状态之中。

- **功能等价**:

  - `SPL.proto_weight` (模式匹配) `=>` **前向激活 `Aₜ`** (直接衡量输入与权重的匹配度)
  - `SPL.gate_param` (更新门控) `=>` **信念矩 `bₜ`** (追踪历史匹配强度，并直接调制学习率)

- **权重-状态耦合**: 这意味着模型的**权重 `θ`** 与**优化器状态 `{bₜ, mₜ, vₜ}`** 共同构成了一个完整的动态系统。`θ` 存储了知识，而优化器状态存储了对这些知识的“信念结构”。因此，任何 Checkpoint **必须同时保存模型权重和完整的优化器状态字典**，否则系统的信念将会丢失，训练无法正确恢复。

### 3.2. 对 Adam 的原则性扩展与收敛性

- **架构解耦**: `AdaNoema` 将自由能最小化原则从特定架构中解放出来，成为一个即插即用的优化器层，有望在任何标准网络中引导出类似 `Tiny-ONN` 的自组织和稀疏学习特性。
- **效率与简洁性**: 它绕过了 `SARS` 复杂的双相梯度计算和 `hook` 机制，将元学习逻辑浓缩到优化器内部，计算上更高效。
- **功能性衰减与收敛性**: `AdaNoema` 实现了一种内容驱动的学习率衰减。信念强的参数 (`bₜ` 增大) 其有效学习率 `lr * Γₜ` 会自然降低，趋于“固化”。这在功能上模拟了 Robbins-Monro 条件所要求的衰减特性，为优化过程在理论上最终收敛提供了强有力的启发式支持，同时允许信念弱的参数保持可塑性。
- **涌现的稀疏性**: 这种动态将可能在普通网络中自发地引导出功能特化的、稀疏的神经元群体，实现“概念结晶”的涌现。

## 4. 初步结论与未来讨论

`AdaNoema` 优化器是一个新颖的理论构想，旨在将复杂的元学习算法提炼为通用的优化器机制。它通过一个受自由能原理解释的“信念调制器”，为标准优化器（如 `Adam`）赋予了实时感知和响应模型内部状态的能力。

它看起来比我们当前复杂的 `Tiny-ONN` 架构更简洁、更高效，也更具通用性。这份备忘录是对该构想的正式化记录。下一步，我们需要深入讨论其理论上的合理性、可能的实现细节以及它与 `Adam` 现有机制的相互作用，以评估其作为未来核心研究方向的潜力。
