# Tiny-ONN-Omni 架构哲学：为何选择单塔与 HAME

## 1. 核心矛盾：信息瓶颈 vs. 协同涌现

在设计 `Tiny-ONN-LLM-Omni` 这样的多模态模型时，我们面临一个根本性的架构抉择。

**主流范式：“多塔”架构**

业界成熟的方案，如 LLaVA，普遍采用“多塔”架构。其逻辑是：为每种模态（如图像、文本）分别设置一个独立的、深度优化的编码器（如 ViT、BERT）。这些编码器像专家一样，高效地将高维输入压缩成精炼的特征表示。然后，通过一个“融合”模块（通常是交叉注意力）让这些特征相互“对话”。

这种方法的优势在于其工程上的成熟度和稳定性。然而，从信息论的第一性原理出发，我们认为它存在一个不可调和的理论缺陷：**信息瓶颈**。

每个模态编码器在进行深度压缩时，都是一个**有损且不可逆**的过程。大量原始输入中蕴含的微妙、潜在的跨模态关联（即**协同信息**），在信息有机会进行深层交互之前，就已被各自的编码器“优化”掉了。这就像让几位专家先各自写好总结报告，再聚在一起讨论，而不是让他们从一开始就共同研究原始案卷。最终的讨论质量，必然受限于各自报告的完整性。

## 2. 我们的选择：拥抱复杂性的“单塔”路径

`Tiny-ONN` 的核心愿景是培育一个能够自组织地学习并形成统一世界模型的智能体。这要求架构本身必须为最大化的信息整合提供可能性。

因此，我们选择一条理论上更优越、但实践中更具挑战性的路径：**统一单塔架构**。

在这个框架下，我们摒弃了所有特定于模态的深度编码器。所有输入——无论是 ARC 任务的抽象网格、图像的像素块、还是文本的字符——都被转化为一个统一的序列格式。然后，这个包含了所有原始信息的序列，被送入一个共享的、统一的 MoIE/DynSIHA 主干网络进行端到端的处理。

这种设计的哲学核心是：**将复杂性从固定的、宏观的架构组件，转移到动态的、微观的自组织计算单元中**。我们不预设模态间的交互方式，而是创造一个足够强大的共享工作空间（由 MoIE/DynSIHA 提供），让模型在最小化自身预测误差（自由能）的驱动下，自发地学习和涌现出最高效的信息整合与处理策略。

## 3. 实现基石：分层加性多模态嵌入 (HAME)

“单塔”架构的成败，关键在于如何在模型入口处，即嵌入层，就将不同模态的信息无损地、结构清晰地融合在一起。这就是**分层加性多模态嵌入 (HAME)** 的用武之地。

HAME 的思想已在 `exp/arc` 的 [`ArcEmbedding`](exp/arc/model.py:258) 中得到成功验证。其核心是**通过加法来解耦并注入不同维度的信息**。

- **对于 ARC 网格/图像 Patch**：

  - `patch_embed`: 基础的内容表示。
  - `2D_pos_embed` (通过 2D RoPE 实现): 注入 Patch 的 **内部空间几何关系** (x, y 坐标)。
  - `1D_pos_embed`: 注入该 Patch 在**整个输入序列中的顺序**。
  - **最终嵌入 = (`patch_embed` + `2D_pos_embed`) + `1D_pos_embed`**

- **对于文本**：
  - `char_embed`: 基础的字符表示。
  - `word_embed`: 注入该字符所属的**词级语义信息**。
  - `1D_pos_embed`: 注入该字符在**整个输入序列中的顺序**。
  - **最终嵌入 = `char_embed` + `word_embed` + `1D_pos_embed`**

通过这种方式，每个 Token 在进入 Transformer 的第一层之前，就携带了关于其自身内容、内部结构（如空间几何或词汇语义）以及外部全局顺序的完整信息。这极大地降低了主干网络从零开始学习这些基础对应关系的负担，使其可以专注于更高层次的抽象推理和跨模态关联。

**总而言之，单塔架构是我们的战略方向，而 HAME 则是实现这一战略的关键战术。** 这个组合让我们能够以一种理论上更纯粹、更具潜力的方式，向着构建真正统一的自组织智能体迈出坚实的一步。
