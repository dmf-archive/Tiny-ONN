{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca05964",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# This script is auto-generated by scripts/build_kaggle_script.py\n",
    "# Do not edit this file directly.\n",
    "# =============================================================================\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rich.console import Console\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee8b1f",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 16\n",
    "    hidden_size: int = 768\n",
    "    num_layers: int = 6\n",
    "    max_position_embeddings: int = 4096\n",
    "    d_ffn_factor: int = 1\n",
    "    routing_gain: float = ROUTING_GAIN\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    # Adjusted path for Kaggle single-file datasets\n",
    "    challenges_path: str = \"/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json\"\n",
    "    solutions_path: str = \"/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json\" \n",
    "    batch_size: int = 1\n",
    "    num_workers: int = 2\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "\n",
    "    # Paths for Kaggle environment\n",
    "    checkpoint_dir: str = \"/kaggle/working/checkpoints/\" if 'kaggle' in os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '') else \"notebook/checkpoints/\"\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    w_route_jsd: float = 1.1\n",
    "    num_epochs: int = 20\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    log_interval: int = 10\n",
    "    max_checkpoints: int = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d33bcd",
   "metadata": {},
   "source": [
    "# Core Code Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "\n",
    "\n",
    "class ArcColorTokenizer:\n",
    "    def __init__(self):\n",
    "        self.control_tokens = {\n",
    "            \"<|pad|>\": 0,\n",
    "            \"<|bos|>\": 1,\n",
    "            \"<|eos|>\": 2,\n",
    "            \"problem\": 3,\n",
    "            \"solution\": 4,\n",
    "            \"\\n\": 5,\n",
    "            \"<im_start>\": 6,\n",
    "            \"<im_end>\": 7,\n",
    "        }\n",
    "\n",
    "        self.color_token_offset = len(self.control_tokens)\n",
    "        self.num_colors = 10\n",
    "        self.color_tokens = {str(i): self.color_token_offset + i for i in range(self.num_colors)}\n",
    "\n",
    "        self.vocab = {**self.control_tokens, **self.color_tokens}\n",
    "        self.inv_vocab: Mapping[int, str] = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def color_to_token_id(self, color: int) -> int:\n",
    "        return self.color_token_offset + color\n",
    "\n",
    "    def token_id_to_color(self, token_id: int) -> int | None:\n",
    "        if token_id < self.color_token_offset or token_id >= self.color_token_offset + self.num_colors:\n",
    "            return None\n",
    "        return token_id - self.color_token_offset\n",
    "\n",
    "    @property\n",
    "    def row_sep_token_id(self) -> int:\n",
    "        return self.control_tokens[\"\\n\"]\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self) -> int:\n",
    "        return self.control_tokens[\"<|pad|>\"]\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self) -> int:\n",
    "        return self.control_tokens[\"<|bos|>\"]\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self.control_tokens[\"<|eos|>\"]\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.vocab[token] for token in text.split(\" \") if token in self.vocab]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return \" \".join([self.inv_vocab.get(token, \"<|unk|>\") for token in tokens])\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(\n",
    "    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mas_normalize(logits: torch.Tensor) -> torch.Tensor:\n",
    "    max_abs_val = torch.max(torch.abs(logits), dim=-1, keepdim=True).values\n",
    "    scaled_logits = logits / (max_abs_val + 1e-9)\n",
    "    return F.relu(scaled_logits)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mas_normalize_negative(logits: torch.Tensor) -> torch.Tensor:\n",
    "    return -F.relu(-mas_normalize(logits))\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def spl_forward(\n",
    "    x: torch.Tensor,\n",
    "    effective_proto: torch.Tensor,\n",
    "    mu_weight: torch.Tensor,\n",
    "    mu_bias: torch.Tensor,\n",
    "    gate_param: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    model_dtype = mu_weight.dtype\n",
    "    x = x.to(model_dtype)\n",
    "    effective_proto = effective_proto.to(model_dtype)\n",
    "    gate_param = gate_param.to(model_dtype)\n",
    "\n",
    "    match_values = F.linear(x, effective_proto) / math.sqrt(x.size(-1))\n",
    "    gate_logit = torch.matmul(x, gate_param.t())\n",
    "    computation_output = F.linear(x, mu_weight, mu_bias)\n",
    "\n",
    "    return computation_output, match_values, gate_logit\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        max_position_embeddings: int = 2048,\n",
    "        base: int = 10000,\n",
    "        device: torch.device | None = None,\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (\n",
    "            self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim)\n",
    "        )\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, seq_len: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos().unsqueeze(0).to(dtype=x.dtype)\n",
    "        sin = emb.sin().unsqueeze(0).to(dtype=x.dtype)\n",
    "        return cos, sin\n",
    "\n",
    "\n",
    "class SparseProtoLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.mu_weight = nn.Parameter(torch.empty(out_features, in_features, dtype=dtype))\n",
    "        self.proto_weight = nn.Parameter(torch.empty(out_features, in_features, dtype=dtype))\n",
    "        self.mu_bias = nn.Parameter(torch.empty(out_features, dtype=dtype))\n",
    "        self.gate_param = nn.Parameter(torch.empty(out_features, in_features, dtype=dtype))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.mu_weight, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.proto_weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.mu_bias)\n",
    "        nn.init.zeros_(self.gate_param)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, effective_proto: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return spl_forward(x, effective_proto, self.mu_weight, self.mu_bias, self.gate_param)\n",
    "\n",
    "\n",
    "class DynamicInfiniteHeadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.d_model = config.hidden_size\n",
    "        self.spl_q = SparseProtoLinear(self.d_model, self.d_model, dtype=dtype)\n",
    "        self.spl_k = SparseProtoLinear(self.d_model, self.d_model, dtype=dtype)\n",
    "        self.spl_v = SparseProtoLinear(self.d_model, self.d_model, dtype=dtype)\n",
    "        self.spl_o = SparseProtoLinear(self.d_model, self.d_model, dtype=dtype)\n",
    "\n",
    "\n",
    "class DynamicInfiniteExpert(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        d_ffn = config.hidden_size * config.d_ffn_factor\n",
    "        self.spl1 = SparseProtoLinear(config.hidden_size, d_ffn, dtype=dtype)\n",
    "        self.spl2 = SparseProtoLinear(d_ffn, config.hidden_size, dtype=dtype)\n",
    "\n",
    "\n",
    "class MoIETransformerBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size, eps=1e-5)\n",
    "        self.attn = DynamicInfiniteHeadAttention(config, dtype=dtype)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size, eps=1e-5)\n",
    "        self.ffn = DynamicInfiniteExpert(config, dtype=dtype)\n",
    "        self.routing_gain = config.routing_gain\n",
    "        d_ffn = config.hidden_size * config.d_ffn_factor\n",
    "        self.proto_transforms = nn.ModuleDict(\n",
    "            {\n",
    "                \"attn_q\": nn.Linear(config.hidden_size, config.hidden_size, bias=False, dtype=dtype),\n",
    "                \"attn_k\": nn.Linear(config.hidden_size, config.hidden_size, bias=False, dtype=dtype),\n",
    "                \"attn_v\": nn.Linear(config.hidden_size, config.hidden_size, bias=False, dtype=dtype),\n",
    "                \"attn_o\": nn.Linear(config.hidden_size, config.hidden_size, bias=False, dtype=dtype),\n",
    "                \"ffn_spl1\": nn.Linear(config.hidden_size, config.hidden_size, bias=False, dtype=dtype),\n",
    "                \"ffn_spl2\": nn.Linear(d_ffn, d_ffn, bias=False, dtype=dtype),\n",
    "            }\n",
    "        )\n",
    "        self.proto_layernorms = nn.ModuleDict(\n",
    "            {\n",
    "                \"attn_q\": nn.LayerNorm(config.hidden_size, eps=1e-5),\n",
    "                \"attn_k\": nn.LayerNorm(config.hidden_size, eps=1e-5),\n",
    "                \"attn_v\": nn.LayerNorm(config.hidden_size, eps=1e-5),\n",
    "                \"attn_o\": nn.LayerNorm(config.hidden_size, eps=1e-5),\n",
    "                \"ffn_spl1\": nn.LayerNorm(config.hidden_size, eps=1e-5),\n",
    "                \"ffn_spl2\": nn.LayerNorm(d_ffn, eps=1e-5),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, pos_emb: tuple, past_kv: tuple | None = None, prev_protos: dict | None = None\n",
    "    ) -> tuple:\n",
    "        effective_protos = {}\n",
    "        spl_modules = {\n",
    "            \"attn_q\": self.attn.spl_q,\n",
    "            \"attn_k\": self.attn.spl_k,\n",
    "            \"attn_v\": self.attn.spl_v,\n",
    "            \"attn_o\": self.attn.spl_o,\n",
    "            \"ffn_spl1\": self.ffn.spl1,\n",
    "            \"ffn_spl2\": self.ffn.spl2,\n",
    "        }\n",
    "        for name, module in spl_modules.items():\n",
    "            if prev_protos is not None and name in prev_protos:\n",
    "                residual = self.proto_layernorms[name](self.proto_transforms[name](prev_protos[name]))\n",
    "                effective_protos[name] = module.proto_weight + residual\n",
    "            else:\n",
    "                effective_protos[name] = module.proto_weight\n",
    "\n",
    "        ln1_out = self.ln1(x)\n",
    "        c_q, mv_q, pc_q = self.attn.spl_q(ln1_out, effective_protos[\"attn_q\"])\n",
    "        c_q = ln1_out + c_q\n",
    "        c_k, mv_k, pc_k = self.attn.spl_k(ln1_out, effective_protos[\"attn_k\"])\n",
    "        c_k = ln1_out + c_k\n",
    "        c_v, mv_v, pc_v = self.attn.spl_v(ln1_out, effective_protos[\"attn_v\"])\n",
    "        c_v = ln1_out + c_v\n",
    "\n",
    "        ln2_out = self.ln2(x)\n",
    "        c1, mv1, pc1 = self.ffn.spl1(ln2_out, effective_protos[\"ffn_spl1\"])\n",
    "        c1 = ln2_out + c1\n",
    "\n",
    "        dummy_h_act = torch.zeros(\n",
    "            ln2_out.shape[0],\n",
    "            ln2_out.shape[1],\n",
    "            self.ffn.spl2.in_features,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        _, _, pc2_pre = self.ffn.spl2(dummy_h_act, effective_protos[\"ffn_spl2\"])\n",
    "\n",
    "        dummy_attn_out = torch.zeros_like(x)\n",
    "        _, _, pc_o_pre = self.attn.spl_o(dummy_attn_out, effective_protos[\"attn_o\"])\n",
    "\n",
    "\n",
    "        all_masked, all_comp, all_raw, all_routing_logits, all_spl_inputs = [], [], [], [], []\n",
    "\n",
    "        comp_qkv, match_qkv, costs_qkv = [c_q, c_k, c_v], [mv_q, mv_k, mv_v], [pc_q, pc_k, pc_v]\n",
    "        q, k, v = torch.zeros_like(c_q), torch.zeros_like(c_k), torch.zeros_like(c_v)\n",
    "\n",
    "        for i in range(3):\n",
    "            cost_score = mas_normalize(costs_qkv[i])\n",
    "            routing_logits = (match_qkv[i] - cost_score) * self.routing_gain\n",
    "            raw_weights = mas_normalize(routing_logits)\n",
    "            masked = comp_qkv[i] * raw_weights\n",
    "            if i == 0: q = masked\n",
    "            elif i == 1: k = masked\n",
    "            else: v = masked\n",
    "            all_masked.append(masked)\n",
    "            all_comp.append(comp_qkv[i])\n",
    "            all_raw.append(raw_weights)\n",
    "            all_routing_logits.append(routing_logits)\n",
    "\n",
    "        cos, sin = pos_emb\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        if past_kv is not None:\n",
    "            k = torch.cat([past_kv[0], k], dim=1)\n",
    "            v = torch.cat([past_kv[1], v], dim=1)\n",
    "        present_kv = (k, v)\n",
    "        q = q.unsqueeze(1)\n",
    "        k = k.unsqueeze(1)\n",
    "        v = v.unsqueeze(1)\n",
    "        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal=past_kv is None)\n",
    "        attn_out = attn_out.squeeze(1)\n",
    "\n",
    "        c_o, mv_o, pc_o = self.attn.spl_o(attn_out, effective_protos[\"attn_o\"])\n",
    "        c_o = attn_out + c_o\n",
    "        cost_score_o = mas_normalize(pc_o)\n",
    "        routing_logits_o = (mv_o - cost_score_o) * self.routing_gain\n",
    "        rw_o = mas_normalize(routing_logits_o)\n",
    "        m_o = c_o * rw_o\n",
    "        x = x + m_o\n",
    "\n",
    "        cost_score_f1 = mas_normalize(pc1)\n",
    "        routing_logits_f1 = (mv1 - cost_score_f1) * self.routing_gain\n",
    "        rw_f1 = mas_normalize(routing_logits_f1)\n",
    "        m1 = c1 * rw_f1\n",
    "        h_act = F.relu(m1)\n",
    "\n",
    "        c2, mv2, pc2 = self.ffn.spl2(h_act, effective_protos[\"ffn_spl2\"])\n",
    "        c2 = h_act + c2\n",
    "        cost_score_f2 = mas_normalize(pc2)\n",
    "        routing_logits_f2 = (mv2 - cost_score_f2) * self.routing_gain\n",
    "        rw_f2 = mas_normalize(routing_logits_f2)\n",
    "        m2 = c2 * rw_f2\n",
    "        x_out = x + m2\n",
    "\n",
    "        all_masked.extend([m_o, m1, m2])\n",
    "        all_comp.extend([c_o, c1, c2])\n",
    "        all_raw.extend([rw_o, rw_f1, rw_f2])\n",
    "        all_routing_logits.extend([routing_logits_o, routing_logits_f1, routing_logits_f2])\n",
    "        all_spl_inputs.extend([ln1_out] * 3 + [attn_out, ln2_out, h_act])\n",
    "\n",
    "        return x_out, all_masked, all_comp, all_raw, all_spl_inputs, all_routing_logits, present_kv, effective_protos\n",
    "\n",
    "\n",
    "class ArcEmbedding(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.color_embedding = nn.Embedding(config.vocab_size, config.hidden_size, dtype=dtype)\n",
    "        self.row_embedding = nn.Embedding(31, config.hidden_size, dtype=dtype)\n",
    "        self.col_embedding = nn.Embedding(31, config.hidden_size, dtype=dtype)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, coords: torch.Tensor) -> torch.Tensor:\n",
    "        color_embed = self.color_embedding(input_ids)\n",
    "\n",
    "        row_coords = coords[..., 0]\n",
    "        col_coords = coords[..., 1]\n",
    "\n",
    "        row_embed = self.row_embedding(row_coords.clamp(min=0, max=30))\n",
    "        col_embed = self.col_embedding(col_coords.clamp(min=0, max=30))\n",
    "\n",
    "        is_special_token = (coords[..., 0] == -1).unsqueeze(-1)\n",
    "\n",
    "        pos_embed = row_embed + col_embed\n",
    "        final_embed = color_embed + torch.where(is_special_token, torch.zeros_like(pos_embed), pos_embed)\n",
    "\n",
    "        return final_embed\n",
    "\n",
    "class ArcTransformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig, device: torch.device | str):\n",
    "        super().__init__()\n",
    "        self.config, self.device = config, device\n",
    "        dtype = torch.float32\n",
    "        self.embedding = torch.jit.script(ArcEmbedding(config, dtype=dtype))\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=config.hidden_size, max_position_embeddings=config.max_position_embeddings, device=device, dtype=dtype\n",
    "        )\n",
    "        self.blocks = nn.ModuleList([MoIETransformerBlock(config, dtype=dtype) for _ in range(config.num_layers)])\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False, dtype=dtype)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, coords: torch.Tensor, past_key_values: list | None = None, return_dict: bool = False):\n",
    "        x = self.embedding(input_ids, coords)\n",
    "        pos_emb = self.rotary_emb(x, seq_len=input_ids.size(1))\n",
    "        past_key_values = past_key_values if past_key_values is not None else [None] * len(self.blocks)\n",
    "\n",
    "        all_masked, all_comp, all_spl_in, all_raw, all_protos, all_routing_logits, presents = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "        prev_protos = None\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            (\n",
    "                x,\n",
    "                masked,\n",
    "                comp,\n",
    "                raw,\n",
    "                spl_inputs,\n",
    "                routing_logits,\n",
    "                present_kv,\n",
    "                effective_protos,\n",
    "            ) = block(x, pos_emb, past_key_values[i], prev_protos)\n",
    "            presents.append(present_kv)\n",
    "            all_masked.extend(masked)\n",
    "            all_comp.extend(comp)\n",
    "            all_spl_in.extend(spl_inputs)\n",
    "            all_raw.extend(raw)\n",
    "            all_protos.append(effective_protos)\n",
    "            all_routing_logits.extend(routing_logits)\n",
    "            prev_protos = effective_protos\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if not return_dict:\n",
    "            return logits, x, all_masked, all_comp, all_protos, all_spl_in, all_raw, all_routing_logits, presents\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"hidden_states\": x,\n",
    "            \"masked_outputs\": all_masked,\n",
    "            \"computation_outputs\": all_comp,\n",
    "            \"proto_states\": all_protos,\n",
    "            \"spl_inputs\": all_spl_in,\n",
    "            \"raw_weights\": all_raw,\n",
    "            \"routing_logits\": all_routing_logits,\n",
    "            \"past_key_values\": presents,\n",
    "        }\n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridSerializer:\n",
    "    def __init__(self, tokenizer: ArcColorTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _serialize_grid(self, grid: list[list[int]]) -> tuple[list[int], list[tuple[int, int]]]:\n",
    "        tokens = []\n",
    "        coords = []\n",
    "        for r, row in enumerate(grid):\n",
    "            if r > 0:\n",
    "                tokens.append(self.tokenizer.row_sep_token_id)\n",
    "                coords.append((-1, -1))\n",
    "            for c, color in enumerate(row):\n",
    "                tokens.append(self.tokenizer.color_to_token_id(color))\n",
    "                coords.append((r, c))\n",
    "        return tokens, coords\n",
    "\n",
    "    def serialize_task(self, task_data: dict[str, Any]) -> tuple[list[int], list[int], list[tuple[int, int]]]:\n",
    "        full_ids: list[int] = [self.tokenizer.bos_token_id]\n",
    "        full_coords: list[tuple[int, int]] = [(-1, -1)]\n",
    "        \n",
    "        im_start_id = self.tokenizer.vocab[\"<im_start>\"]\n",
    "        im_end_id = self.tokenizer.vocab[\"<im_end>\"]\n",
    "\n",
    "        for pair in task_data[\"train\"]:\n",
    "            input_ids, input_coords = self._serialize_grid(pair[\"input\"])\n",
    "            output_ids, output_coords = self._serialize_grid(pair[\"output\"])\n",
    "            \n",
    "            full_ids.extend([im_start_id] + input_ids + [im_end_id])\n",
    "            full_coords.extend([(-1, -1)] + input_coords + [(-1, -1)])\n",
    "            \n",
    "            full_ids.extend([im_start_id] + output_ids + [im_end_id])\n",
    "            full_coords.extend([(-1, -1)] + output_coords + [(-1, -1)])\n",
    "\n",
    "        test_input_ids, test_input_coords = self._serialize_grid(task_data[\"test\"][0][\"input\"])\n",
    "        full_ids.extend([im_start_id] + test_input_ids + [im_end_id])\n",
    "        full_coords.extend([(-1, -1)] + test_input_coords + [(-1, -1)])\n",
    "\n",
    "        target_start_index = len(full_ids)\n",
    "\n",
    "        test_output_ids, test_output_coords = self._serialize_grid(task_data[\"test\"][0][\"output\"])\n",
    "        full_ids.extend([im_start_id] + test_output_ids + [im_end_id])\n",
    "        full_coords.extend([(-1, -1)] + test_output_coords + [(-1, -1)])\n",
    "        full_ids.append(self.tokenizer.eos_token_id)\n",
    "        full_coords.append((-1, -1))\n",
    "\n",
    "        labels = list(full_ids[1:]) + [-100] \n",
    "        \n",
    "        for i in range(target_start_index):\n",
    "            labels[i] = -100\n",
    "            \n",
    "        return full_ids, labels, full_coords\n",
    "\n",
    "    def serialize_for_inference(self, task_data: dict[str, Any]) -> tuple[list[int], list[tuple[int, int]]]:\n",
    "        prompt_ids: list[int] = [self.tokenizer.bos_token_id]\n",
    "        prompt_coords: list[tuple[int, int]] = [(-1, -1)]\n",
    "\n",
    "        im_start_id = self.tokenizer.vocab[\"<im_start>\"]\n",
    "        im_end_id = self.tokenizer.vocab[\"<im_end>\"]\n",
    "\n",
    "        for pair in task_data[\"train\"]:\n",
    "            input_ids, input_coords = self._serialize_grid(pair[\"input\"])\n",
    "            output_ids, output_coords = self._serialize_grid(pair[\"output\"])\n",
    "\n",
    "            prompt_ids.extend([im_start_id] + input_ids + [im_end_id])\n",
    "            prompt_coords.extend([(-1, -1)] + input_coords + [(-1, -1)])\n",
    "\n",
    "            prompt_ids.extend([im_start_id] + output_ids + [im_end_id])\n",
    "            prompt_coords.extend([(-1, -1)] + output_coords + [(-1, -1)])\n",
    "\n",
    "        test_input_ids, test_input_coords = self._serialize_grid(task_data[\"test\"][0][\"input\"])\n",
    "        prompt_ids.extend([im_start_id] + test_input_ids + [im_end_id, im_start_id])\n",
    "        prompt_coords.extend([(-1, -1)] + test_input_coords + [(-1, -1), (-1, -1)])\n",
    "\n",
    "        return prompt_ids, prompt_coords\n",
    "\n",
    "\n",
    "class ArcCollator:\n",
    "    def __init__(self, tokenizer: ArcColorTokenizer, max_len: int):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.serializer = GridSerializer(tokenizer)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_sample_entropy(labels: list[int]) -> float:\n",
    "        valid_labels = [l for l in labels if l != -100]\n",
    "        if not valid_labels:\n",
    "            return 0.0\n",
    "\n",
    "        counts = torch.bincount(torch.tensor(valid_labels))\n",
    "        probs = counts.float() / len(valid_labels)\n",
    "        probs = probs[probs > 0]\n",
    "        return -torch.sum(probs * torch.log2(probs)).item()\n",
    "\n",
    "    def __call__(self, batch: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "        all_input_ids, all_labels, all_coords, all_entropies = [], [], [], []\n",
    "\n",
    "        for task_data in batch:\n",
    "            input_ids, labels, coords = self.serializer.serialize_task(task_data)\n",
    "\n",
    "            if len(input_ids) > self.max_len:\n",
    "                continue\n",
    "\n",
    "            entropy = self._calculate_sample_entropy(labels)\n",
    "            all_entropies.append(entropy)\n",
    "            all_input_ids.append(torch.tensor(input_ids, dtype=torch.long))\n",
    "            all_labels.append(torch.tensor(labels, dtype=torch.long))\n",
    "            all_coords.append(torch.tensor(coords, dtype=torch.long))\n",
    "\n",
    "        if not all_input_ids:\n",
    "            return {}\n",
    "\n",
    "        padded_input_ids = pad_sequence(all_input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        padded_labels = pad_sequence(all_labels, batch_first=True, padding_value=-100)\n",
    "        padded_coords = pad_sequence(all_coords, batch_first=True, padding_value=-1)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": padded_input_ids,\n",
    "            \"labels\": padded_labels,\n",
    "            \"coords\": padded_coords,\n",
    "            \"task_data\": batch,\n",
    "            \"sample_entropy\": torch.tensor(all_entropies, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rich.console import Console\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _jsd_from_distributions(p_dist_unnorm: torch.Tensor, q_dist_unnorm: torch.Tensor) -> torch.Tensor:\n",
    "    epsilon = 1e-9\n",
    "    p_dist = p_dist_unnorm / (p_dist_unnorm.sum(dim=-1, keepdim=True) + epsilon)\n",
    "    q_dist = q_dist_unnorm / (q_dist_unnorm.sum(dim=-1, keepdim=True) + epsilon)\n",
    "    m_dist = 0.5 * (p_dist + q_dist)\n",
    "    kl_p_m = torch.sum(p_dist * (torch.log(p_dist + epsilon) - torch.log(m_dist + epsilon)), dim=-1)\n",
    "    kl_q_m = torch.sum(q_dist * (torch.log(q_dist + epsilon) - torch.log(m_dist + epsilon)), dim=-1)\n",
    "    return (0.5 * kl_p_m + 0.5 * kl_q_m).mean()\n",
    "\n",
    "\n",
    "def _get_task_representation(tensors: list[torch.Tensor]) -> torch.Tensor | None:\n",
    "    if not tensors or any(t.ndim != 3 for t in tensors) or tensors[0].shape[0] > 1:\n",
    "        return None\n",
    "\n",
    "    matrix = torch.cat(tensors, dim=2).squeeze(0).float()\n",
    "\n",
    "    if matrix.numel() == 0:\n",
    "        return None\n",
    "\n",
    "    if matrix.shape[0] < 2:\n",
    "        mean_vec = matrix.mean(dim=0)\n",
    "        std_vec = torch.zeros_like(mean_vec)\n",
    "    else:\n",
    "        mean_vec = matrix.mean(dim=0)\n",
    "        std_vec = matrix.std(dim=0, unbiased=False)\n",
    "\n",
    "    return torch.cat([mean_vec, std_vec], dim=0)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _augment_and_map_kernel(grids: list[torch.Tensor], transform_idx: int, color_map: torch.Tensor) -> list[torch.Tensor]:\n",
    "    transformed_grids = []\n",
    "    for x in grids:\n",
    "        if transform_idx == 0:\n",
    "            transformed_x = x\n",
    "        elif transform_idx == 1:\n",
    "            transformed_x = torch.rot90(x, 1, [0, 1])\n",
    "        elif transform_idx == 2:\n",
    "            transformed_x = torch.rot90(x, 2, [0, 1])\n",
    "        elif transform_idx == 3:\n",
    "            transformed_x = torch.rot90(x, 3, [0, 1])\n",
    "        elif transform_idx == 4:\n",
    "            transformed_x = torch.flip(x, [0])\n",
    "        elif transform_idx == 5:\n",
    "            transformed_x = torch.flip(x, [1])\n",
    "        elif transform_idx == 6:\n",
    "            transformed_x = torch.transpose(x, 0, 1)\n",
    "        else:\n",
    "            transformed_x = torch.rot90(torch.flip(x, [0]), 1, [0, 1])\n",
    "        transformed_grids.append(color_map[transformed_x])\n",
    "    return transformed_grids\n",
    "\n",
    "class InMemoryArcDataset(Dataset):\n",
    "    def __init__(self, challenges_path: str, solutions_path: str):\n",
    "        with open(challenges_path, 'r') as f:\n",
    "            challenges = json.load(f)\n",
    "        with open(solutions_path, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "\n",
    "        self.tasks = []\n",
    "        for task_id, task_data in challenges.items():\n",
    "            if task_id in solutions:\n",
    "                # Ensure the 'test' list exists and is not empty\n",
    "                if \"test\" not in task_data:\n",
    "                    task_data[\"test\"] = [{}]\n",
    "                elif not task_data[\"test\"]:\n",
    "                    task_data[\"test\"].append({})\n",
    "                \n",
    "                # Merge the solution into the corresponding test case\n",
    "                task_data[\"test\"][0][\"output\"] = solutions[task_id][0]\n",
    "                self.tasks.append(task_data)\n",
    "\n",
    "        # Sort tasks by sequence length for efficiency, similar to the original implementation\n",
    "        tokenizer_for_sorting = ArcColorTokenizer()\n",
    "        serializer_for_sorting = GridSerializer(tokenizer_for_sorting)\n",
    "        tasks_with_lengths = [(task, len(serializer_for_sorting.serialize_task(task)[0])) for task in self.tasks]\n",
    "        self.tasks = [task for task, length in sorted(tasks_with_lengths, key=lambda x: x[1])]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tasks)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict[str, Any]:\n",
    "        return self.tasks[idx]\n",
    "\n",
    "\n",
    "\n",
    "class MinimalObserver:\n",
    "    def __init__(self, console: Console, config: TrainConfig):\n",
    "        self.console, self.config = console, config\n",
    "\n",
    "    def calculate_metrics(self, main_loss: torch.Tensor, model_outputs: dict, signals: dict) -> dict[str, float]:\n",
    "        logits, labels = model_outputs[\"logits\"], model_outputs[\"labels\"]\n",
    "        raw_weights = model_outputs.get(\"raw_weights\", [])\n",
    "        mask = labels[:, 1:] != -100\n",
    "        active_logits = logits[:, :-1, :][mask]\n",
    "        acc = (torch.argmax(active_logits, dim=-1) == labels[:, 1:][mask]).float().mean().item() if mask.any() else 0.0\n",
    "        act_rates = [rw.gt(0).float().mean().item() for rw in raw_weights] if raw_weights else [0.0]\n",
    "        routing_failure_rate = sum(torch.all(rw == 0, dim=-1).float().mean().item() for rw in raw_weights) / len(raw_weights) if raw_weights else 0.0\n",
    "        \n",
    "        flat_logits = torch.cat([rl.detach().float().view(-1) for rl in model_outputs.get(\"routing_logits\", []) if rl.numel() > 0])\n",
    "        metrics = {\"main_loss\": main_loss.item(), \"token_acc\": acc, \"route_jsd_loss\": signals.get(\"route_jsd_loss\", torch.tensor(0.0)).item(),\n",
    "                   \"sample_entropy\": model_outputs.get(\"sample_entropy\", torch.tensor(0.0)).mean().item(), \"seq_len\": float(labels.shape[1]),\n",
    "                   \"activation_rate_avg\": sum(act_rates) / len(act_rates) if act_rates else 0.0, \"routing_failure_rate\": routing_failure_rate}\n",
    "        if flat_logits.numel() > 0:\n",
    "            metrics.update({\"gate_logit_avg\": flat_logits.mean().item(), \"gate_logit_sigma\": flat_logits.std().item()})\n",
    "        return metrics\n",
    "\n",
    "    def log_step(self, epoch: int, step: int, task_idx: int, view_idx: int, metrics: dict, elapsed_time: float):\n",
    "        steps_per_sec = 1 / elapsed_time if elapsed_time > 0 else float(\"inf\")\n",
    "        log_str = (f\"E{epoch} S{step} T{task_idx} V{view_idx} | L({metrics['main_loss']:.3f}/{metrics['route_jsd_loss']:.4f}) | \"\n",
    "                   f\"Acc: {metrics['token_acc']:.3f} | Act%: {metrics['activation_rate_avg']*100:.1f} | \"\n",
    "                   f\"Fail: {metrics['routing_failure_rate']*100:.1f}% | Speed: {steps_per_sec:.2f} st/s\")\n",
    "        self.console.print(log_str)\n",
    "\n",
    "\n",
    "\n",
    "class LearningDynamics:\n",
    "    def __init__(self, config: TrainConfig, model: nn.Module, computation_params: list, routing_params_with_names: list[tuple[str, nn.Parameter]]):\n",
    "        self.config, self.model = config, model\n",
    "        self.computation_params, self.routing_params_with_names = computation_params, routing_params_with_names\n",
    "        self.routing_params = [p for _, p in routing_params_with_names]\n",
    "        self.optimizer_comp = torch.optim.AdamW(computation_params, lr=self.config.lr)\n",
    "        self.optimizer_route = torch.optim.AdamW(self.routing_params, lr=self.config.lr)\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.jit.script\n",
    "    def _calculate_jsd_loss(p_logits: torch.Tensor, q_logits: torch.Tensor) -> torch.Tensor:\n",
    "        return _jsd_from_distributions(F.relu(mas_normalize(p_logits)), mas_normalize(q_logits).detach())\n",
    "\n",
    "    def compute_and_apply_gradients(self, main_loss: torch.Tensor, model_outputs: dict, device: torch.device) -> dict[str, Any]:\n",
    "        self.optimizer_comp.zero_grad(); self.optimizer_route.zero_grad()\n",
    "        computation_outputs, masked_outputs = model_outputs[\"computation_outputs\"], model_outputs[\"masked_outputs\"]\n",
    "        mu_weights = [p for name, p in self.model.named_parameters() if \"mu_weight\" in name]\n",
    "        params_to_grad = self.computation_params + computation_outputs + mu_weights\n",
    "        all_grads = torch.autograd.grad(main_loss, params_to_grad, retain_graph=True, allow_unused=True)\n",
    "        \n",
    "        comp_grads = all_grads[:len(self.computation_params)]\n",
    "        intermediate_grads = all_grads[len(self.computation_params):len(self.computation_params) + len(computation_outputs)]\n",
    "        mu_weight_grads = all_grads[len(self.computation_params) + len(computation_outputs):]\n",
    "\n",
    "        c_output_norms = [torch.norm(g, p=2, dim=(0, 1)) for g in intermediate_grads if g is not None]\n",
    "        c_param_norms = [torch.norm(g, p=2, dim=-1) for g in mu_weight_grads if g is not None]\n",
    "        i_effective_norms = [torch.norm(mo, p=2, dim=(0, 1)) for mo in masked_outputs if mo is not None]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_goodness = []\n",
    "            num_modules = min(len(i_effective_norms), len(c_output_norms), len(c_param_norms))\n",
    "            for i in range(num_modules):\n",
    "                benefit_eff, benefit_rel = mas_normalize(i_effective_norms[i]), mas_normalize(c_output_norms[i])\n",
    "                synergistic_benefit = mas_normalize(benefit_eff * benefit_rel)\n",
    "                learning_cost = mas_normalize(c_param_norms[i])\n",
    "                all_goodness.append(F.relu(synergistic_benefit / (learning_cost + 1e-9)))\n",
    "\n",
    "        for param, grad in zip(self.computation_params, comp_grads):\n",
    "            if grad is not None: param.grad = grad.clone()\n",
    "        \n",
    "        meta_losses = [self._calculate_jsd_loss(logit, good) for logit, good in zip(model_outputs[\"routing_logits\"], all_goodness) if logit.numel() > 0 and good.numel() > 0 and logit.shape[-1] == good.shape[-1]]\n",
    "        if meta_losses:\n",
    "            total_meta_loss = self.config.w_route_jsd * torch.stack(meta_losses).mean()\n",
    "            meta_grads = torch.autograd.grad(total_meta_loss, self.routing_params, allow_unused=True)\n",
    "            for param, grad in zip(self.routing_params, meta_grads):\n",
    "                if grad is not None: param.grad = grad.clone()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(self.computation_params, max_norm=1.0)\n",
    "        if IS_TPU:\n",
    "            xm.optimizer_step(self.optimizer_comp)\n",
    "            xm.optimizer_step(self.optimizer_route)\n",
    "            xm.mark_step()\n",
    "        else:\n",
    "            self.optimizer_comp.step()\n",
    "            self.optimizer_route.step()\n",
    "        return {\"route_jsd_loss\": torch.stack(meta_losses).mean() if meta_losses else torch.tensor(0.0)}\n",
    "\n",
    "\n",
    "\n",
    "class KaggleTrainer:\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        self.config, self.device = config, torch.device(config.device)\n",
    "        torch.manual_seed(config.seed)\n",
    "        self.console = Console()\n",
    "        self.observer = MinimalObserver(self.console, config)\n",
    "        self.tokenizer, self.serializer = ArcColorTokenizer(), GridSerializer(ArcColorTokenizer())\n",
    "        self._setup_data()\n",
    "        self._setup_model_and_optimizer()\n",
    "        self.checkpoint_dir = Path(config.checkpoint_dir)\n",
    "        self.global_step, self.epoch, self.start_task_idx, self.start_view_idx = 0, 0, 0, 0\n",
    "\n",
    "    def _setup_data(self):\n",
    "        self.train_dataset = InMemoryArcDataset(\n",
    "            challenges_path=self.config.data.challenges_path,\n",
    "            solutions_path=self.config.data.solutions_path\n",
    "        )\n",
    "        collator = ArcCollator(self.tokenizer, max_len=self.config.model.max_position_embeddings)\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.config.data.batch_size,\n",
    "            collate_fn=lambda x: x,  # We process one task at a time, so no collation needed at loader level\n",
    "            num_workers=self.config.data.num_workers,\n",
    "            shuffle=False, # We iterate sequentially\n",
    "        )\n",
    "\n",
    "    def _setup_model_and_optimizer(self):\n",
    "        self.config.model.vocab_size = self.tokenizer.vocab_size\n",
    "        self.model = ArcTransformer(self.config.model, device=self.device).to(self.device)\n",
    "        computation_params = [p for name, p in self.model.named_parameters() if \"proto_weight\" not in name and \"gate_param\" not in name]\n",
    "        routing_params_with_names = [(name, p) for name, p in self.model.named_parameters() if \"proto_weight\" in name or \"gate_param\" in name]\n",
    "        self.dynamics = LearningDynamics(self.config, self.model, computation_params, routing_params_with_names)\n",
    "\n",
    "    def _prepare_batch(self, task_data: dict, view_idx: int) -> dict[str, torch.Tensor] | None:\n",
    "        grids_cpu_lists = [pair[k] for pair in task_data[\"train\"] for k in (\"input\", \"output\")]\n",
    "        grids_cpu_lists.extend([task_data[\"test\"][0][\"input\"], task_data[\"test\"][0][\"output\"]])\n",
    "        all_colors = {c for grid in grids_cpu_lists for row in grid for c in row}\n",
    "        active_colors = [c for c in all_colors if c != 0]\n",
    "        color_map_cpu = torch.arange(10, dtype=torch.long)\n",
    "        if len(active_colors) >= 2:\n",
    "            c1, c2 = random.sample(active_colors, 2)\n",
    "            color_map_cpu[c1], color_map_cpu[c2] = c2, c1\n",
    "        augmented_grids = [g.tolist() for g in _augment_and_map_kernel([torch.tensor(g, dtype=torch.long) for g in grids_cpu_lists], view_idx, color_map_cpu)]\n",
    "        \n",
    "        transformed_train, ptr = [], 0\n",
    "        for _ in task_data[\"train\"]:\n",
    "            transformed_train.append({\"input\": augmented_grids[ptr], \"output\": augmented_grids[ptr + 1]}); ptr += 2\n",
    "        augmented_task = {\"train\": transformed_train, \"test\": [{\"input\": augmented_grids[ptr], \"output\": augmented_grids[ptr + 1]}]}\n",
    "        \n",
    "        ids, labels, coords = self.serializer.serialize_task(augmented_task)\n",
    "        if len(ids) > self.config.model.max_position_embeddings: return None\n",
    "        return {\"input_ids\": torch.tensor([ids], dtype=torch.long), \"labels\": torch.tensor([labels], dtype=torch.long), \"coords\": torch.tensor([coords], dtype=torch.long), \"sample_entropy\": torch.tensor([ArcCollator._calculate_sample_entropy(labels)], dtype=torch.float32)}\n",
    "\n",
    "    def _train_step(self, batch: dict, epoch: int, task_idx: int, view_idx: int):\n",
    "        start_time = time.time()\n",
    "        self.model.train()\n",
    "        with sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION), torch.autocast(device_type=self.config.device, dtype=DTYPE):\n",
    "            model_outputs = self.model(batch[\"input_ids\"], coords=batch[\"coords\"], return_dict=True)\n",
    "            main_loss = F.cross_entropy(model_outputs[\"logits\"][:, :-1, :].contiguous().view(-1, self.config.model.vocab_size), batch[\"labels\"][:, 1:].contiguous().view(-1), ignore_index=-100)\n",
    "        \n",
    "        if not torch.isfinite(main_loss):\n",
    "            self.console.print(f\"[bold red]NaN detected in main_loss at step {self.global_step}. Aborting step.[/bold red]\"); return None\n",
    "        \n",
    "        signals = self.dynamics.compute_and_apply_gradients(main_loss, model_outputs, self.device)\n",
    "        model_outputs.update({\"labels\": batch[\"labels\"], \"sample_entropy\": batch[\"sample_entropy\"]})\n",
    "        metrics = self.observer.calculate_metrics(main_loss, model_outputs, signals)\n",
    "        \n",
    "        # Only log and save checkpoints on the master process\n",
    "        if not IS_TPU or xm.is_master_ordinal():\n",
    "             if self.global_step % self.config.log_interval == 0:\n",
    "                self.observer.log_step(epoch, self.global_step, task_idx, view_idx, metrics, time.time() - start_time)\n",
    "                self._save_checkpoint(task_idx, view_idx)\n",
    "\n",
    "        self.global_step += 1\n",
    "        return metrics\n",
    "\n",
    "    def _train_epoch(self, epoch: int):\n",
    "        dataset = self.train_dataset\n",
    "        for task_idx in range(self.start_task_idx, len(dataset)):\n",
    "            task_data = dataset[task_idx]\n",
    "            start_view = self.start_view_idx if task_idx == self.start_task_idx else 0\n",
    "            for view_idx in range(start_view, 8):\n",
    "                batch_cpu = self._prepare_batch(task_data, view_idx)\n",
    "                if not batch_cpu:\n",
    "                    if not IS_TPU or xm.is_master_ordinal():\n",
    "                        self.console.print(f\"[yellow]Skipping Task {task_idx} View {view_idx} due to excessive length.[/yellow]\")\n",
    "                    continue\n",
    "\n",
    "                batch = {k: v.to(self.device) for k, v in batch_cpu.items()}\n",
    "                \n",
    "                converged = False\n",
    "                for step in range(500):\n",
    "                    metrics = self._train_step(batch, epoch, task_idx, view_idx)\n",
    "                    if not metrics:\n",
    "                        break\n",
    "                    if metrics[\"main_loss\"] <= 0.03 and metrics[\"token_acc\"] >= 0.999:\n",
    "                        if not IS_TPU or xm.is_master_ordinal():\n",
    "                            self.console.print(f\"Task {task_idx} view {view_idx} converged in {step + 1} steps.\")\n",
    "                        converged = True\n",
    "                        break\n",
    "                \n",
    "                if not converged and (not IS_TPU or xm.is_master_ordinal()):\n",
    "                    self.console.print(f\"[red]Task {task_idx} view {view_idx} hit MAX_STEPS.[/red]\")\n",
    "\n",
    "            self.start_view_idx = 0\n",
    "        self.start_task_idx, self.start_view_idx = 0, 0\n",
    "\n",
    "    def train(self):\n",
    "        self._load_checkpoint()\n",
    "        self.console.print(\"[bold green]Starting Training...[/bold green]\")\n",
    "        for epoch in range(self.epoch, self.config.num_epochs):\n",
    "            self.epoch = epoch; self._train_epoch(epoch)\n",
    "        self.console.print(\"[bold green]Training Finished.[/bold green]\")\n",
    "\n",
    "    def _save_checkpoint(self, task_idx: int, view_idx: int):\n",
    "        state = {\"epoch\": self.epoch, \"step\": self.global_step, \"task_idx\": task_idx, \"view_idx\": view_idx,\n",
    "                 \"model_state_dict\": self.model.state_dict(), \"optimizer_comp_state_dict\": self.dynamics.optimizer_comp.state_dict(),\n",
    "                 \"optimizer_route_state_dict\": self.dynamics.optimizer_route.state_dict()}\n",
    "        path = self.checkpoint_dir / f\"checkpoint_{self.global_step}.pt\"\n",
    "        torch.save(state, path)\n",
    "        # Rotate checkpoints\n",
    "        ckpts = sorted(self.checkpoint_dir.glob(\"*.pt\"), key=os.path.getmtime)\n",
    "        if len(ckpts) > self.config.max_checkpoints:\n",
    "            os.remove(ckpts[0])\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        # Priority 1: Check working directory for existing session checkpoints\n",
    "        ckpts = sorted(self.checkpoint_dir.glob(\"*.pt\"), key=os.path.getmtime, reverse=True)\n",
    "        if ckpts:\n",
    "            try:\n",
    "                ckpt = torch.load(ckpts[0], map_location=self.device)\n",
    "                self.model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "                self.dynamics.optimizer_comp.load_state_dict(ckpt[\"optimizer_comp_state_dict\"])\n",
    "                self.dynamics.optimizer_route.load_state_dict(ckpt[\"optimizer_route_state_dict\"])\n",
    "                self.global_step, self.epoch, self.start_task_idx, self.start_view_idx = ckpt[\"step\"], ckpt[\"epoch\"], ckpt[\"task_idx\"], ckpt[\"view_idx\"]\n",
    "                self.console.print(f\"[bold green]Loaded session checkpoint from {ckpts[0]} at step {self.global_step}.[/bold green]\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                self.console.print(f\"[bold red]Failed to load session checkpoint {ckpts[0]}: {e}. Starting from scratch.[/bold red]\")\n",
    "                return\n",
    "\n",
    "        # Priority 2: If no session checkpoints, try to load the initial checkpoint from input\n",
    "        initial_ckpt_path = Path(\"/kaggle/input/tiny-onn-arc/pytorch/default/1/checkpoint_21440.pt\")\n",
    "        if initial_ckpt_path.is_file():\n",
    "            try:\n",
    "                ckpt = torch.load(initial_ckpt_path, map_location=self.device)\n",
    "                self.model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "                self.dynamics.optimizer_comp.load_state_dict(ckpt[\"optimizer_comp_state_dict\"])\n",
    "                self.dynamics.optimizer_route.load_state_dict(ckpt[\"optimizer_route_state_dict\"])\n",
    "                self.global_step, self.epoch, self.start_task_idx, self.start_view_idx = ckpt[\"step\"], ckpt[\"epoch\"], ckpt[\"task_idx\"], ckpt[\"view_idx\"]\n",
    "                self.console.print(f\"[bold green]Loaded initial checkpoint from {initial_ckpt_path} at step {self.global_step}.[/bold green]\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                self.console.print(f\"[bold red]Failed to load initial checkpoint {initial_ckpt_path}: {e}. Starting from scratch.[/bold red]\")\n",
    "                return\n",
    "\n",
    "        # Final fallback\n",
    "        self.console.print(\"[yellow]No valid checkpoints found. Starting from scratch.[/yellow]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6722f0",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceefabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\" Starting training process...\")\n",
    "    config = TrainConfig()\n",
    "    torch.manual_seed(config.seed)\n",
    "    random.seed(config.seed)\n",
    "\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        IS_TPU = True\n",
    "    except ImportError:\n",
    "        IS_TPU = False\n",
    "    \n",
    "    if IS_TPU:\n",
    "        config.device = xm.xla_device()\n",
    "    \n",
    "    trainer = KaggleTrainer(config)\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\" Training process finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
