# DynNSIHA: 实验归档与最终结论

> **最终结论**: `DynNSIHA` 路线已被正式放弃。实验证据表明，在 `DynSIHA` 的神经元级动态稀疏机制之上，再叠加一个块级稀疏机制，不仅存在理论上的冗余，更会在实践中导致“过度稀疏化”和模型学习能力的崩溃。`DynSIHA` 已被确认为当前阶段更优越、更自洽的注意力架构。

## 1. 初始构想：理论优雅与工程可行性的融合

`DynNSIHA` 的初始构想，是为 `Tiny-ONN` 项目设计下一代注意力机制。它旨在将两种看似正交的稀疏化方案进行一次深度、互补的融合，以解决 `DynSIHA` 在当前硬件上因“伪稠密”计算而产生的高昂性能开销。

- **`DynSIHA` (Dynamic Sparse Infinite-Head Attention)**: 提供**理论上的优雅**。它通过 `SBL` (稀疏贝叶斯层) 为每个 token 动态地、内容感知地合成专属的 QKV 投影函数，实现了**神经元级别**的、涌现式的软性稀疏。
- **`NSA` (Native Sparse Attention)**: 提供**工程上的可行性**。它利用硬件友好的计算模式（如分块 Top-K 选择），在**宏观块级别**上高效地执行稀疏计算。

其核心假设是：我们可以在 `DynSIHA` 动态合成的高质量 QKV 表征之上，应用 `NSA` 的高效计算核，从而实现理论表达力与硬件性能的统一。

---

## 2. 实验过程与暴露的核心问题

我们在 [`exp/DynNSIHA_PoC.py`](exp/DynNSIHA_PoC.py) 中对该构想进行了一系列 PoC 实验。整个过程暴露了从实现到理论基础的多个核心问题：

1. **静态 Top-K 的失效**: 初版实现中，静态的 `top-k` 块选择机制由于分块粒度设置不当 (`block_size=64`)，对于 `SEQ_LEN=256` 的上下文来说过大，导致总块数过少，无法形成有意义的稀疏选择。

2. **动态门控的崩溃**: 在修复 `top-k` 逻辑并引入基于模型不确定性 (`τ`) 的动态门控后，我们观察到了更深层次的问题：当模型在简单任务上迅速过拟合时，`τ` 趋近于零，导致门控阈值过高，注意力机制被完全关闭 (`AttnBlk% -> 0.00%`)。这证明了该门控机制在设计上存在根本性缺陷，过于脆弱。

3. **双重稀疏的理论冲突 (根本性问题)**: 最关键的发现是，`DynSIHA` 和 `NSA` 的稀疏化哲学存在根本性冲突。
   - `DynSIHA` 的 `SBL` 模块已经是一个**内生的、神经元级别的动态稀疏化**机制。它通过学习，将不重要的信息在 QKV 投影阶段就进行了过滤。
   - `NSA` 是一个**外生的、宏观块级别的静态/动态稀疏化**机制。
   - 将两者叠加，实际上是在一个已经被细粒度过滤的信息流上，再进行一次粗粒度的过滤。实验证明，这种“双重稀疏”会过度惩罚信息流动，破坏了 `SBL` 自身的自组织过程，最终导致模型无法有效学习。

---

## 3. 最终战略决策：放弃 DynNSIHA 路线

基于上述确凿的实验证据和理论反思，我们做出以下战略决策：

- **`DynNSIHA` 路线被正式放弃**。它被证明是一次理论上不自洽、实践中不可行的尝试。
- **回归并聚焦 `DynSIHA`**: `DynSIHA` 的 PoC 实验（[`exp/MoIE_PoC.py`](exp/MoIE_PoC.py)）已经独立地证明了 `SBL` 作为动态函数合成器和神经元级稀疏化机制的巨大潜力。未来的研究和工程重心将完全回归到优化和扩展 `DynSIHA` 及其衍生的 `MoIEFFN` 架构上。
- **核心洞察**: 真正的性能优化不应通过在 `SBL` 之上叠加额外的稀疏机制来实现，而应聚焦于如何通过硬件感知的设计（如 `BCPT` 范式）来直接加速 `SBL` 本身的“伪稠密”计算。

这份文档标志着 `DynNSIHA` 探索性研究的结束。所有相关发现和教训已被吸收，并将指导 `Tiny-ONN` 项目未来的发展方向。
