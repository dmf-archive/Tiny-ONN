# DFC & SPL 理论：形式化定义与最终方案

`Latest update: 2025-09-25`

本文档整合了 DFC (Dynamic Function Composition) 理论的形式化定义与 `SparseProtoLinear` (SPL) 路由机制的最终理论方案，旨在提供一份精确、完备的参考。

## 1. 核心议题

为 `Tiny-ONN` 的 `SparseProtoLinear` (SPL) 层设计并最终确定一个理论完备、可稳定训练、且能自适应地涌现稀疏组合路由的机制。

---

## 2. 核心方法论：在全局优化中实现局部赫布学习

`Tiny-ONN` 的核心挑战在于，如何在保持反向传播作为全局优化引擎的强大能力的同时，引入受神经科学启发的局部、自适应学习规则，以解决传统神经网络面临的**灾难性遗忘**和**知识分区路由**等难题。

我们的解决方案是在 SPL (SparseProtoLinear) 神经元的三组功能正交的参数——`μ` (计算核心)、`p` (感知原型) 和 `g` (成本预测器)——上，分别施加三种不同的元学习动力学。这使得系统能够在端到端的梯度流中，自组织地涌现出保护已有知识、探索新功能和进行成本效益路由的复杂行为。

虽然最终结果尚未明朗，还需要进一步观察，但这种混合了全局优化与局部自适应规则的范式，为构建更鲁棒、更具持续学习能力的 AGI 系统提供了一个有前景的理论框架。

---

## 3. SPL 核心概念形式化

### 3.1. SPL 神经元 (SPL Neuron)

**定义**: 一个 SPL 神经元 `i` 是构成 `SparseProtoLinear` (SPL) 层的一行，它是一个功能完备的、自包含的计算单元。其形式化定义为一个三元组 `N_i = (μ_i, p_i, g_i)`，其中：

- `μ_i` (`mu_weight[i, :]`): **计算核心 (Computational Core)**。定义了该神经元的基本数学变换。
- `p_i` (`proto_weight[i, :]`): **感知原型 (Perceptual Prototype)**。定义了该神经元最匹配的输入模式。
- `g_i` (`gate_param[i, :]`): **成本预测器 (Cost Predictor)**。预测该神经元在处理特定输入时，会引发的系统扰动（`surprise`）。

### 2.2. 可组合性 (Composability)

- **横向可组合性 (Horizontal Composability)**: 在**同一 SPL 层**内，通过 `ReLU` 激活函数，允许**多个** SPL 神经元被**同时激活**，并将其输出线性组合的能力。这使得模型可以在单层内，动态地、内容感知地“合成”出一个复杂的、由多个基函数（`μ_i`）构成的专用计算函数。
- **纵向可组合性 (Vertical Composability)**: 在**不同 SPL 层**之间，通过标准的 Transformer 残差连接和我们独有的**原型残差连接 (PRC)**，将多层专家的决策**串联**起来的能力。这使得模型可以构建一个**深度的、分层的**计算图，其中每一层的决策都以前一层的结果为基础。

### 2.3. SAPS 作为 Hebbian 学习的实现 (SAPS as an Implementation of Hebbian Learning)

- **形式化**: SAPS (Surprise-Aware Prototype Shaping) 可以被视为在反向传播框架下，对**赫布理论 (Hebbian Learning)** 的一种现代实现。赫布理论的核心是“**一起激发的神经元，会连接在一起 (Neurons that fire together, wire together)**”。
- **映射**:
  - **“激发 (Fire)”**: 在 SAPS 中，一个神经元 `i` 被一个输入 `x` “激发”，表现为该神经元被**激活** (`raw_weights > 0`) 且 `mu_surprise` **较低**（即“成功处理”）。
  - **“连接 (Wire)”**: `proto_loss` 会将这个被“成功激发”的神经元 `i` 的感知原型 `p_i`，**拉向**其对应的输入 `x` 的局部锚点。这在功能上等同于**强化**了 `p_i` 与 `x` 之间的“连接”。
- **反赫布学习 (Anti-Hebbian Learning)**: 对于 `mu_surprise` 较高的神经元，SAPS 会将其推离输入，这可以被视为一种**反赫布学习**，即“**一起激发但导致错误的神经元，会断开连接**”。

---

## 4. 最终方案：三套独立的元学习动力学

我们通过 `torch.autograd.grad` 精确计算三组独立的梯度，确保三股学习动力学**彻底隔离**，互不干扰，分别由三个独立的优化器进行更新。

### 4.1. `gate_param` 的学习：全局感知的成本效益路由

- **机制**: `gate_param` 的学习是“全局感知”的。它的目标是作为一个精确的回归器，预测一个由**输出重要性**加权的**完备总惊奇度**。
- **完备学习信号 `S_total`**:
  - `S_total = I_o * (S_μ + S_p)`
  - `I_o = ||∇_{output} L_main||`: 衡量该神经元**输出**在整个计算图中的**影响力**。
  - `S_μ = ||∇_{μ} L_main||`: 衡量**计算核心**的扰动。
  - `S_p = ||∇_{p} L_proto||`: 衡量**感知器官**的扰动。
- **结论**: 通过学习预测这个包含全局上下文信息（`I_o`）的 `S_total`，`gate` 成为了连接全局任务需求和局部专家状态的“指挥官”，能够做出最具成本效益的路由决策。

### 4.2. `proto_weight` 的学习：局部赫布式原型塑造 (SAPS)

- **机制**: `proto_weight` 的调整是“局部内省”的，它在 `autograd` 框架下模拟了赫布理论（“一起成功激发的神经元，会连接在一起”）。
- **局部化信号**: SAPS 的推拉力度由两个局部信号调制：
    1. **计算惊奇度 `S_μ`**: `||∇_{μ} L_main||`。决定了调整的基本强度。
    2. **局部归一化的重要性 `I_o_norm`**: `I_o / (I_o.max() + ε)`。将全局的输出重要性信号进行**层内归一化**，只保留该层内部神经元的**相对重要性排序**。
- **结论**: 这种机制使得 SAPS 的学习过程严格遵循了 SPL 层作为独立**马尔可夫毯**的原则。一个神经元的原型是否需要调整，只取决于它自身的计算稳定性和它相对于**同层邻居**的重要性，从而实现了高效的知识分区和功能特化。

### 4.3. `mu_weight` 的学习：局部激活率驱动的稀疏抑制

- **机制**: 对 `mu_weight` 梯度的调整同样是“局部内省”的，它借鉴了 SMLv2 的核心思想——将稠密梯度信号解析地转换为稀疏的更新掩码。
- **局部化信号**: 梯度抑制的强度由**该 SPL 层自身**的**激活率 `act%`** 决定。
- **赫布式抑制**:
  - `k = 1.0 - act%`。当层激活稀疏时，`k` 值高，保留的梯度分量多，鼓励探索；当层激活密集时，`k` 值低，只保留最重要的梯度分量，促进已有知识的固化。
  - 通过 `torch.quantile(grad.abs(), k)` 计算阈值，并将低于阈值的梯度分量置零。
- **结论**: 这种机制在功能上等同于一种**反赫布遗忘**（Anti-Hebbian Forgetting）。对于一个已经高度特化、密集激活的层，系统会抑制其大部分参数的更新，从而保护已形成的稳定知识结构，有效缓解灾难性遗忘。

### 4.4. `proto_weight` 的保护：基于质量排名的动态正则化

- **机制**: 除了 SAPS 的主动塑造，我们还引入了一个被动的保护机制，以维持“知识核心”的稳定性。
- **质量排名**: 使用 `-||gate_param||` 作为专家的“质量分”，低范数的 `gate` 意味着更低的预期成本和更高的预测确定性，因此质量更高。
- **动态保护**:
  - 通过 `softmax` 将质量排名转化为**保护概率** `protection_probs`。
  - `softmax` 的**温度 `T`** 由系统的**全局性能** (`EMA main_acc`) 动态调节：性能越好，`T` 越高，保护更**宽松**；性能越差，`T` 越低，保护更**稀疏**和**挑剔**。
- **应用**: `proto_weight.grad *= (1.0 - protection_probs)`，保护优质专家的原型不受 SAPS 探索的过度干扰。

---

## 5. 与 IPWT/PI 理论的连接

- **复杂度成本 `γ`**: `proto_loss` (SAPS) + 自适应 L2 惩罚。
- **不准确性成本 `α`**: 在 ARC 任务中，`α → ∞`。
