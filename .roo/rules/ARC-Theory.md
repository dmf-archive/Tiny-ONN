# Tiny-ONN-ARC 理论备忘录

`Latest update: 2025-10-09`

## H₀: 涌现式可微程序搜索 (Emergent Differentiable Program Search)

ARC 任务中蕴含的抽象算法可以通过梯度下降在通用计算基质中自组织地涌现出来。这一核心假设将模型训练重新定义为一种**程序搜索 (Program Search)** 过程。它认为，解决 ARC 任务所需的复杂、离散的符号逻辑程序，无需被硬编码，而是可以在一个高维、连续的参数空间（即神经网络的权重）中，通过可微优化（梯度下降）被**发现**。模型本身作为一个通用计算基质，通过最小化预测误差，其内部连接和激活模式会自组织地配置，最终在功能上**等价于**一个能够解决该任务的特定算法。

## H₁: 优化动力学 (Optimization Dynamics): Key-in-Lock

ARC 任务的损失函数地形不连续且崎岖，每个 `(task,view)` 对都构成独立的损失子空间，不适合梯度平均。极小值稀疏且尖锐，需要精确收敛。严格禁止梯度平滑或梯度累积等平均操作，以保留梯度信号的精确性。

## H₂: 损失掩码 (Loss Masking): 样本对样本的规则推断

ARC 任务的本质并非简单的输入-输出映射，而是一个规则推断问题。模型必须从训练对中主动推断出底层的抽象规则，再将其应用于测试输入。为此，我们采用了**样本对样本**的损失掩码策略。该策略要求模型预测所有`output`，而不仅仅是`test output`。这一改变为规则推断这一核心过程提供了直接的、多点的监督信号，迫使模型去学习训练对之间共享的抽象规则。
