# Tiny-ONN-ARC 理论备忘录

`Latest update: 2025-10-09`

## H₀: 涌现式可微程序搜索 (Emergent Differentiable Program Search)

ARC 任务中蕴含的抽象算法可以通过梯度下降在通用计算基质中自组织地涌现出来。这一核心假设将模型训练重新定义为一种**程序搜索 (Program Search)** 过程。它认为，解决 ARC 任务所需的复杂、离散的符号逻辑程序，无需被硬编码，而是可以在一个高维、连续的参数空间（即神经网络的权重）中，通过可微优化（梯度下降）被**发现**。模型本身作为一个通用计算基质，通过最小化预测误差，其内部连接和激活模式会自组织地配置，最终在功能上**等价于**一个能够解决该任务的特定算法。

## H₁: 损失掩码 (Loss Masking): 样本对样本的规则推断

ARC 任务的本质并非简单的输入-输出映射，而是一个规则推断问题。模型必须从训练对中主动推断出底层的抽象规则，再将其应用于测试输入。为此，我们采用了**样本对样本**的损失掩码策略。该策略要求模型预测所有`output`，而不仅仅是`test output`。这一改变为规则推断这一核心过程提供了直接的、多点的监督信号，迫使模型去学习训练对之间共享的抽象规则。

## H₂: 差分损失注意力 (Differential Loss Attention)

为了让模型更高效地聚焦于 ARC 任务中的核心“转换”规则，而非简单的“复制”操作，我们引入了一种损失函数层面的动态注意力机制。该机制由**差分辅助交叉熵 (Differential Auxiliary Cross-Entropy)** 和**自适应动态 λ (Adaptive Dynamic λ)** 共同构成。

1. **差分识别**: 系统首先通过比较输入和输出网格，为每个输出 Token 生成一个 `diff_mask`，精确标记出发生变化的像素。
2. **差分损失**: 一个辅助的交叉熵损失 (`diff_loss`) 仅在这些被标记为“变化”的 Token 上计算。这迫使模型集中优化资源来学习转换规则，而不是在易于学习的恒等映射（复制）上浪费计算。
3. **自适应加权 (λ)**: `diff_loss` 的权重 λ 是动态调整的，其值与“不变 Token 数量”和“变化 Token 数量”的比率成正比 (`λ ∝ N_identity / N_diff`)。

这种设计在功能上等价于一个**注意力机制**：当包含核心规则的“变化”信号相对于背景“不变”信号越稀疏时，系统会自动给予其越高的关注度（损失权重）。这完美契合了预测编码（PCT/FEP）理论中，认知系统必须优先处理高“意外”（Prediction Error）信号的核心原则，从而将优化压力精确地施加在最需要学习的地方。
