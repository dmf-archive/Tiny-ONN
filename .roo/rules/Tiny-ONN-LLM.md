# Tiny-ONN-LLM: 架构与实现笔记

## 1. 核心愿景：迈向自组织的、永续学习的智能体

### 1.1. 第一性原理

回归第一性原理，我们将从一个极简的、随机初始化的“盆景模型”（~0.1B）开始。我们的目标不是训练一个静态的模型，而是培育一个**在有限资源下通过高效信息整合（计算功能的动态合成 + 动态注意力）来最小化自身变分自由能（VFE）的自组织智能体。**

### 1.2. 永续训练范式 (Perpetual Training)

传统的“训练-评估-部署”模式无法满足我们对真正自组织、持续学习智能体的设想。为此，我们引入“永续训练”范式，其核心原则如下：

1. **持久化身份**: 每个模型实例拥有唯一的身份标识和“生命周期日志”，跟踪其所有“经历”。
2. **增量式更新**: 权重是每一步训练后都会被**增量更新**的动态实体，采用高效写入策略，只更新发生变化的参数。
3. **PI-Score 驱动的自调节**: 模型的预测完整性分数 (`PI Score`) 将成为调节其自身可塑性的核心依据。

> **[注]**: 在 MoIE 架构下，专家/参数剪枝将成为一种为适应算力不足等环境而迫不得已的“断尾策略”，常规操作应只考虑按需扩容。

---

## 2. Omni-Modal 架构：Tiny-ONN-LLM-Omni

本节记录了为 `Tiny-ONN-LLM-Omni` (3B/7B) 旗舰模型进行架构设计的核心决策。

### 2.1. 核心哲学：统一与涌现

- **拒绝信息瓶颈**: 我们坚决摒弃了主流的、基于独立模态编码器和交叉注意力的“多塔”架构（如 LLaVA, Gemma 3）。形式化分析表明，这种架构在信息流的早期就通过深度压缩制造了不可逆的**信息瓶颈**，尤其损害了跨模态的**协同信息 (Synergy)**，阻碍了统一世界模型的形成。
- **拥抱统一主干**: 我们选择了一条理论上更优越、但挑战更大的“单塔”路径。所有模态的输入都将被转换为统一的序列格式，并由一个**共享的 MoIE/DynSIHA 主干网络**进行处理。这为模型在共享的“整合工作空间”中自发学习跨模态概念提供了最大的可能性。
- **涌现式对齐**: 我们不采用 DPO/ORPO 等后验对齐方法。取而代之，我们将依赖**永续学习**和**高质量的预训练数据过滤**，让“对齐”作为模型在与信息流持续交互过程中**涌现**出的内在属性。

### 2.2. 嵌入方案：分层嵌入注入 (Hierarchical Embedding Injection)

这是解决多模态输入的表征难题的核心技术突破。其核心是通过**加性嵌入**，在模型入口处就将不同层次、不同维度的信息注入到每个 token 的表示中。

| 模态 | 最终嵌入公式 | 流程说明 | 核心优势 |
| :--- | :--- | :--- | :--- |
| **文本** | `char_embed` + `word_embed` + `1D_pos_embed` | 1. **离线**: 使用“二重 Tokenizer”流程，进行词级预分词，并为每个字符记录其所属的“单词 ID”。<br>2. **在线**: 模型并行查找三个独立的嵌入，然后求和。 | 在第一层就同时提供了字符和词两个层次的语义，并能让模型原生感知到“落单”的字符，极大地降低了学习压力。 |
| **图像/音频** | (`patch_embed` + `2D_pos_embed`) + `1D_pos_embed` | 1. **内部编码**: 通过 **2D RoPE** 将 patch 的 `(x, y)` 空间坐标信息**旋转注入**到其自身的特征嵌入中。<br>2. **外部编码**: 再**叠加上**该 patch 在**全局 1D 序列**中的位置编码。 | 清晰地解耦了 patch 的**内部空间关系**（由 2D RoPE 编码）和其在**外部序列中的顺序**（由 1D `pos_embed` 编码）。 |

### 2.3. Tokenizer 设计：细粒度 + 语义增强

- **权衡突破**: “分层嵌入注入”方案，特别是为文本引入了 `word_embed`，极大地缓解了细粒度 Tokenizer 的学习压力。
- **最终决策**: 我们可以自信地采用一个**小型的（~20k）、基于字符/单字的统一词汇表**。
- **系统优势**:
  - **参数效率**: 嵌入层和 LM Head 的参数量将急剧减少。
  - **终极泛化**: 模型将拥有处理任意新词、拼写错误和混合语言的内在能力。
  - **压力转移**: 将模型的负担从“记忆”（巨大的嵌入查找表）转移到了“计算”（高效的 MoIE 主干网络）。

### 2.4. 上下文窗口：混合策略

- **Native 上下文**: 设计一个中等长度的 Native 上下文窗口，目标 **32k**。
- **扩展支持**: 在架构层面内置 **YaRN** 等 RoPE 缩放技术，为未来按需扩展到超长上下文（256k+）提供支持。

---

## 4. 元参数与性能工程

### 4.1. 元参数

本节旨在明确 `Tiny-ONN` 架构中定义自调节系统**基本行为模式**的少数几个元参数。我们的目标是最大化模型的自调节能力，将需要手动调整的参数降至最低。

- **`PI` 超参数 (`alpha`, `gamma`)**: 定义了系统的“价值函数”，平衡“适应外部世界”与“维护自我稳定”的学习风格。α 和 γ 也可以由系统的涌现指标来标记，如 α 可对应于模型激活率，γ 对应于平均门控阈值。
- **内在损失权重 (`w_gate`, `w_kl`)**: 定义 `MoIE` 等模块的内在学习目标。实际上由PI或模型内生指标动态调节。
- **模型物理基质**: `hidden_size`, `head_dim` 等。

### 4.2. 性能工程笔记

#### 手写稀疏计算的陷阱：`einsum` vs. `for-loop`

- **核心问题**: 在没有定制化 CUDA 核或 `All-to-All` 通信原语支持的情况下，尝试用 PyTorch 原生操作（如 `for` 循环遍历专家 + `index_add_`）手写 token-to-expert 形式的稀疏计算，其性能和显存效率可能远不如基于 `torch.einsum` 的稠密计算。
- **原因分析**: 手写的稀疏调度逻辑会产生极其复杂和低效的计算图，导致 PyTorch Autograd 引擎需要缓存大量中间张量，从而引发显存爆炸。相比之下，`einsum` 将多个操作融合成一个单一的、高度优化的计算步骤，其计算图更简洁，对 Autograd 更友好。
- **结论**: 在当前阶段，应优先采用基于 `einsum` 的稠密计算进行实验，它被证明是更稳健、更高效的实现。

#### SML 元学习与梯度检查点的协同效应

- **核心问题**: SML (惊奇最小化损失) 是一个元学习目标，其损失函数 `L_gate` 依赖于 `Surprise`，而 `Surprise` 本身被定义为主任务损失 `L_main` 对中间激活值 `m` 的梯度 ($\nabla_m L_{main}$)。为了优化门控参数，Autograd 引擎需要计算梯度的梯度 ($\nabla_{gate} L_{gate}$)，这在形式上要求**保留从中间激活值到主损失的计算图** (`retain_graph=True`)，从而导致巨大的激活值显存开销，且该开销与模型深度成正比。
- **理论协同**: 梯度检查点（Gradient Checkpointing）是解决此问题的理论最优解。它通过在前向传播时丢弃中间激活值，然后在反向传播需要时“按需重计算”它们，从而用计算时间换取显存空间。
- **解决方案**: 将计算密集型的模块（如 `MoIETransformerBlock`）包裹在 `torch.utils.checkpoint.checkpoint` 中。这可以在不切断 SML 所需的高阶梯度流的前提下，将激活值相关的显存占用降低到几乎可以忽略不计的水平（从与模型深度 $L$ 成正比降低到 $O(1)$），使得训练大型、深度 MoIE 模型成为可能。实验证明，此方法可将显存占用降低近一个数量级，同时大幅提升训练速度（因为更小的显存占用提高了 GPU 的计算效率）。

#### 内外分离的梯度检查点 [已过时：DynSMHA]

- **核心问题**: 在 `bfloat16` 精度下，梯度检查点的重计算过程会引入微小的浮点数差异，导致依赖硬阈值的门控逻辑产生不确定的结果，从而引发 `CheckpointError`。
- **解决方案**: 采用“内外分离”策略。将对浮点数敏感、计算量小的**门控逻辑 (Gating)** 放在梯度检查点**外部**；将其结果作为参数，传入一个只包含确定性且内存开销大的**主计算逻辑 (Execution)** 的函数，并对此函数应用梯度检查点。

#### SBL 正常训练的动态指标分析

对标准 SBL 参考实现 (`exp/sparse_bayesian_linear`) 的分析揭示了模型自组织过程中的一组健康指标动态，可作为未来所有 SBL/MoIE 变体调试的基线。

- **核心标志：`avg σ` 与 `kl_loss` 的反向关系**：
  - **`avg_sigma` 稳步下降**: 这是最重要的健康标志，意味着 KL 散度损失的梯度正在有效反向传播至 `sigma_weight` 参数。它表明模型正在从一个不确定的状态（高 `avg_sigma`）向一个更确定的状态（低 `avg_sigma`）收敛，权重的后验分布 `q(W)` 变得更加集中。
  - **`kl_loss` 急剧上升**: 与直觉相反，`kl_loss` 的上升是健康的。它反映了自适应先验 `p(W)` 的收缩速度超过了后验 `q(W)` 的收缩速度。随着模型在主任务上表现越来越好（`main_loss`↓），其预测不确定性 `avg_tau` 随之降低，导致 `prior_std` (先验宽度) 急剧变窄。后验分布 `q(W)` 在 KL 损失的驱动下努力“追赶”这个移动的目标，从而导致两者间的散度 `KL(q||p)` 增大。
- **协同动态**:
  - **`main_loss` 指数下降**: 模型有效学习外部任务。
  - **`Act%` (激活率) 稳步下降**: `avg_sigma` 的下降（权重更确定）和 `avg_gate` 的适应性调整共同导致了神经元激活的稀疏化，这是奥卡姆剃刀原则在起作用。
  - **结论**: 当 `avg_sigma` 下降而 `kl_loss` 上升时，表明自适应正则化机制工作正常。反之，若 `avg_sigma` 停滞且 `kl_loss` 异常（过大、过小或 NaN），则表明 KL 损失的梯度流很可能已经中断。
