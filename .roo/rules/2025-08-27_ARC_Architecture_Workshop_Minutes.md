# Tiny-ONN ARC 架构研讨会纪要 (v2.0)

**日期**: 2025-08-28
**会议目标**: 基于前次会议的深度思辨，沉淀并固化项目最终的技术决策与未来研究方向。

---

## 1. 目前的最终决策

经过对模型架构、训练范式及其底层机制的反复推演与澄清，我们最终确立了以下兼具理论自洽性与工程可行性的核心技术路线。

### 模型架构

- **核心模块**: 模型的基础构件将采用 **NSA (Native Sparse Attention)** 和 **SDL DynMoE (Dynamic Mixture of Experts with Sparse-Diversity Loss)**。这在保证计算与内存效率的同时，为模型提供了动态的、内容感知的计算路由能力。

- **门控机制**: DynMoE 的门控网络采用一种确定性的、基于内容的路由机制：通过计算输入 token 与专家“原型”向量之间的**余弦相似度**，并与一个可学习的**激活阈值**进行比较，来决定是否激活专家。

- **系统稳态**: 模型的长期健康度通过两种机制保障：
  - **SDL (稀疏-多样性损失)**: 作为一种结构化正则项，鼓励专家功能分化。
  - **周期性专家重置**: 通过一个离散的、非梯度的周期性操作，识别并重新初始化“死亡”（长期未被激活）的专家，以此取代传统的负载均衡损失，更彻底地将系统维护与任务学习解耦。

- **输出层**: 维持标准的 Transformer 设计，即一个线性层 (`LM Head`) 输出 `logits`，随后通过 **`Softmax`** 函数将其转换为标准的 token 概率分布。

### 训练流程

- **核心哲学**: 创新集中于训练流程而非模型架构本身。我们采用一种**两阶段训练哲学**，以平衡计算效率与模型的泛化能力：
  - **阶段一：预训练 (Pre-training)**: 使用计算效率极高的 **Teacher Forcing** 范式。此阶段旨在让模型快速掌握基础的模式补全能力，并利用海量数据驱动 DynMoE 完成初步的专家功能分化。
  - **阶段二：对齐微调 (Alignment Fine-tuning)**: 在预训练模型的基础上，应用专门设计的 **EAVI (Excursion-Alignment Variational Inference)** 范式。此阶段旨在修正 Teacher Forcing 带来的“暴露偏差”，通过对模型独立生成的完整序列进行全局对齐，赋予模型真正的推理能力。

- **EAVI 高效实现**: 为解决 EAVI 训练速度慢的瓶颈，我们采用一种计算上高效的解耦方案：
  - **1. 无梯度生成 (`no_grad`)**: 在 `torch.no_grad()` 上下文中，以自回归方式快速生成完整的输出序列（其形式为概率分布序列），此过程不构建计算图，开销极低。
  - **2. 并行求导**: 将上一步生成的**整个序列**与地面真值序列进行一次并行的前向传播，计算出全局对齐损失，并高效地进行一次反向传播。

- **梯度流**: 训练过程直接在 `Softmax` 前的 `logits` 上进行，以执行变分推断。`Argmax` 采样仅用于最终对外提交结果，不参与梯度计算。

---

## 2. 未来考虑的研究方向

在确立了当前的核心技术路线后，我们同样展望了未来可能进一步提升模型能力和效率的两个关键研究方向。

### DynSMLoRA: 动态稀疏多低秩头注意力

- **动机**: 当前的 DynSMHA (动态稀疏多头注意力) 方案虽然表达力强，但在有限显存下对长上下文支持不佳。我们曾讨论过“基座 MHA + 动态低秩自适应”的设想。

- **概念**: DynSMLoRA (Dynamic Sparse Multi LoRA-head) 是该思想的进一步具体化。它将一个静态的、通用的基座注意力层（如 GQA）与一组由门控网络动态选择和混合的、轻量级的低秩（LoRA）“增量矩阵”相结合。

- **优势**: 此架构旨在“标准 MHA”的**效率**和“全秩 DynSMHA”的**表达力**之间取得平衡，通过将动态计算限制在低秩子空间内，有望在严格的硬件约束下实现更强大的、逐 token 的动态注意力能力。

### 自定义算子 (Custom Kernels)

- **动机**: 当前依赖 PyTorch 原生 `einsum` 的稀疏计算实现，在性能和显存效率上仍有瓶颈。此外，SML (Surprise Minimization Loss) 这种强大的元学习门控损失，因其二阶导数带来的巨大计算开销而被暂时放弃。

- **概念**: 探索编写自定义 CUDA 算子，以实现真正高效的 token-to-expert 稀疏计算，绕过 PyTorch Autograd 引擎的诸多限制。

- **优势**:
  - **性能**: 一个高度优化的稀疏计算核，有望在速度和显存占用上远超 `einsum` 的稠密模拟。
  - **SML 可行性**: 通过在算子内部手动实现 SML 的梯度计算，可能以远低于 `autograd.grad` 的开销获得二阶导数信息，从而使这种更强大的元学习门控损失在工程上重新变得可行。
