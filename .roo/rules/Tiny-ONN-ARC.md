# Tiny-ONN-ARC: 项目报告与实施计划 (v2.2)

## 摘要

`Tiny-ONN-ARC` 是一个为解决 `ARC-AGI-2` 抽象推理任务而设计的专用语言模型。作为 `Tiny-ONN` 项目的一个分支，它继承了**超稀疏混合专家 (Hyper-SMoE)** 的核心思想，并针对 ARC 任务的特性进行了深度特化。本项目并非传统的程序合成，而是通过构建一个通用的**可微计算基质 (Differentiable Computational Substrate)**，让解决问题的抽象程序在梯度下降中**自组织地涌现**。模型架构基于 **DynSIHA (动态稀疏无限头注意力)** 和 **MoIE (无限专家混合)**，训练范式依赖 **Teacher Forcing** 和 **多视角数据增强** 来驱动模型学习抽象规则。

---

## 1. 系统工作流程

整个系统的工作流程遵循一个清晰的、顺序化的数据处理与学习管道：

1. **M1: 数据表征 (Data Representation)**
    - **Tokenizer**: 使用一个极小的、针对 ARC 任务的专属词汇表，包含 0-9 的颜色 token 和必要的控制 token (`<|im_start|>`, `<|im_end|>`, `problem`, `solution` 等)。
    - **Serializer**: 将 2D 网格及其坐标信息线性化为 1D token 序列。这是模型能够理解空间关系的基础。

2. **M3: 数据加载与预处理 (Data Loading & Pre-processing)**
    - **ArcDataset**: 从 JSON 文件中读取并加载完整的任务数据，每个任务（包含其所有 `train` 和 `test` 对）作为一个样本。
    - **ArcCollator**: 作为数据加载器的核心，它接收一批原始任务数据，并负责执行"多示例上下文"拼接、D8 对称数据增强、序列化以及最终的批次填充，直接产出可供模型计算的规整张量。

3. **M2: 模型架构 (Model Architecture)**
    - **核心**: `ArcTransformer`，一个标准的 Decoder-only Transformer 骨架。
    - **创新**: 其核心计算单元是 `DynONNBlock`，它用 **DynSIHA (动态稀疏无限头注意力)** 和我们定制的 **MoIE (无限专家混合)** 分别取代了标准的注意力和前馈网络层。

4. **M6 & M4: 训练编排与范式 (Orchestration & Paradigms)**
    - **Trainer**: 作为顶层协调器，负责驱动整个训练与评估循环。
    - **StepRunners**: `Trainer` 根据配置，实例化具体的训练步骤执行器 (`TFStepRunner`)，将训练范式的具体逻辑解耦。
    - **训练循环**: `Trainer` 从 `DataLoader` 获取批次，交由 `StepRunner` 执行，然后负责梯度累积和优化器步骤。

5. **M5: 推理与评估 (Inference & Evaluation)**
    - **EvaluationStep**: 一个独立的评估逻辑单元，负责在 `torch.no_grad()` 上下文中执行完整的评估流程。
    - **Sequence Generator**: 利用 `model.generate` 进行自回归推理，生成解答序列。
    - **Grid Decoder**: `Serializer` 的逆过程，将 token 序列解码回 2D 网格。
    - **AugScore Evaluator**: 实现多视角一致性评估，为模型的最终解答提供鲁棒性评分。

6. **M6: 系统支持 (System Support)**
    - **Config**: 提供统一的、结构化的配置管理。
    - **Observer**: 负责所有训练过程中的日志记录与可视化。

---

## 2. 核心范式与理论基础

### 2.1. 方法论：涌现式可微程序搜索

我们将 ARC 视为一个"可微程序搜索"问题，但摒弃了传统的、基于预定义符号原语的显式搜索方法，因为该方法面临组合爆炸和泛化难题。

取而代之，我们采用一种"自下而上"的**涌现式 (Emergent)** 方法：

- **通用计算基质**: `ArcTransformer` (尤其是其 `MoIE` 层) 本身被视为一个极其灵活和通用的、完全可微的计算图。注意力机制是通用的信息路由，而 `MoIE` 则是通用的计算单元动态选择器。
- **隐式程序涌现**: 我们不预设任何高层级的操作（如 `Copy`, `Resize`）。相反，我们相信，通过端到端的梯度下降，模型能够在其权重空间中，自组织地学习到实现这些功能所必需的底层计算模式。`MoIE` 在训练的驱动下，会自发地专门化为处理某种特定模式（如颜色替换、对称性识别）的"微型程序"。

### 2.2. 核心训练范式: Teacher Forcing + 多视角增强

我们采用单一且专注的训练范式来解决 ARC 任务：

- **Teacher Forcing (TF)**:
  - **目标**: 快速学习 ARC 任务的"语法"和局部模式规则。
  - **方法**: 使用标准的自回归训练。模型被给予一个完整的"问题-答案"序列，并被要求在每个时间步预测下一个正确的 token。
  - **作用**: 此阶段计算效率高，能让模型快速掌握基础知识，并促使 MoE 专家进行功能分化。

- **多视角数据增强 (Multi-View Data Augmentation)**:
  - **目标**: 学习几何不变性，增强模型的泛化能力。
  - **方法**: 在训练过程中，对输入的 ARC 网格进行 D8 对称变换（旋转、翻转），让模型在多种视角下学习同一个任务的解决方案。
  - **作用**: 这是驱动模型从"记住模式"到"理解规则"的助推。

### 2.3. AugScore 多视角一致性评估

- **动机**: 一个真正理解了抽象规则的模型，其对一个问题的解答不应因观察视角（如旋转或翻转）的改变而改变。
- **方法**:
    1. **并行生成**: 将一个测试问题进行所有 8 种 D8 对称变换，形成 8 个不同的"视角"。让模型为每个视角生成一个或多个候选解答。
    2. **逆变换与聚合**: 将所有生成的解答通过逆变换统一到原始坐标系下，形成一个候选池。
    3. **交叉评分**: 对候选池中的**每一个**候选解，我们再将其变换到所有 8 个视角，并计算模型在每个视角下生成该解的**对数概率**。
    4. **最终选择**: 将一个候选解在 8 个视角下的对数概率求和，得到其最终的 `AugScore`。分数最高的候选解被认为是模型"共识度"最高的答案。

---

## 附录 B: v3.0 组件来源形式化分析与融合计划

本附录基于对 `exp/tiny_onn_arc` 和 `exp/bit_sbl_poc` 的形式化代码分析，为 v3.0 的实现提供了更精确的组件提取与适配计划。

### B.1 `tiny_onn_arc` (项目外骨骼)

此项目提供了构建 ARC 任务端到端解决方案所需的、经过验证的**训练与数据基础设施**。

- **计划提取的组件**:
    1. **训练与评估框架**: 提取其顶层的**训练协调器** (Trainer)、**评估器** (EvaluationStep) 和**观测器** (Observer) 的设计哲学，包括检查点管理、基于 `model.generate` 的评估流程以及高质量的可视化界面。
    2. **数据管道与增强**: 复用其完整的 ARC **数据处理流水线**，包括 `Dataset`、**网格序列化/反序列化**逻辑，以及至关重要的 **D8 对称数据增强** (`ConsistencyTools`)，后者是驱动模型学习泛化规则的基础。
    3. **模型骨架**: 不得沿用其基于 `Qwen3` 的 Transformer 骨架，模型层将完全被替换以避免 `Transformers` 适配的复杂性。

- **需要适配的部分**:
  - **日志系统 (`Observer`)**: 其现有的日志功能需要重构，以兼容并展示 `bit_sbl_poc` 中用于观测 SBL 内部状态的、信息更丰富的指标集。
  - **模型核心 (`model.py`)**: 必须移除其现有的、基于启发式损失的 `DynMoE` 实现，以全面移植 `BitNet-SparseBayesianLinear` 系列的 `DynSIHA` 和 `MoIE` 模块。

### B.2 `bit_sbl_poc` (模型核心与训练范式)

此项目是 `BitNet-SBL` 的功能验证，提供了 v3.0 的**核心计算架构**和**自组织学习范式**。

- **计划提取的组件**:
    1. **核心模型架构**: 提取 `BitSBL`、`DynamicInfiniteHeadAttention` (DynSIHA) 和 `DynamicInfiniteExpert` (MoIE) 的**架构设计**作为 v3.0 的计算核心。
    2. **核心训练范式**: 提取其完整的 **SML + KL 散度**双辅助损失的**训练哲学**。这包括“通过 instrumented forward pass 暴露内部状态以计算 Surprise”的设计模式。
    3. **关键观测指标**: 提取其日志中用于洞察 SBL 动态行为的**指标集**，如 `Avg σ/g` (不确定性/门控), `Act%` (激活率), `τ/p_std` (模型熵/先验宽度)。

- **需要适配的部分**:
  - **代码内聚性**: **[已完成]**模型定义已整合至统一的 `model.py` 模块中。
  - **位置编码**: 必须废弃其原有的绝对位置编码，以采用更先进的 RoPE 方案。
  - **上下文扩展**: 引入**滑动窗口注意力 (SWA)** 机制作为可配置选项。需要对 SWA (基于位置的粗粒度稀疏) 与 DynSIHA (基于内容的细粒度稀疏) 的兼容性进行消融实验，以规避“过度稀疏化”风险。
  - **关键发现与约束 (2025-09-10)**:
    - **`bfloat16` 精度陷阱**: 实验 (`exp/bit_sbl_poc`) 证明，`bfloat16` 的低精度与 SML 的二阶梯度和 KL 散度中的 `log` 函数可能不兼容，会导致梯度计算下溢为零，尤其是在非原生硬件支持的 GPU 上。
    - **`float32` 训练约束**: 因此，在获得具有原生 `bfloat16` 支持的硬件之前，所有涉及 SBL/SML 的实验 **应该** 在 `float32` 精度下进行以确保数值稳定性。

---

## 附录 C: v3.0 训练流程哲学与梯度设计

为确保梯度流的完整性和训练的稳定性，v3.0 的单步训练流程被设计为一个包含三个核心目标的多目标优化过程。此流程严格遵循以下顺序，以保证各部分梯度的正确计算与传播。

1. **数据准备与 Instrumented 前向传播**:
    - **流程**: 训练步骤始于获取一批经过 D8 多视角数据增强的 ARC 任务数据。
    - **核心设计**: 模型的前向传播过程需要被**“instrumented” (检测和揭示)**。除了计算用于最终预测的 `logits`，它还必须**暴露和传递**出模型内部所有 `BitSBL` 层在门控之后、用于计算的**内部激活值** (`masked_outputs`)。这一设计是实现 SML 元学习目标的**结构性先决条件**。

2. **主目标：预测准确性**:
    - **定义**: 基于模型的 `logits` 和真实标签，计算标准的**交叉熵损失** (`main_loss`)。
    - **核心原则**: 此损失是驱动模型学习解决 ARC 任务的**主要外部信号**。
    - **设计约束**: **严禁使用标签平滑**。ARC 任务的离散特性要求模型学习清晰、明确的决策边界。标签平滑会模糊这个信号，对需要精确梯度信息的 SML 造成“梯度污染”，与我们的自组织目标背道而驰。

3. **内部目标：计算自组织**:
    - **惊奇最小化损失 (SML - 元学习目标)**:
        - **哲学**: SML 的核心假设是，一个高效的计算系统应将信息路由到能以最低“系统扰动”处理它的功能单元上。
        - **实现原则**: 这种“扰动”或“惊奇度 (`Surprise`)”在计算上被定义为：**`main_loss` 相对于每个神经元内部激活值的梯度**。训练框架需要有能力计算这个高阶梯度。SML 损失函数随后被构建为**惩罚产生高 `Surprise` 的路由决策**，从而引导门控参数进行自组织的、面向效率的优化。
    - **KL 散度损失 (自适应正则化目标)**:
        - **哲学**: 该损失通过在贝叶斯框架下引入一个**动态先验**，实现对模型可塑性的自适应调节。
        - **实现原则**: 模型的**整体预测不确定性**（由 `logits` 的熵 `avg_tau` 来量化）被用作一个反馈信号，来动态调整模型对自身权重的先验信念 `p(W)` 的宽度。当模型**不确定**时（高 `avg_tau`），先验放宽，允许权重进行更大幅度的探索；当模型**确定**时（低 `avg_tau`），先验收紧，鼓励权重稳定下来并走向稀疏。`kl_loss` 则负责惩罚模型的权重分布 `q(W)` 与这个动态先验 `p(W)` 之间的偏差。

4. **梯度融合与参数更新**:
    - **组合总损失**: 最终的 `total_loss` 是上述三个目标的**动态加权和**。其中，SML 和 KL 损失的无需额外加权——直接相加即可。
    - **参数更新**: 在 `total_loss` 上计算全局梯度，并经过**梯度裁剪**以确保稳定性后，用于更新模型的全部可学习参数。这个统一的梯度流确保了模型的外部预测能力和内部自组织能力在同一个优化过程中协同演进。
