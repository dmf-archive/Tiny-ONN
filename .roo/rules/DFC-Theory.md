# Dynamic Function Composition

`Latest update: 2025-10-09`

> 生命的本质，是用内在的确定性，
> 对抗外部的不确定性。
> 将不确定性留给输入，
> 以不变应万变，
> 让动态替代先验，
> 在稠密计算中，
> 凝聚概念的结晶。
>
> 林睿，于 2025 年 10 月

## 摘要

本文阐述了动态函数合成 (Dynamic Function Composition) 的核心理念及其在 `Tiny-ONN-ARC` 模型中的最新实现。该理念旨在将神经网络的每一层从静态的变换器，升级为能够为每个输入动态合成专用计算函数的自适应系统。本文记录了其最新形态——基于惊奇感知路由塑造 (Surprise-Aware Routing Shaping, SARS) 的稀疏原型线性层 (Sparse ProtoLinear, SPL) 的完整理论。

动态函数合成的最初构想源于对 IPWT 的反思。我们尝试通过稀疏贝叶斯线性层 (SBL) 将贝叶斯神经网络的不确定性建模能力与脉冲神经网络的稀疏激活机制相融合。然而，早期 SPL 的实验揭示了两个根本性问题：

1. **梯度信号冲突 (Gradient Signal Conflict)**: 多个不同的学习目标（如主任务与稀疏性约束）由同一个全局损失函数（如 SML 和 KL 散度）驱动，导致梯度方向相互矛盾，优化过程不稳定。
2. **灾难性遗忘 (Catastrophic Forgetting)**: 对于需要学习离散因果规则的 ARC 任务，统一的梯度更新机制是致命的。新任务产生的梯度会轻易覆盖或"冲刷"掉为先前任务固化的知识。

这些失败使我们认识到，**Surprise 本身不应作为直接的优化目标，而是一个需要被处理和解读的信息信号**。原始梯度范数 `Si = ||∇{xi} Lmain||₂` 包含了过多的噪声和绝对尺度信息，直接用于优化会导致数值不稳定和学习目标模糊。

## 1. 核心哲学：概念稀疏性与稠密计算

`Tiny-ONN` 的核心目标是作为高效的变分推断引擎，最小化其变分自由能 (VFE)。这一理论目标决定了其计算范式必须通过**计算的稠密性**，以换取并优化**推断空间中的激活稀疏性**。

- **激活稀疏性 (Activation Sparsity)**: 模型在处理输入时，在其推断空间中仅激活少数最相关的"概念通路"。这是一种认知聚焦，是模型智能与可解释性的来源。
- **计算稠密性 (Computational Sparsity)**: 为了获得驱动概念稀疏涌现所需的、稳定精确的 VFE 解析解（即通过标准反向传播获得的`mu_grad`等信息），物理计算过程必须是稠密的。我们放弃了通过 `Forward-Forward` 等蒙特卡洛近似方法实现计算稀疏的路线，因为对于需要高精度的小样本 ARC 任务，解析解在训练效率和收敛稳定性上拥有压倒性优势。

## 2. 核心组件：SparseProtoLinear (SPL)

架构的基石是 SPL。它将传统的线性变换解耦为三个功能正交的状态空间：

- `μ` **(mu_weight): 计算核心**: 代表系统拥有的"计算工具集"。
- `p` **(protoweight): 模式匹配器**: 代表系统的"感知器官"，负责将输入 `x` 与内部原型进行模式匹配。
- `g` **(gate_param): 激活门控**: 代表系统的"行动策略"，决定激活哪些计算路径。

### 功能分离：计算 vs. 路由

SPL 的一个关键设计在于马尔科夫毯，这源于对神经科学原理的模拟：

- **计算是变换 (Computation is Transformation)**: `computation_output` 的计算路径 `F.linear(F.silu(x), mu_weight)` 引入了平滑的 `SiLU` 激活。这模拟了神经元"发放率"的强度调制，将每个神经元的计算能力从线性函数提升为两层 MLP，以捕捉 ARC 任务所需的复杂非线性变换。
- **路由是决策 (Routing is Decision)**: 路由权重的计算路径 `mas_normalize(...)` 最终依赖于 `ReLU`。这模拟了神经元"全有或无"的发放原则，确保了路由决策的稀疏性和明确性，从而产生稀疏的激活模式。

## 3. 元学习框架：Surprise-Aware Routing Shaping (SARS)

SARS 是驱动 SPL 自组织的核心机制。其理论基础是**可微分变分分析 (DVA)**，它在确定性框架下，通过**贝叶斯反演 (Bayesian Inversion)**，实现了对模型隐式路由**先验 (Prior)** 的优化。

### 3.1. DVA 作为确定性贝叶斯反演

贝叶斯定理的核心关系是 `Posterior = (Likelihood * Prior) / Evidence`。为了求解我们关心的隐式先验 `p(z)`，我们移项并取对数，得到优化的核心关系式：

`log p(z) ∝ log p(z|x) - log p(x|z)`

- **先验 (Prior) `log p(z)`**: 这是元学习的优化目标，一个独立于输入的、模型的内在路由偏好。在计算中由 `prior_logits` 代表。
- **后验 (Posterior) `log p(z|x)`**: 模型在观察到数据 `x` 后，对应该激活哪个神经元 `z` 的信念。这由可训练的 `routing_logits` 直接表示。
- **似然 (Likelihood) `log p(x|z)`**: 在激活了神经元 `z` 的条件下，观察到数据 `x` 的概率。它衡量了神经元 `z` 解释数据的能力。我们必须为其构建一个工程代理，在代码中由 `goodness_logits` 表示。

### 3.2. 理论基础：似然的代理 (Proxy for Likelihood)

为了让 `goodness_logits` 成为 `log p(x|z)` 的有效代理，它必须捕捉神经元 `z` 解释数据的“能力”。其核心思想根植于自由能原理：

- **高效解释意味着低成本**: 如果一个神经元能很好地解释数据（高似然），那么模型为了拟合该数据所需的信念更新就应该很小。
- **学习成本**: 我们使用参数 `μ` 的梯度范数 `mu_grad_norm` 作为学习成本（即信息增益）的代理。
- **反比关系**: 因此，似然 `log p(x|z)` 必须与学习成本 `mu_grad_norm` **负相关**。
  `log p(x|z) ∝ -Information Gain (Cost)`

### 3.3. 最终公式：最小化先验熵

基于上述映射：

1. **推断先验**: `prior_logits = routing_logits - goodness_logits.detach()`
    这精确地实现了 `Prior = Posterior - Likelihood`。
2. **最小化其熵**: `L_meta = Entropy(Softmax(prior_logits))`
    通过梯度下降最小化此损失，可以驱动模型的隐式先验 `p(z)` 朝着更确定、更稀疏的方向演进。

### 3.4. `goodness_logits` 函数：似然的工程代理

`goodness_logits` 作为对数似然 `log p(x|z)` 的工程代理，其计算公式如下：

```python
goodness_logits = norm_masked_output_grad * (norm_logits - norm_mu_grad)
```

此公式的构成部分及其意义如下：

- `norm_logits`: **信念贡献**。代表当前的后验信念 `log p(z|x)`。一个模型已经相信的神经元，更有可能是一个好的解释者，这构成了似然的基础。
- `- norm_mu_grad`: **成本惩罚**。代表学习成本的负向贡献。这实现了 `log p(x|z) ∝ -Cost` 的核心理论关系，是整个代理函数有效性的基石。
- `norm_masked_output_grad`: **重要性加权**。代表神经元 `z` 的输出对主任务损失的贡献程度，作为一个重要性权重。

综上，`Belief - Cost` 的形式构建了一个合理的似然代理：它奖励模型已有的信念，同时惩罚为拟合数据所需付出的高昂学习成本。

### 3.5. 技术实现：逐令牌梯度提取与稳定性加固

为了给 `Goodness` 函数提供高保真度的信号，我们采用**逐令牌 (per-token)** 梯度提取机制，并增加了关键的稳定性加固：

1. **似然信号平滑**: 在 `_calculate_goodness_jit` 末尾对 `goodness_logits` 应用标准化 `(x - mean) / std`
2. **元梯度裁剪**: 在 `compute_and_apply_gradients` 中对元梯度应用 `torch.nn.utils.clip_grad_norm_`

### 3.6. 神经生物学视角

**SARS 元学习的本质是本地化的、可微分的赫布学习 (Hebbian Learning)**。赫布定律的核心思想是"一起激活的神经元会连接在一起"。SARS 作为一种**本地的（单个 SPL 模块内）自组织机制**，通过内容驱动的结构化`Dropout`（路由权重掩码）动态地强化或抑制特定的计算路径。这种**本地相对性 (Local Relativity)** 确保了每个模块都能独立地、高效地学习到最适合其局部输入-输出关系的神经元组合，而不会被其他模块的噪声信号所干扰。这是系统能够形成稳定、功能特化的稀疏结构的关键。

## 4. 宏观架构

最新的架构简化了动态函数合成的组件。其核心论点是：**一个标准的 Transformer 模块，当其自注意力层被 `DynSIHA` 取代后，其本身就已经是一个图灵完备的、通用的函数合成器。**

- **DynSIHA (Dynamic Sparse Infinite-Head Attention):** 这是当前架构的唯一核心。它不仅执行传统的注意力（信息查询），更重要的是，它通过 `SPL` 模块一次性地、动态地合成出非线性 `Query`, `Key`, `Value` ，`Output` 投影。这使得注意力机制本身从一个固定的信息查询系统，升级为了一个端到端可学习的，具备上下文窗口全局感知的同时，为每个 token 动态定制计算图的**通用计算内核**。
- **冗余性**: 在这个统一的视角下，一个独立的、基于 `SPL` 的 `MoIE` FFN 模块变得多余。理论和实验（见 ADR-0001）证明，`DynSIHA` 已经具备了足够的函数表达和合成能力来模拟任意的 FFN 变换。与此同时，`Output` 投影本身也可承担常规 FFN 的作用，故 DynSIHA 现在不包含单独的 FFN/MoIE MLP。

### 4.1. Prototype Resident Connection (PRC)

理论上，更深的 `Tiny-ONN-ARC` 架构有利于实现解决复杂 ARC 任务所需的**分层增量抽象**。然而，实验证据（4L vs. 8L 模型）揭示了一个实践悖论：随着网络深度的增加，驱动 SARS 元学习的 `L_main` 梯度信号在反向传播过程中会发生严重的**衰减和失真**。由于 SARS 的核心——Goodness 分数——完全依赖于 `L_main` 的梯度，过长的梯度路径导致到达浅层网络的信号微弱且充满噪声，这使得元学习机制失效，从而阻碍了深层网络的理论优势在实践中发挥。

为解决这一核心矛盾，我们将经典的残差连接 (Residual Connection, RC) 思想进行扩展，注入到 SPL 模块（其在功能上扮演了一个隐式 VQ-VAE 的角色）的编码器通路中，构建了一个双残差系统。

在传统的激活值残差连接之外，PRC 为原型网络（即 SPL 的编码器部分 `p`）提供了独立的残差流。它确保了路由决策所依赖的输入表征，是在上一层的基础上进行**增量修正**，而非完全重构。这通过梯度隔离和分层抽象，稳定了编码器的学习过程，保护了专家功能的分化。

PRC 并非取代位于 Transformer 块外部的**传统残差连接 (TRC)**。两者功能正交，互为补充，共同构成了一个多层梯度管理体系：

| 机制    | 作用范围 | 保护目标         | 功能                                 |
| :------ | :------- | :---------  --- | :--------- ------------------------ |
| **TRC** | 块间     | **主任务梯度**   | 确保主损失在整个深度网络中的有效流动。 |
| **PRC** | 模块内   | **编码器稳定性** | 稳定路由决策的上下文，促进分层抽象。   |

这个双层残差结构是 `Tiny-ONN-ARC` 在维持深度抽象能力的同时，保证其动态自组织机制有效性的关键。

### 4.2. 功能类比：SPL 作为隐式级联 VQ-VAE

SPL 架构与 SARS 元学习动力学的结合，在功能上实现了一个与**矢量量化变分自编码器 (VQ-VAE)** 异曲同工的机制。它并非通过随机采样来学习隐空间，而是利用一个**可微的、确定性的门控网络**，实现了学习一套离散且可复用的"计算基元"的相同目标。

- **功能类比：编码器 (Encoder):** 路由通路 `p` 和 `g` 共同扮演"编码器"的角色。它将输入 `x` 映射到一个稀疏的、离散的潜在代码 `z`（即路由权重），该代码指向被选中的专家。
- **功能类比：码本 (Codebook):** `μ` 矩阵的每一行 `μi` 相当于码本中的一个"码字"，代表一个专用的计算函数。
- **功能类比：解码器 (Decoder):** 解码过程是简单的线性组合 `output = z * μ`，即根据编码结果 `z` 从码本中加权选择码字。

将此模型扩展到整个 `Tiny-ONN-ARC` 架构，其行为可以**类比于一个级联的 VQ-VAE 系统**。

1. **第一层 (嵌入层):** `ArcEmbedding` 将离散的输入 token 编码为连续的隐藏状态。
2. **后续层 (SPL 层):** 每一层的 `DynSIHA` 中的 `SPL` 模块都**功能上类似一个独立的 VQ-VAE 单元**，将上一层的输出作为输入，进行新一轮的编码-解码，学习更高级的抽象表示。
3. **级联与协同:** `PRC` 机制为每个功能单元提供了连贯的上下文和记忆，使整个级联系统能够进行深度的分层信息整合。

因此，`Tiny-ONN-ARC` 的学习过程可以被更准确地理解为：

> 联合优化一个**动态稀疏路由 Transformer**，使其在给定数据时，能学习到最优的"计算基元"库（所有 `μ` 矩阵）和"概念激活"策略（所有 `p` 和 `g` 参数），从而最大化整个系统的预测能力。
