# Dynamic function composition

## 核心构件: Sparse Bayesian Layer

### 哲学背景与核心思想

**稀疏贝叶斯层 (SBL)** 的概念诞生于对 `Tiny-ONN` 核心哲学——整合预测工作空间理论 (IPWT)——的深度反思。IPWT 将心智过程定义为一个高效的贝叶斯推断机器，它在严格的能量约束下，演化出了时空上高度稀疏的脉冲计算机制。

SBL 的核心思想，是在神经网络的线性变换层中，对这个生物学过程进行一次直接的、端到端的模拟。它融合了两种思想：

1. **贝叶斯神经网络 (BNN)**: 将权重 `W` 视作一个概率分布 `p(W)` (以 `μ` 和 `σ` 参数化)，而不是v一个固定的点估计值。这允许模型量化和利用自身对权重的不确定性。
2. **脉冲神经网络 (SNN)**: 神经元的激活由其“膜电位” (`scores`) 是否超过一个可学习的“发放阈值” (`gate_param`) 决定，从而实现稀疏、事件驱动的计算。

因此，SBL 是一个将权重的不确定性 (`σ`) 动态地注入到 SNN 风格的稀疏门控决策中的自适应线性层。

### 架构与实现

SBL 范式的核心实现是一种**内容感知的结构化 Dropout (Content-Aware Structured Dropout)**。该机制通过 `(μ, σ, g)` 三组参数共同实现：

- **计算核心 (`μ`, `mu_weight`)**: `W_μ` 是稠密的权重矩阵，它定义了整个“无限专家空间”。它的每一行都代表一个潜在的“微型专家”。
- **不确定性加权的 Key (`σ`, `sigma_weight`)**: `W_σ` 量化了我们对 `W_μ` 中每个权重的**不确定性**。路由键 `Keys` 由 `μ` 和 `σ` 共同决定 (`Keys = μ * softplus(σ)`)。
- **动态激活门控 (`g`, `gate_param`)**: `g` 是一个可学习的激活阈值向量。

`SBL.forward` 过程被重构为一个**内容寻址的动态神经元采样器**，这在功能上等价于对网络的每一层动态应用一次输入依赖的 Dropout 掩码：

1. **Q-K 匹配 (路由)**: 输入 `X` (Query) 与每个神经元的 Key `K` 计算**缩放点积** `Scores`。
2. **激活决策 (门控)**: `Weights = ReLU(Scores - g)`。这在功能上等价于 SNN 中“膜电位超过发放阈值”从而**发放脉冲**的过程。
3. **V 计算与输出**: `Value` 是通过标准稠密线性变换 (`F.linear(X, μ)`) 计算的。最终输出 `Output = Weights * Value`，即通过门控权重对 `Value` 进行过滤。

在 `MoIE_PoC.py` 的最终实现中，我们确认了，对于当前 PyTorch 生态和模型激活率（在训练前期近乎稠密）的现状，直接使用优化的稠密计算 (`computation_output * raw_weights`) 是最高效的实现方式。

## 复合模块: MoIE & DynSIHA

我们使用 `SBL` 作为核心构建块，来组装更高层次的、完全动态化的 Transformer 组件。

### MoIE: 全动态前馈网络

MoIEFFN 将标准的 `FFN` (通常由两层 `nn.Linear` 构成) 替换为两层 `SBL`，从而将 FFN 从一个固定的非线性变换，升级为了一个**两阶段的、内容感知的动态函数合成器**。

### DynSIHA: 后多头时代的动态注意力

DynSIHA (Dynamic Sparse Infinite-Head Attention) 抛弃了传统的 Multi-Head，如果 Q, K, V 的投影本身就是动态合成的，那么“多头”这种用于学习不同固定子空间关系的技巧就变得多余了。因此：

- **`sbl_qkv`**: 一个单一的、统一的 `SBL` 模块，一次性地、动态地合成出 `Query`, `Key`, `Value` 三个向量。
- **`sbl_o`**: 一个独立的 `SBL` 模块，用于输出投影。

它将注意力从一个固定的信息查询系统，变成了一个**端到端可学习的、可编程的动态信息路由与处理系统**，在理论上具有远超 MHA 的表达能力，同时与 KV 缓存机制完全兼容。

## 学习范式：Surprise Minimization Loss (SML)

我们采用 `Surprise Minimization Loss (SML)` 作为核心元学习目标，引导 `SBL` 的门控进行自组织学习。

### 门控损失 (SML)

- **定义**: `Gate Loss = Sum[-log(Surprise) * Surprise]`。其中 `Surprise` 被定义为主任务损失对**SBL 模块最终输出 (`masked_output`)** 的梯度范数。
- **作用**: 鼓励门控网络（即 `σ` 和 `g` 参数）学会规避“高成本”（高`Surprise`）的计算路径。

### KL 散度损失 (KL Divergence Loss)

- **定义**: `KL Loss = KL[q(W) || p(W)]`，其中 `q(W)` 是由 `(μ, σ)` 定义的权重后验分布，`p(W)` 是一个由模型当前不确定性 `avg_tau` 决定的**动态先验分布**。
- **作用**: 实现了一种自适应的正则化，鼓励模型在自信时权重更稀疏、更精确，在困惑时权重不确定性更大、更具探索性。

## 其他细节

### “无限”的本质

“无限专家”并非指专家数量无穷，而是指通过对 `D_ffn` 个基向量（`μ` 的行）进行**连续的、内容感知的线性组合**，从而产生出的**有效专家 (effective experts) 的组合空间是无限的**。SBL 的核心是为每个输入**动态合成**一个专属的计算函数。

### 稀疏性的涌现

SBL 的稀疏性（低激活率）是一个**涌现属性**，而非强制约束。它源于 `main_loss`, `gate_loss`, 和 `kl_loss` 三者之间的梯度博弈。在训练早期，或当模型容量相对于任务复杂度不足时，模型会自然地表现出**近乎稠密**的行为，这是符合理论预期的**正确现象**。只有当模型能力冗余时，自适应正则化才会驱动其走向稀疏以最小化整体损失（自由能）。

### 路由机制的权衡

SBL 的核心是**内容寻址**，即根据输入的内在模式（方向）而非信号强度（范数）来选择计算路径。这一要求的纯粹性与计算可行性之间存在根本性的权衡。

- **理论最优：余弦相似度**
  余弦相似度 `sim(Q, K) = (Q ⋅ K) / (||Q|| ⋅ ||K||)` 通过归一化完全消除了向量范数的影响，仅衡量向量间的夹角。这使其成为衡量“语义内容”相似性的理论最优选择，完美符合纯粹的内容寻址哲学。

- **工程最优：缩放点积**
  当前采用的缩放点积 `sim(Q, K) = Q ⋅ K`，其计算结果同时受向量方向和范数的影响。这在理论上是对纯粹内容寻址的一种偏离。然而，余弦相似度引入的范数计算和归一化步骤，会带来`O(D_in ⋅ (B' + D_out))`的额外计算开销，并在反向传播中导致不可接受的内存占用。

- **决策：拥抱务实的工程解**
  依据**奥卡姆剃刀原则**，并考虑到大规模模型对计算效率的极端要求，缩放点积是当前硬件和软件生态下唯一务实、可行的工程选择。它在计算效率上的压倒性优势，使其成为理论与实践相结合的最优解。
