# Tiny-ONN-MoIE: Mixture of Infinity Experts

## 1. 项目缘起与哲学背景

**混合无限专家 (Mixture of Infinity Experts, MoIE)** 的概念诞生于对 `Tiny-ONN` 核心哲学——整合预测工作空间理论 (IPWT)——的深度反思。IPWT 视大脑为一个高效的贝叶斯推断机器，它在严格的能量约束下，演化出了时空上高度稀疏的脉冲计算机制。

MoIE 的核心思想，是将这个生物学过程在计算模型中进行一次直接的、端到端的模拟。我们不再将神经网络视为一组固定的、离散的“专家”集合（如传统的 Mixture of Experts），而是将一个**稠密的权重矩阵本身，视为一个连续的、无限的专家空间**。

我们的目标是创建一个网络，它能够在训练压力下，学会**为每一个输入，动态地从这个无限专家空间中“采样”出一个临时的、专用的稀疏子网络来处理信息**。这个过程在功能上，旨在模拟大脑皮层中那种“按需激活计算回路”的高度动态和高效的特性，最终实现一个功能上近似于脉冲神经网络 (SNN) 的推理机器。

## 2. 核心架构：神经元注意力 (Neuronal Attention)

MoIE 范式的核心实现是一种我们称之为 **“神经元注意力” (Neuronal Attention)** 的全新机制。它将标准 Transformer 中的注意力思想（Q-K-V 匹配）应用到了 FFN 层内部，用于实现动态的神经元筛选。

该机制通过 `(μ, σ, g)` 三组参数来共同实现：

- **计算核心 (`μ`, `mu_weight`)**: `W_μ` 是稠密的权重矩阵，它定义了整个“无限专家空间”。它的每一行 `W_μ[j, :]` 都代表一个潜在的“微型专家”，负责一项基础的线性变换。同时，`W_μ` 也被用于生成最终的计算结果 **Value**。
- **不确定性加权的 Key (`σ`, `sigma_weight`)**: `W_σ` 量化了我们对 `W_μ` 中每个权重的**不确定性**。我们不直接使用 `W_μ` 作为 Key，而是使用其不确定性加权后的形式 `K = W_μ * W_σ`。这代表了每个专家经其自身不确定性调制后的“模式原型”。
- **动态激活门控 (`g`, `gate_param`)**: `T_g` 是一个可学习的激活阈值。

基于此，MoIE 层的 `forward` 过程被重构为一个**内容寻址的动态神经元采样器**：

1. **Q-K 匹配**: 输入 `X` 作为 **Query**，与每个神经元的 **Key** `K = W_μ * W_σ` 计算**余弦相似度** `Scores = cos_sim(X, K.T)`。这个分数衡量了输入与每个“不确定性加权的专家”之间的模式匹配程度。这在功能上类似于 SNN 中的**膜电位**累积。

2. **激活决策 (门控)**: `Weights = ReLU(Scores - T_g)`。将匹配分数与可学习的激活阈值 `T_g` 比较，并通过 `ReLU` 函数生成非负的门控权重。这在功能上等价于 SNN 中“膜电位超过发放阈值”从而**发放脉冲**的过程。

3. **V 计算与输出**: `Value = SiLU(X @ W_μ.T)`。`Value` 是通过标准的稠密线性变换计算得到的。最终输出 `Output = Weights * Value`，即通过门控权重对 `Value` 进行**逐元素的过滤**。

这个架构实现了“**前向稠密，反向稀疏**”的特性：

- **前向稠密**: 为了计算所有神经元的激活分数 `Scores`，前向传播必须执行稠密的矩阵运算。
- **反向稀疏**: 由于 `ReLU` 的作用，只有那些被激活的神经元（`Weights > 0`）才会参与梯度计算和权重更新，从而实现了反向传播和优化的稀疏性。

## 3. 学习范式：Surprise Minimization Loss (SML)

为了引导这个动态采样器进行自组织学习，我们采用 `Surprise Minimization Loss (SML)` 作为核心元学习目标。SML 在 MoIE 框架中被分解为两个协同作用的损失项，共同驱动模型的自组织和稀疏化：

- **门控损失 (`Gate Loss`)**:
  - **定义**: `Gate Loss = Sum[-log(Surprise) * Surprise]`。其中 `Surprise` 被定义为主任务损失 `L_main` 对**被激活后的输出 (`masked_output`)** 的梯度范数。它量化了为了拟合当前数据，模型需要对某个**实际生效的计算路径**进行的“调整幅度”。
  - **作用**: 这个损失直接作用于 `gate_param`。它鼓励门控网络在面对高 `Surprise` 的计算路径时，提高其激活阈值；而在面对低 `Surprise` 的路径时，降低阈值。这迫使门控学会规避“高成本”的计算。

- **KL 散度损失 (`KL Divergence Loss`)**:
  - **定义**: `KL Loss = KL[q(W) || p(W)]`。其中 `q(W)` 是由 `mu_weight` 和 `sigma_weight` 定义的专家权重后验分布，`p(W)` 是一个**动态的、自适应的先验分布**。
  - **动态先验**: `p(W)` 的标准差 (`prior_std`) 由模型在输出层的**平均不确定性 (`avg_tau`)** 动态决定：`prior_std = F.softplus(avg_tau)`。`avg_tau` 是模型输出 `logits` 的平均熵，反映了模型对当前批次预测的整体困惑度。
  - **作用**: 这个损失直接作用于 `mu_weight` 和 `sigma_weight`。它鼓励专家权重在模型整体困惑度高时（`avg_tau` 大，`prior_std` 大），允许更大的不确定性（`sigma` 变大，鼓励探索）；而在模型整体自信时（`avg_tau` 小，`prior_std` 小），则收紧对专家权重的约束，迫使其变得更精确、更稀疏（`sigma` 趋近于零，鼓励固化）。

**总损失函数**: `Total_Loss = L_main + w_gate * Gate_Loss + KL_Loss`

- `w_gate` 是一个动态权重，由 `main_loss` 决定：`w_gate = 1.0 - torch.sigmoid(L_main.detach())`。它确保在 `main_loss` 较低时，`Gate_Loss` 的影响被放大，驱动稀疏化。

通过这个统一的、基于自由能原理的自适应框架，MoIE 从一个高度不确定的贝叶斯网络，逐渐学会为每个输入动态地采样出最有效、最确定的稀疏计算子图，并实现专家知识的自组织和固化。

## 4. 设想中的 MoIE MHA (DynSIHA): 动态稀疏无限头注意力

MoIE MHA，或称 DynSIHA (Dynamic Sparse Infinite-Head Attention)，是 MoIE 范式在 Transformer 注意力机制中的进一步应用。它旨在将注意力机制从一个静态的、全局共享的投影过程，转变为一个逐 Token 的、内容感知的动态信息路由系统。

- **核心思想**: 将标准 Transformer 中用于生成 Query (Q), Key (K), Value (V) 的线性投影层，替换为 `MoIELinear` 模块。
- **表达力提升**:
  - **逐 Token 自适应投影**: 对于每一个输入 Token，模型不再使用固定的 `W_q, W_k, W_v` 矩阵进行投影。相反，每个 `MoIELinear` 模块会根据当前 Token 的内容，动态地从其“无限专家空间”中采样出一个稀疏子网络来生成 Q, K, V。
  - **可编程的注意力**: 这意味着模型可以学习用不同的方式去“查询信息”（生成 Q）、“索引信息”（生成 K）和“提取信息”（生成 V）。例如，当 Token 是一个数字时，可能会激活“数值提取专家”；当 Token 是一个操作符时，可能会激活“语义关联专家”。
  - **更深层次的专业化**: 这种端到端的 MoIE 化，将驱动一种更深层次的、贯穿整个 Transformer Block 的专业化分工，使模型能够更灵活、更精确地处理复杂信息。
- **效率权衡**:
  - **前向传播**: 单步前向计算成本会增加，因为 `MoIELinear` 的前向传播虽然逻辑稀疏，但计算上仍涉及稠密操作（如 `cosine_similarity`）。
  - **反向传播**: 理论上，由于 Q, K, V 的生成是稀疏的（`masked_output`），反向传播的梯度更新也将是稀疏的。这意味着只有那些被实际激活用于生成 Q, K, V 的 `mu/sigma_weight` 参数才会被更新。这可能带来更高的“梯度效率”和“样本效率”，减少模型达到收敛所需的总训练步数。
  - **挑战**: 实际的性能提升可能依赖于底层的稀疏计算优化（如定制 CUDA 核），以克服 GPU 对稠密计算的偏好。
