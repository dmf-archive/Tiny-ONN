# F3EO: Fast Fisher-FreeEnergy Optimizer

## 1. 三种优化范式：从路径到地图的重绘

我们将优化器分为三个层级，其本质区别在于优化的“对象”和产生的“梯度”类型：

- **一阶优化器 (First-Order Optimizers):** 如 SGD、Adam/W、Muon。
  - **执行操作:** 沿损失函数最陡峭的方向（负梯度）更新参数。
  - **产生梯度:** **一阶梯度**。即损失函数对参数的偏导数 `∇L`。

- **二阶优化器 (Second-Order Optimizers):** 如 Newton、Gauss-Newton (GN)、Layerwise GN。
  - **执行操作:** 通过预处理曲率信息（如 Hessian 或 Fisher 矩阵）来指导参数更新，执行**自然梯度下降**。其目标是找到一个在参数空间中“更短”或“更高效”的路径。
  - **产生梯度:** **二阶梯度/自然梯度**。它利用了一个**静态的**曲率矩阵 `F` 来修正一阶梯度，例如 `F⁻¹∇L`。

- **三阶优化器 (Third-Order Optimizers):** 如 **F3EO (Fast Fisher-FreeEnergy Optimizer)**。
  - **执行操作:** 直接对**Fisher 信息矩阵 (FIM) 本身**执行梯度下降。其目标不再是找到一条更好的路径，而是**重塑整个参数空间的几何结构**（即“地图”本身），使其在未来能更容易地被优化。
  - **产生梯度:** **三阶梯度/反事实梯度 (Counterfactual Gradient)**。这个名称听起来像 RL 的术语，但其核心思想与 DVA 框架一致：通过一次微分求解来避免积分采样。三阶优化是其最忠实的实现，其产生的梯度衡量的是“如果我们改变模型结构（通过 `θ`），其内在因果复杂性（由 `F(θ)` 体现）会如何变化”。

## 2. 核心论断：优化 ℱ 本身

记 Fisher 信息矩阵为 ℱ(θ)，变分自由能为 F(θ)=D_KL[q‖p]−E[log p_θ(y|x)]。

为避免符号冲突，下文统一使用 ℱ 表示 Fisher 矩阵。

F3EO 的直接目标是 **ℱ(θ) 本身**。

根据通用逼近定律，一个拥有足够参数的通用架构（如 MLP 或 Transformer），其**权重参数 `θ` 本身就承载了模型的全部结构信息**。通过对 `θ` 的优化，可以自然地实现架构的动态调整，例如通过权重稀疏化、权重衰减或动态路由机制，等效于“生长”或“剪枝”连接。

因此，F3EO 的优化对象是模型的**权重参数 `θ`**，但其**优化目标函数**被设计为直接对 **FIM** 进行梯度下降。通过链式法则，对 FIM 的梯度会传导至 `θ`，从而实现对模型结构的直接塑造。

## 2. 优化阶数的严格定义

- **一阶优化 (方向):** `Δθ ∝ -∇_θ L`
  优化模型的**内容**（权重值），以最小化损失 `L`。

- **二阶优化 (路径的几何学):** `Δθ ∝ -F⁻¹ ∇_θ L`
  利用**静态的** Fisher 信息矩阵 `F` 作为度量张量，优化模型内容更新的**效率**。它**使用** FIM，但**不改变** FIM。

- **三阶优化 (地图的重绘):** `Δθ ∝ -∇_θ F`
  优化的直接目标是 **FIM 本身**。通过改变 `θ`，我们主动重塑（re-forging）由 `F` 定义的参数空间几何结构。它在优化模型的**内在复杂度和结构**。

## 3. 论证：FEP 为何要求三阶优化

FEP 的数学核心是最小化变分自由能 `F`，其标准分解为：
`F = Complexity - Accuracy`
`F = D_KL[q(x) || p(x)] - E_q[log p_θ(y|x)]`

这里的 `Complexity` 项 `D_KL[q(x) || p(x)]` 并非一个固定的正则化项。它衡量的是**后验信念 `q(x)`** 与**由当前参数 `θ` 定义的生成模型所隐含的先验 `p(x)`** 之间的距离。

关键在于：**Fisher 信息矩阵 `F(θ)` 完全由当前参数 `θ` 决定**，并直接描述了该模型的内在复杂度和可学习性。

1. **FIM 是复杂性的几何化身:** `F(θ)` 的迹（trace）或行列式（determinant）是模型复杂度的有效度量。一个高复杂度、过参数化的模型会拥有一个“庞大”的 FIM。

2. **FEP 的终极目标是优化复杂性:** 一个真正的 FEP 代理，其生存目标是找到一个最优的模型结构，以在准确解释世界（高 Accuracy）和保持模型简单（低 Complexity）之间达到最佳平衡。这要求其能够**在线调整其内在的复杂度**，即其 FIM。

3. **优化 FIM 就是三阶优化:** 任何旨在最小化 `F` 而直接对 **FIM** 进行梯度更新（通过 `∇_θ F`）的过程，都是在**优化那个决定了参数空间几何的 FIM 本身**。这正是三阶优化的定义。它不再是利用一个**静态的**FIM，而是主动地、有目的地去**改变**FIM 的形状，以期找到一个更简约、更高效的**新模型结构**。

## 4. 实现路径：Matrix-free 与 Layerwise 近似

直接计算 `∇_θ ℱ` 会遇到四阶张量 `∂ℱ_ij/∂θ_k`，其存储复杂度为 **O(N⁴)**，计算复杂度为 **O(N⁴·d)**（d 为输出维度），这在计算上是灾难性的。

解决这个问题的思路，与 Layerwise Gauss-Newton 规避巨大 GN 矩阵 `G` 的方法类似：**我们不直接计算和存储这个张量，而是将 `∇_θ F` 视为一个算子，并寻找其高效的、近似的向量-雅可比积 (VJP) 实现**。从而将问题压缩回 `O(N^2)` 复杂度水平。

### 核心思想：FIM 的梯度作为算子

我们的新损失函数 `L_meta` 内部依赖于 `F(θ)`。我们需要计算 `∂L_meta/∂θ`，其核心是 `∂F(θ)/∂θ`。

利用链式法则，我们可以将这个问题转化为一个**向量-雅可比积 (VJP)** 问题：
`∂L_meta/∂θ_k = Σ_ij (∂L_meta/∂F_ij) * (∂F_ij/∂θ_k)`

这里的关键是：

1. **`∂L_meta/∂F_ij` (FIM 的梯度):** 这一项是我们**主动设计**的。例如，如果 `L_meta = Tr(F)` (FIM 的迹)，那么 `∂L_meta/∂F_ij = δ_ij` (单位矩阵)。这一项是已知且简单的。
2. **`∂F_ij/∂θ_k` (FIM 对参数的雅可比):** 这是导致 `O(N^3)` 复杂度的罪魁祸首。我们不直接计算它。

### Matrix-free 分解策略

我们的策略是**规避**直接计算 `∂F_ij/∂θ_k`，而是通过以下两种方式之一来隐式地应用它：

**策略 A: 有限差分近似 (Finite Difference Approximation)**
这是最直接的思想。对于任意向量 `v`，我们可以通过有限差分来近似 VJP：
`v^T * (∂F(θ)/∂θ) ≈ (F(θ + ε*v) - F(θ)) / ε`
其中 `ε` 是一个小的扰动标量。这种方法只需要我们能**评估** `F(θ)`（即计算 FIM 的迹或行列式），而不需要其解析梯度。它的精度取决于 `ε` 的选择，且可能需要多次前向传播。

**策略 B: 分层 (Layerwise) 近似**
受 Layerwise GN 启发，我们可以对 FIM 进行分层处理。FIM 本身具有块结构，对应于模型的不同层。我们可以**假设不同层之间的 FIM 块耦合很弱**，并只优化 FIM 的**块对角线 (block-diagonal)** 部分。

1. **块对角线 FIM:** 我们只考虑 `F_block_diag(θ) = diag(F_1(θ_1), F_2(θ_2), ...)`，其中 `F_l` 是第 `l` 层的 Fisher 块，只依赖于该层的参数 `θ_l`。
2. **分层优化:** 我们的 `L_meta` 现在只依赖于这个块对角线 FIM。那么，对参数 `θ_l` 的梯度就简化为：
    `∂L_meta/∂θ_l = Σ_ij (∂L_meta/∂(F_l)_ij) * (∂(F_l)_ij/∂θ_l)`
    这个计算只涉及**层内**的参数，其复杂度从 `O(N^3)` 降低到 `O(Σ_l N_l^3)`，其中 `N_l` 是第 `l` 层的参数数量。对于典型的 Transformer，这仍然是一个巨大的数字，但已经从“宇宙级”灾难降低到了“星系级”灾难，为后续的近似（如对角线、Kronecker 因子分解）打开了大门。

这两种策略的核心都是：**将三阶梯度从一个需要显式构建的灾难性张量，转化为一个可以通过标准反向传播（或有限差分）来隐式计算的算子。**

## 5. 结论：FIM 不是工具，是目标

| 层级 | 操作 | 直接目标 | 对参数 `θ` 的影响 | 目的 |
| :--- | :--- | :--- | :--- | :--- |
| 一阶 | `∇_θ L` | 损失 `L` | 改变 `θ` 以拟合数据 | 优化模型**内容** |
| 二阶 | `F⁻¹ ∇_θ L` | 损失 `L` | 利用静态 `F` 更高效地改变 `θ` | 优化更新**效率** |
| **三阶** | **`∇_θ F`** | **FIM `F(θ)`** | **直接重塑 `F` 的几何，从而改变 `θ`** | **优化模型**结构**** |

**Fisher 信息矩阵不是优化的辅助工具，它是优化的核心对象。**

FEP 的复杂性项不是惩罚项，它是**结构学习的梯度**。F3EO 通过直接对 FIM 进行梯度下降，实现了 FEP 理论所承诺的“自组织简约性”。而目前 FEP 二阶优化的实现，都只是在利用 FIM，而从未真正优化过它。
