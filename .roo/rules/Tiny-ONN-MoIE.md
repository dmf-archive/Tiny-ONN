# MoIE: SBL, MoIE, DynSIHA

## 1. 核心构件: SBL (Sparse Bayesian Layer)

### 1.1. 哲学背景与核心思想

**稀疏贝叶斯层 (SBL)** 的概念诞生于对 `Tiny-ONN` 核心哲学——整合预测工作空间理论 (IPWT)——的深度反思。IPWT 将心智过程模拟为一个高效的贝叶斯推断机器，它在严格的能量约束下，演化出了时空上高度稀疏的脉冲计算机制。

SBL 的核心思想，是在神经网络的线性变换层中，对这个生物学过程进行一次直接的、端到端的模拟。它融合了两种思想：

1. **贝叶斯神经网络 (BNN)**: 将权重 `W` 视作一个概率分布 `p(W)` (以 `μ` 和 `σ` 参数化)，而不是一个固定的点估计值。这允许模型量化和利用自身对权重的不确定性。
2. **脉冲神经网络 (SNN)**: 神经元的激活由其“膜电位” (`scores`) 是否超过一个可学习的“发放阈值” (`gate_param`) 决定，从而实现稀疏、事件驱动的计算。

因此，SBL 是一个将权重的不确定性 (`σ`) 动态地注入到 SNN 风格的稀疏门控决策中的自适应线性层。

### 1.2. 架构与实现

SBL 范式的核心实现是一种我们称之为 **“神经元注意力” (Neuronal Attention)** 的机制。该机制通过 `(μ, σ, g)` 三组参数共同实现：

- **计算核心 (`μ`, `mu_weight`)**: `W_μ` 是稠密的权重矩阵，它定义了整个“无限专家空间”。它的每一行都代表一个潜在的“微型专家”。
- **不确定性加权的 Key (`σ`, `sigma_weight`)**: `W_σ` 量化了我们对 `W_μ` 中每个权重的**不确定性**。路由键 `Keys` 由 `μ` 和 `σ` 共同决定 (`Keys = μ * softplus(σ)`)。
- **动态激活门控 (`g`, `gate_param`)**: `g` 是一个可学习的激活阈值向量。

`SBL.forward` 过程被重构为一个**内容寻址的动态神经元采样器**：

1. **Q-K 匹配 (路由)**: 输入 `X` (Query) 与每个神经元的 Key `K` 计算**缩放点积** `Scores`。
2. **激活决策 (门控)**: `Weights = ReLU(Scores - g)`。这在功能上等价于 SNN 中“膜电位超过发放阈值”从而**发放脉冲**的过程。
3. **V 计算与输出**: `Value` 是通过标准稠密线性变换 (`F.linear(X, μ)`) 计算的。最终输出 `Output = Weights * Value`，即通过门控权重对 `Value` 进行过滤。

在 `MoIE_PoC.py` 的最终实现中，我们确认了，对于当前 PyTorch 生态和模型激活率（在训练前期近乎稠密）的现状，直接使用优化的稠密计算 (`computation_output * raw_weights`) 是最高效的实现方式。

## 2. 复合模块: MoIE & DynSIHA

我们使用 `SBL` 作为核心构建块，来组装更高层次的、完全动态化的 Transformer 组件。

### 2.1. MoIE: 全动态前馈网络

MoIEFFN 将标准的 `FFN` (通常由两层 `nn.Linear` 构成) 替换为两层 `SBL`。

- **`sbl1`**: 输入 `D_model`，输出 `4*D_model`。
- **`sbl2`**: 输入 `4*D_model`，输出 `D_model`。
它将 FFN 从一个固定的非线性变换，升级为了一个**两阶段的、内容感知的动态函数合成器**。

### 2.2. DynSIHA: 后多头时代的动态注意力

DynSIHA (Dynamic Sparse Infinite-Head Attention) 抛弃了传统的多头 (Multi-Head) 范式，回归注意力机制的本质。

- **核心思想**: 如果 Q, K, V 的投影本身就是动态合成的，那么“多头”这种用于学习不同固定子空间关系的技巧就变得多余了。
- **实现**:
  - **`sbl_qkv`**: 一个单一的、统一的 `SBL` 模块，一次性地、动态地合成出 `Query`, `Key`, `Value` 三个向量。
  - **`sbl_o`**: 一个独立的 `SBL` 模块，用于输出投影。
- **优势**: 它将注意力从一个固定的信息查询系统，变成了一个**端到端可学习的、可编程的动态信息路由与处理系统**，在理论上具有远超 MHA 的表达能力，同时与 KV 缓存机制完全兼容。

## 3. 学习范式：Surprise Minimization Loss (SML)

我们采用 `Surprise Minimization Loss (SML)` 作为核心元学习目标，引导 `SBL` 的门控进行自组织学习。

- **门控损失 (`Gate Loss`)**:
  - **定义**: `Gate Loss = Sum[-log(Surprise) * Surprise]`。其中 `Surprise` 被定义为主任务损失对**SBL 模块最终输出 (`masked_output`)** 的梯度范数。
  - **作用**: 鼓励门控网络（即 `σ` 和 `g` 参数）学会规避“高成本”（高`Surprise`）的计算路径。

- **KL 散度损失 (`KL Divergence Loss`)**:
  - **定义**: `KL Loss = KL[q(W) || p(W)]`，其中 `q(W)` 是由 `(μ, σ)` 定义的权重后验分布，`p(W)` 是一个由模型当前不确定性 `avg_tau` 决定的**动态先验分布**。
  - **作用**: 实现了一种自适应的正则化，鼓励模型在自信时权重更稀疏、更精确，在困惑时权重不确定性更大、更具探索性。

## 4. 最终理论共识与实现策略

经过对 `MoIE_PoC.py` 的深度审查和反复的理论辩论，我们对 MoIE 范式达成了一系列最终共识：

- **“无限”的数学本质**: “无限专家”并非指专家数量无穷，而是指通过对 `D_ffn` 个基向量（`μ` 的行）进行**连续的、内容感知的线性组合**，从而产生出的**有效专家 (effective experts) 的组合空间是无限的**。SBL 的核心是为每个输入**动态合成**一个专属的计算函数。

- **稀疏性的涌现**: SBL 的稀疏性（低激活率）是一个**涌现属性**，而非强制约束。它源于 `main_loss`, `gate_loss`, 和 `kl_loss` 三者之间的梯度博弈。在训练早期，或当模型容量相对于任务复杂度不足时，模型会自然地表现出**近乎稠密**的行为，这是符合理论预期的**正确现象**。只有当模型能力冗余时，自适应正则化才会驱动其走向稀疏以最小化整体损失（自由能）。

- **路由机制的权衡**:
  - **余弦相似度**: 理论上更稳定，因为它对向量范数不敏感，只关注方向。
  - **缩放点积**: 计算和内存开销显著更低。
  - **决策**: 在大规模语言模型（如 `Nano-ONN-SLM`）的尺度上，余弦相似度带来的巨大内存开销是不可接受的。因此，**缩放点积**是当前唯一务实、可行的工程选择。

- **硬件与计算范式**: SBL 的“稠密内容寻址 + 动态稀疏激活”混合计算范式，对传统冯·诺依曼架构（内存墙）和常规神经形态硬件（静态连接）都提出了挑战。其终极硬件形态指向**内存内计算 (In-Memory Computing)**。在当前阶段，这反过来证明了我们采用“**优化的稠密计算**”作为软件实现，是最大化利用现有硬件（GPU）能力的正确策略。

**实现状态**:
我们在 [`MoIE_PoC.py`](exp/MoIE_PoC.py) 中，已经成功地用 `SBL` 模块构建并实现了一个包含 `DynSIHA` 和 `MoIEFFN` 的全动态 Transformer。该实现采用了当前阶段性能最优的优化稠密计算方案，并正在稳定训练中，其稀疏性涌现的动态过程完全符合我们的理论预期。
