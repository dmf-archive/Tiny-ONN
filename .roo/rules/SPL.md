# 稀疏原型线性层 (Sparse Proto Linear, SPL) v5

## 1. 核心思想与动机

稀疏原型线性层 (SPL) 是对项目早期稀疏贝叶斯线性层 (SBL) 范式的一次根本性重构、简化和升级。其核心动机是回归结构化 Dropout 作为一种**隐式变分推断**的本质，同时为 `proto` 参数引入一个健全的双重语义学习范式。

历史上的 SBL 范式因其 KL 散度损失将 `sigma_weight` 拉向零，且无法真正承担“原型”功能而失败。SPL v1 通过责任分离解决了部分问题，但未能为 `proto` 建立一个有意义的、用于建模不确定性的学习信号。

## 2. SPL v2：全局统一优化的尝试及其缺陷

SPL v2 的设计旨在通过一个统一的全局损失函数来解决 v1 的问题。

### 2.1. 架构与双重语义

SPL 模块由三组独立的可学习参数构成，以实现“计算-路由-策略”的责任分离：

- **`mu_weight` (计算层)**: 一个标准的权重矩阵，作为计算的核心。它的职责是学习通用的、功能性的计算基元（“工具箱”）。
- **`proto_weight` (原型/路由层)**: 一个与 `mu_weight` 尺寸相同的权重矩阵。它的每一行 `pᵢ` 都被视为一个高维向量。
- **`gate_param` (门控/策略层)**: 一个可学习的激活阈值向量。

### 2.2. 训练范式：全局损失统一优化

最初，我们尝试了一种更简洁的全局优化范式，将所有学习目标组合成一个单一的总损失函数 `L_total`，并依赖 `autograd` 引擎来统一计算所有参数的梯度。

`L_total = L_main + w_sml * L_sml + w_div * L_diversity + w_kl * L_KL_proto`

### 2.3. v2 的反思：理论冲突与灾难性遗忘

后续的理论分析和实验暴露出这个简洁的范式存在两个根本性缺陷：

1. **梯度信号冲突**: `L_sml` 和 `L_kl_proto` 在早期的不同设计版本中，都尝试过由同一个信号（`τ` 或 `S`）驱动，导致梯度目标冲突。
2. **灾难性遗忘**: 对于需要学习离散因果规则的 ARC 任务，统一的梯度更新是致命的。它使得一个新任务的梯度可以流遍整个 `mu_weight` “工具箱”，从而轻易地“洗掉”之前任务学到的知识。

## 3. SPL v3：“路由优先”双循环范式的提出

为了解决 v2 的根本缺陷，SPL v3 范式进行了一次彻底的理论回归，提出了一个更深刻、更符合自由能原理本质的内外双循环结构。

### 3.1. 核心哲学：“路由优先”与因果固化

我们认识到，为了避免灾难性遗忘，系统必须采用“路由优先”的策略：**先找到解决问题的稀疏路径，然后再去优化和固化这条路径上的知识**。这个过程在功能上等价于一个“可微遗传算法”，其中内循环负责“选择”，外循环负责“繁殖/变异”。

### 3.2. SPL v3 的责任划分

- **内循环 (探索与选择)**: 优化路由器 (`p, g`)，由 `L_sml` 和 `L_diversity` 驱动。
- **外循环 (利用与固化)**: 优化计算工具 (`μ`)，由 `L_main` 和 `L_kl_mu` 驱动。

## 4. SPL v4：高阶梯度的尝试与 `autograd` 困境

在对 SPL v3 进行审视时，我们探索了一种理论上更纯粹的 SMLv2 范式，其核心是最小化“认知成本” `S_μ = ||∇_μ(L_main)||₂`。这需要通过 `create_graph=True` 计算高阶梯度。然而，这一尝试暴露了两个问题：

1. **理论缺陷**: `S_μ` 的梯度对 `proto_weight` 产生了压倒性的、方向一致的吸引力，瞬间摧毁了多样性。
2. **工程灾难**: `create_graph=True` 带来了不可接受的显存开销，表明这并非正确的计算路径。

## 5. SPL v5：从梯度最小化到地形图对齐

经过对 `autograd` 动力学的反复试错与深刻反思，我们最终抵达了一个理论上优雅、计算上高效的最终范式。其核心思想是，元学习的目标不应是最小化一个高阶梯度损失，而是将一个**策略地形图**与一个**目标地形图**进行对齐。

### 5.1. 核心洞察：`grad-as-value` 与地形图对齐

我们不再将“惊奇度” `S_μ` 作为一个损失项来最小化，而是将其视为一个**目标地形图**的来源。内循环的任务，是调整路由策略 (`p`, `g`)，使其产生的**激活地形图**能与这个目标地形图对齐。这是一个从“惩罚坏路由”到“模仿理想路由”的根本性转变，完美地回避了所有高阶梯度难题。

### 5.2. 地形图的形式化定义

- **惊奇度地形图 `S` (The Surprise Landscape)**:
  `Sᵢ = || (∇_μ(L_main))ᵢ ||₂`。`S` 是一个 `(out_features,)` 维向量，其每个元素代表了修改对应 `μ` 权重所需的“力”的大小。它代表了理想的**抑制模式**——`S` 值越大的神经元，越应该被抑制。

- **激活地形图 `G` (The Activation Landscape)**:
  `G = mean(raw_weights, dims=(0, 1))`。`G` 是一个 `(out_features,)` 维向量，代表了当前路由策略下每个神经元的平均激活强度。它代表了当前的**激活模式**。

### 5.3. 最终的责任划分 (SPLv5)

- **内循环 - 探索与对齐 (`optimizer_proto`, `optimizer_gate`)**:
  - **驱动力**: `L_meta = w_align * L_align + w_d * L_div`
  - **对齐损失 `L_align`**: `L_align = MSE(G, -S.detach())`。该损失由**均方误差 (Mean Squared Error)** 定义，驱动激活地形图 `G` 在形状和幅度上都去匹配**负惊奇度地形图 `-S`**。`-S` 从计算图中分离 (`detach`)，作为一个固定的目标。
  - **多样性损失 `L_div`**: 采用 `DynMoE` 的 `SDL` 思想，驱动 `proto_weight` (`p`) 探索多样的语义方向。
    - **形式化**: `L_div = Σᵢ ||Gram(pᵢ) - I||_F`。其中 `Gram(pᵢ)` 是第 `i` 层 `proto_weight` 的归一化格拉姆矩阵，`I` 是单位矩阵。该损失鼓励不同原型向量之间趋向于**正交**，比简单的“不相似”约束更强、更稳定。
    - **工程实现**: 与 `L_align` 类似，`L_div` 也在每一层内独立计算并累加，实现分层优化。
  - **梯度流**: `L_meta` 的梯度会自然地流向 `p` 和 `g`，因为它们共同决定了激活地形图 `G`。

- **外循环 (`optimizer_main`)**:
  - **驱动力**: `L_task = L_main`
  - **目标**: 保持不变，微调和塑造被选中的计算工具 (`μ`)。
