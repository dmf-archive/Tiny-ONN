# 稀疏原型线性层 (Sparse Proto Linear, SPL) v2

## 1. 核心思想与动机

稀疏原型线性层 (SPL) 是对项目早期稀疏贝叶斯线性层 (SBL) 范式的一次根本性重构、简化和升级。其核心动机是回归结构化 Dropout 作为一种**隐式变分推断**的本质，同时为 `proto` 参数引入一个健全的双重语义学习范式。

历史上的 SBL 范式因其 KL 散度损失将 `sigma_weight` 拉向零，且无法真正承担“原型”功能而失败。SPL v1 通过责任分离解决了部分问题，但未能为 `proto` 建立一个有意义的、用于建模不确定性的学习信号。SPL v2 旨在通过引入一种新颖的、动态的 KL 散度损失来最终解决此问题。

## 2. 架构与双重语义

SPL 模块由三组独立的可学习参数构成，以实现“计算-路由-策略”的责任分离。其中，`proto_weight` 被赋予了双重语义：

- **`mu_weight` (计算层)**: 一个标准的权重矩阵，作为计算的核心。它的职责是学习通用的、功能性的计算基元（“工具箱”）。

- **`proto_weight` (原型/路由层)**: 一个与 `mu_weight` 尺寸相同的权重矩阵。它的每一行 `pᵢ` 都被视为一个高维向量，其语义被分解为：

  - **方向 `pᵢ / ||pᵢ||`**: 代表了第 `i` 个神经元所识别的**抽象模式的内容**。其学习由 `L_diversity` 损失驱动，旨在形成一组正交的“模式字典”。
  - **范数 `||pᵢ||`**: 代表了对该模式识别的**置信度**，即 `mu` 对应权重的**逆不确定性（精度）**。其学习由一个新的 `L_KL_proto` 损失驱动。

- **`gate_param` (门控/策略层)**: 一个可学习的激活阈值向量。它的职责是学习一个**门控策略**，即根据 `proto` 提供的、由方向和置信度共同决定的 `scores`，来动态地组合 `mu` 中的计算基元。其梯度来自元学习损失 `L_sml`。

## 3. 训练范式：全局损失统一优化

与内外双循环的复杂机制不同，我们回归到一个更简洁、更强大的全局优化范式。该范式将所有学习目标组合成一个单一的总损失函数 `L_total`，并依赖 PyTorch 的 `autograd` 引擎来统一计算所有可学习参数 (`mu_weight`, `proto_weight`, `gate_param`) 的梯度。

**总损失函数 (Total Loss):**
`L_total = L_main + w_sml * L_sml + w_div * L_diversity + w_kl * L_KL_proto`

其中，`w_*` 是各辅助损失的权重超参数，它们可以被设置为固定值，也可以根据模型的训练状态（如 `main_loss` 或 `τ`）进行动态调节。

这个方法将梯度计算的复杂性完全交给了自动微分系统，允许不同损失函数的梯度自然地、相互作用地影响所有相关参数，从而在一个统一的优化步骤中协同演进。

## 4. 辅助损失函数详解

### 4.1. `L_sml` (门控损失)

- **定义**: `L_sml = SML(∇_computation_output L_main)`
- **作用**: 主要驱动**门控/策略层 `gate_param`** 的学习，但其高阶梯度也会影响到 `mu` 和 `proto`。

### 4.2. `L_diversity` (多样性损失)

- **定义**: `L_div = 1 - PairwiseCosineSimilarity(normalized_prototypes)`
- **作用**: 主要驱动**原型 `proto_weight` 的方向**学习，鼓励形成正交化的“模式字典”。

### 4.3. `L_KL_proto` (全动态 KL 散度损失)

- **定义**: `L_KL_proto = ½ [ (1 / (||p||² * τ²)) - 1 + ln(τ² * ||p||²) ]`
- **推导**: 此损失源于最小化后验 `q = N(μ, 1/||p||²)` 与动态先验 `p = N(μ, τ²)` 之间的 KL 散度。它为 `proto` 的范数 `||p||` 设定了一个动态目标 `1/τ`。
- **作用**: 主要驱动**原型 `proto_weight` 的范数**学习，使其能够建模与模型自身认知不确定性 `τ` 相反的置信度。

## 5. 与 SBL 的关系

SPL 可以被视为 SBL 思想在实践中的一次“奥卡姆剃刀式”的提纯。它继承了 SBL 通过结构化 Dropout 实现动态稀疏的核心思想，但抛弃了其早期实现中复杂的、显式的贝叶斯组件，转而通过一个更精巧的、分离的训练范式，来实现隐式变分推断的真正潜力。SPL 的有效性仍有待进一步的实验验证。
