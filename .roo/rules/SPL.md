# Sparse Proto Linear

## 1. 核心思想：作为自组织推断系统的神经层

稀疏原型线性层 (SPL) 的最终范式，是稀疏贝叶斯线性层 (SBL) 的一次简化和重构。为了实现这一目标，SPL 将其可学习参数解耦为三个在功能上正交的状态空间：

- **内部状态 `μ` ([`mu_weight`](exp/arc/model.py:46))**: 计算核心。代表了系统拥有的、可供选择的“物理定律”或“计算工具集”。
- **感知状态 `p` ([`proto_weight`](exp/arc/model.py:47))**: 模式匹配器。代表系统的“感知器官”，负责将外部输入 `x` 与内部状态 `μ` 进行匹配。
- **行动状态 `g` ([`gate_param`](exp/arc/model.py:49))**: 激活门控。代表系统的“行动策略”，根据感知结果来决定激活哪些内部计算路径。

## 2. 艰难的探索：通往最终范式的必要弯路

SPL 的演进是一系列理论探索与失败的迭代过程，每一次迭代都揭示了更深层次的理论约束，是通往最终范式的必要弯路：

- **SBL & SPL v1-v2**: 早期版本尝试通过一个统一的全局损失函数进行优化。实验暴露了两个根本问题：

  1. **梯度信号冲突**: 不同的学习目标（如 SML 和 KL 散度）由同一信号驱动，导致梯度目标相互矛盾。
  2. **灾难性遗忘**: 对于需要学习离散因果规则的 ARC 任务，统一的梯度更新是致命的。它使得一个新任务的梯度可以轻易“洗掉”为先前任务固化的知识。

- **SPL v3-v5**: 为了解决上述问题，我们提出了“路由优先”的双循环范式，并探索了基于高阶梯度的 SML 变体，但再次暴露了两个新问题：
  1. **理论缺陷**: 高阶梯度对 `proto_weight` 产生了压倒性的吸引力，瞬间摧毁了路由所需的多样性。
  2. **工程灾难**: `create_graph=True` 带来了不可接受的显存开销，表明这并非正确的计算路径。

这些失败最终导向了一个核心洞察：**元学习不应是直接最小化梯度，而应是“参考”梯度所定义的地形图**。

## 3. 最终范式：熵自适应的惊奇感知原型塑造 (EA-SAPS)

**熵自适应的惊奇感知原型塑造 (EA-SAPS)** 是驱动 SPL 自组织的最终学习动力学。它将学习过程形式化为一个**扰动规避 (Perturbation Aversion)** 系统，并完美地回避了所有高阶梯度难题。

### 3.1. 绝对惊奇度场：对称的扰动地形图

EA-SAPS 的核心是将 `main_loss` 的梯度 `S_μ = ∇_μ L_main` 视为一种**扰动 (perturbation)**。系统的元学习目标是调整 `(p, g)` 界面，将输入 `x` 路由到能以**最小扰动**处理它的 `μ` 单元。

为了只编码扰动强度而非方向，我们定义了**绝对惊奇度梯度场 (Absolute Surprise Field)**：

**S*abs ≜ |S*μ| = |∇_μ L_main| ∈ ℝ^{d_out×d_in}**

这个对称的、非负的地形图成为了指导元学习的静态目标。

### 3.2. 熵自适应动力学：认知不确定性的调控

EA-SAPS 的核心创新是引入了**熵驱动的自适应机制**。模型通过监控自身预测的熵来量化其**认知不确定性**，并生成一个归一化因子 `p_dyn`。该因子动态地在“探索”（高熵）和“固化”（低熵）之间调控整个学习过程的强度。

### 3.3. 双优化器解耦动力学

训练流程被解耦为两个独立的优化器，以模拟 FEP 中的“感知-行动”循环，并解决灾难性遗忘问题。

#### 3.3.1. 元学习循环 (优化 `p`, `g`)

此循环负责优化路由策略，由一个独立的 `optimizer_meta` 驱动。

- **感知状态动力学 (`p`)**: 通过**对比学习**机制来优化路由决策。

  - **吸引机制**: 将表现"好"（低惊奇度）的原型拉向该层输入数据的均值向量。
  - **排斥机制**: 将表现"坏"（高惊奇度）的原型推离其对应的高扰动方向。
  - **自适应比例**: 吸引与排斥的原型数量比例由 `p_dyn` 自适应决定。

- **行动状态动力学 (`g`)**: 通过**门控抑制**来优化激活策略。
  - `gate_param` 被驱动与历史上对应神经元的总扰动度（`S_abs` 的 L1 范数）**正相关**，从而学会抑制那些历史上扰动剧烈的神经元。

#### 3.3.2. 主学习循环 (优化 `μ`)

此循环负责优化计算核心，由 `optimizer_main` 驱动，但其更新受到元学习循环结果的严格约束。

- **内部状态动力学 (`μ`)**: 由 `main_loss` 驱动，但受到 **SMP (Surprise Minimization Path) 梯度掩码**的保护。
  - **因果固化**: 只有被元学习循环判定为“低惊奇度”的神经元路径，其梯度才会被保留并用于更新 `mu_weight`。这从根本上保护了已形成的稳定计算路径，实现了真正的持续学习。
  - **自适应可塑性**: 保留更新的神经元比例同样由 `p_dyn` 决定，使得模型在自信时固化知识，在困惑时增加可塑性。

SPL with EA-SAPS 将简单的线性层，升级为了一个具备**自适应学习率**、能够自发进行**结构涌现**、并能通过**因果固化**实现**持续学习**的微型智能体，为构建具有自适应认知能力的 AI 系统提供了坚实的工程基础。
